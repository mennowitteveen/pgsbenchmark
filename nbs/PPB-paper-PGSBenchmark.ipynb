{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:75% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This code is not functional without individual level data!\n",
    "\n",
    "# %load_ext rpy2.ipython\n",
    "########################################################\n",
    "## Base Imports:\n",
    "\n",
    "# Sys Imports:\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Standard Imports:\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scientific computing\n",
    "from scipy import stats\n",
    "from scipy import linalg\n",
    "\n",
    "#########################################################\n",
    "## Experiment Specific Imports\n",
    "\n",
    "# Logistics Imports:\n",
    "import psutil\n",
    "from sys import getsizeof\n",
    "import inspect\n",
    "from importlib import reload \n",
    "import copy\n",
    "import IPython as ip\n",
    "import pickle\n",
    "import sys\n",
    "import submitit\n",
    "# from mjwt.tools import Timer, Tree, beep, fullvars #dont forget about mro \n",
    "# from mjwt.tools import sizegb, implot, redo_all_above, Struct, ResultsStorage\n",
    "# from mjwt.tools import jobinfo, corr\n",
    "from IPython.display import display, HTML, Audio, Javascript\n",
    "from tqdm.notebook import tqdm\n",
    "import glob, re\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "\n",
    "\n",
    "# ML Imports:\n",
    "from sklearn.metrics import r2_score, roc_auc_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "\n",
    "# Genomics Imports:\n",
    "from pysnptools.snpreader import Bed, Pheno, SnpHdf5, SnpData\n",
    "from pysnptools.pstreader import PstData, PstHdf5, PstReader\n",
    "from pysnptools.kernelreader import KernelHdf5, KernelData\n",
    "from pysnptools.standardizer import UnitTrained\n",
    "import pysnptools.util as pstutil\n",
    "\n",
    "\n",
    "\n",
    "########################################################\n",
    "## Configuration & Initialisation\n",
    "\n",
    "# Display Configuration:\n",
    "from IPython.display import set_matplotlib_formats\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "pd.set_option('display.max_colwidth', None) # No pd trunkation (radical)\n",
    "display(HTML(\"<style>.container { width:75% !important; }</style>\"))\n",
    "\n",
    "# Initializations:\n",
    "timer = Timer(); toc = timer.toc; tic = timer.tic; tic('')\n",
    "job_dt_lst   = [] if not 'job_dt_lst'   in locals() else job_dt_lst\n",
    "model_dt_lst = [] if not 'model_dt_lst' in locals() else model_dt_lst\n",
    "notebook = False  if '__file__' in locals() else True\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = str(int(os.environ['SLURM_JOB_CPUS_PER_NODE']) - 1)\n",
    "log=np.log10\n",
    "# get_ipython().run_line_magic('load_ext', 'line_profiler')\n",
    "# import line_profiler\n",
    "# %load_ext line_profiler\n",
    "# sys.version_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is not functional without individual level data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinkageData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "LinkageData\n",
    "durr tst\n",
    "\"\"\"\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "from sys import getsizeof\n",
    "\n",
    "random = []\n",
    "from collections import OrderedDict\n",
    "from pysnptools.standardizer import Unit\n",
    "import pysnptools\n",
    "import warnings\n",
    "from collections import deque, defaultdict\n",
    "import pandas as pd\n",
    "import pysnptools.util as pstutil\n",
    "from pysnptools.standardizer import UnitTrained\n",
    "\n",
    "\n",
    "class SqrtNinv(Unit):\n",
    "    def __init__(self):\n",
    "        super(SqrtNinv, self).__init__()\n",
    "\n",
    "\n",
    "class BaseLinkageData():\n",
    "\n",
    "    def __init__(self, *, sst_df, regdef_df, n_samples_sst=None,\n",
    "                 srd=None, bim_df=None, sda_standardizer=Unit,\n",
    "                 prd=None, fam_df=None, pda_standardizer=Unit,\n",
    "                 lrd=None, lda_standardizer=None,\n",
    "                 grd=None, gda_standardizer=False,\n",
    "                 singurd=None,\n",
    "                 distal_linkage='shiftblocks', shift=0, cm=None, do_setzero=True,\n",
    "                 allow_onthefly_linkage_gen=False,\n",
    "                 clear_decomp_local_linkage=False, \n",
    "                 clear_xda=True,\n",
    "                 clear_linkage=False,\n",
    "                 compute_sumstats=False,\n",
    "                 calc_allelefreq=False,\n",
    "                 always_unit=True,\n",
    "                 _region_filter_fun=None,\n",
    "                 gb_size_limit=10., dtype='float32', verbose=False):\n",
    "\n",
    "        # bim and fam df have to be supplied because pysnptools halvely\n",
    "        # implemented these portions of the genetic data into their object\n",
    "        # meaning that srd cannot be relied uppon\n",
    "\n",
    "        # New rule: blx have to be created from the inside\n",
    "        # Perhaps later it can be made into a special load instead of a compute\n",
    "\n",
    "        # Initial checks:\n",
    "        assert type(sst_df) is pd.DataFrame\n",
    "        if cm is not None: assert cm > 0\n",
    "        \n",
    "        self.regdef_df = regdef_df  # checks?\n",
    "        # Checks that are needed: region ranges count upward, check if regid is present, else make it.\n",
    "        # assert chrom contains no None's and nans\n",
    "\n",
    "        self.srd = srd  # SNP reader\n",
    "        self.prd = prd\n",
    "        self.grd = grd\n",
    "        self.singurd = singurd # Singular value ReaDer.\n",
    "        self.lrd = lrd\n",
    "        if lrd is not None: raise NotImplementedError('lrd not possible atm.')\n",
    "        # This should go different in the future:\n",
    "        # https://github.com/scikit-learn/scikit-learn/blob/7e1e6d09bcc2eaeba98f7e737aac2ac782f0e5f1/sklearn/base.py#L31\n",
    "        # clone objects instead of messing with classes.. skl -> clone(estimator)\n",
    "        self.sda_Standardizer = sda_standardizer  # Pysnptools standardizer\n",
    "        self.pda_Standardizer = pda_standardizer\n",
    "        self.lda_Standardizer = lda_standardizer\n",
    "        if grd is not None:\n",
    "            assert gda_standardizer or (gda_standardizer is None)\n",
    "        self.gda_Standardizer = gda_standardizer\n",
    "        self.dtype = dtype  # dtype to work in\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Linkage Algo Settings:\n",
    "        self.distal_linkage = distal_linkage  # Refering to the approach to determine left and right linkage\n",
    "        self.shift = shift  # The integer left&right blockshift that is to be used\n",
    "        self.cm    = cm\n",
    "        self.do_setzero = do_setzero\n",
    "        #if not allow_onthefly_linkage_gen:\n",
    "            #raise NotImplementedError('blocks against on the fly comp have not been coded. contact dev.')\n",
    "        self.allow_onthefly_linkage_gen = allow_onthefly_linkage_gen \n",
    "        #self.clear_decomp_local_linkage = clear_decomp_local_linkage\n",
    "        self.clear_xda = clear_xda  # whether to clear the gda, sda, etc. when linkage is determined\n",
    "        self.clear_linkage = clear_linkage\n",
    "        self.always_unit = always_unit\n",
    "\n",
    "        # Summstats:\n",
    "        assert type(compute_sumstats) is bool\n",
    "        self.compute_sumstats = compute_sumstats  # Determines whether sumstats should be computed.\n",
    "\n",
    "        # DataFrames & Other non pst-reader objects:\n",
    "        self.sst_df = sst_df  # Should become a bim compliant sst dataframe.\n",
    "        self.bim_df = bim_df  # assert chrom contains no nons & more\n",
    "        self.fam_df = fam_df  # Not really used atm.\n",
    "        self.reg_dt = OrderedDict()\n",
    "        self.regid2i_dt = OrderedDict()\n",
    "        self.cur_total_size_in_gb = 0.0\n",
    "        self.gb_size_limit = gb_size_limit\n",
    "        self.xda_q = deque()\n",
    "        [self.xda_q.append((-1,'')) for _ in range(5)]  # put 5x -1 in queue\n",
    "        self.reloaded_xda_cnt = 0\n",
    "        self.stansda_dt = OrderedDict()\n",
    "\n",
    "        # Checks:\n",
    "        if self.srd is not None:\n",
    "            self._check_xrd()\n",
    "        else:\n",
    "            raise NotImplementedError('need srd for now')\n",
    "\n",
    "        # Determination of # of samples in this data:\n",
    "        if self.compute_sumstats:\n",
    "            assert self.prd is not None\n",
    "            self.n_samples_sst = self.prd.shape[0]\n",
    "        else:\n",
    "            assert n_samples_sst is not None\n",
    "            self.n_samples_sst = n_samples_sst\n",
    "        assert type(self.n_samples_sst) is int\n",
    "        self.calc_allelefreq = calc_allelefreq\n",
    "\n",
    "        # Init the regions:\n",
    "        self.init_regions()\n",
    "\n",
    "        # Filtering of regions functionality:\n",
    "        if _region_filter_fun is None:\n",
    "            self._region_filter_fun = self._default_region_filter_fun\n",
    "        else:\n",
    "            self._region_filter_fun = _region_filter_fun\n",
    "\n",
    "    def _check_xrd(self):\n",
    "\n",
    "        if self.srd is not None:\n",
    "            assert pysnptools.snpreader.SnpReader in self.srd.__class__.__mro__\n",
    "\n",
    "        if self.prd is not None:\n",
    "            n_start = len(self.prd.iid)\n",
    "            self.srd, self.prd = pstutil.intersect_apply([self.srd, self.prd])\n",
    "            if len(self.prd.iid) != n_start:\n",
    "                warnings.warn(f'Number of samples do not match up after internal intersection, samples were lost: {n_start - len(self.prd.iid)}, start = {n_start}, after_intersection = {len(self.prd.iid)}')\n",
    "\n",
    "        if self.grd is not None:\n",
    "            # Check alignment for now, auto alignment needs work cause iid stuffs:\n",
    "            if self.srd is not None:\n",
    "                if not np.all(self.grd.sid == self.srd.sid):\n",
    "                    raise Exception('snps of grd and srd not matching up, align first,'\n",
    "                                    ' auto align will be implemented later')\n",
    "            else:\n",
    "                raise NotImplementedError('Not sure what to do with grd if no srd is present. not implemented.')\n",
    "        \n",
    "    def init(self): \n",
    "        return self\n",
    "\n",
    "    ###########################\n",
    "    # Regions Administration:\n",
    "\n",
    "    def init_regions(self):\n",
    "        do_beta_moving = ('beta_mrg' in self.sst_df.columns)\n",
    "        if not do_beta_moving:\n",
    "            warnings.warn('No \\'beta\\' column detected in sst_df! This means that no summary stats were detected.')\n",
    "        cur_chrom = None\n",
    "        i = 0; n_snps_cumsum = 0\n",
    "        sst_df_lst = []\n",
    "        for reg_cnt, (_, row) in enumerate(self.regdef_df.iterrows()):\n",
    "            # Move region into specialized dictionary\n",
    "            regid = row['regid'];\n",
    "            chrom = row['chrom']\n",
    "            start = row['start'];\n",
    "            stop  = row['stop']\n",
    "\n",
    "            # Map Variants to region\n",
    "            ind = self.sst_df.chrom == chrom\n",
    "            ind = (self.sst_df['pos'] >= start) & ind\n",
    "            ind = (self.sst_df['pos'] < stop) & ind\n",
    "            sid = self.sst_df['snp'][ind].values\n",
    "            indices = self.srd.sid_to_index(sid)  # if sid not strickly present this will give an error!\n",
    "            n_snps_reg = len(indices)\n",
    "            if n_snps_reg == 0:\n",
    "                continue\n",
    "            else:\n",
    "                geno_dt = dict(regid=regid,\n",
    "                               chrom=chrom,\n",
    "                               start=start,\n",
    "                               stop=stop,\n",
    "                               start_j=n_snps_cumsum)\n",
    "                n_snps_cumsum += n_snps_reg\n",
    "                geno_dt['stop_j'] = n_snps_cumsum\n",
    "                geno_dt['n_snps_reg'] = n_snps_reg\n",
    "                sst_df = self.sst_df[ind].copy(); sst_df['i'] = i\n",
    "                geno_dt['sst_df'] = sst_df\n",
    "                assert geno_dt['start_j'] == sst_df.index[0]; sst_df_lst.append(sst_df)\n",
    "                assert geno_dt['stop_j']  == sst_df.index[-1] + 1\n",
    "                if do_beta_moving:\n",
    "                    geno_dt['beta_mrg'] = geno_dt['sst_df']['beta_mrg'].copy().values[:, np.newaxis]\n",
    "                    assert len(geno_dt['beta_mrg'].shape) == 2\n",
    "                self.regid2i_dt[regid] = i\n",
    "                if self.srd is not None:\n",
    "                    geno_dt['srd'] = self.srd[:, indices]\n",
    "                    geno_dt['stansda'] = self.sda_Standardizer() if self.sda_Standardizer is not None else None\n",
    "                else:\n",
    "                    raise NotImplementedError()\n",
    "                if self.grd is not None:\n",
    "                    geno_dt['grd'] = self.grd[:, indices]\n",
    "                    geno_dt['stangda'] = self.gda_Standardizer() if self.gda_Standardizer is not None else None\n",
    "                # Count up if things are actually stored in reg_dt\n",
    "                self.reg_dt[i] = geno_dt\n",
    "                i += 1\n",
    "        self.n_snps_total = n_snps_cumsum\n",
    "        sst_df = pd.concat(sst_df_lst, axis=0)\n",
    "        self.sst_df = sst_df\n",
    "\n",
    "    def get_i_regions_lst(self, filter_like_lst=None):\n",
    "        if filter_like_lst is not None:\n",
    "            raise NotImplementedError()\n",
    "        return list(self.reg_dt.keys())\n",
    "\n",
    "    def _default_region_filter_fun(self, *, geno_dt):\n",
    "        filter_key = geno_dt['chrom']\n",
    "        return filter_key\n",
    "\n",
    "    def set_region_filter_fun(self, fun):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def gen_filter_dt(self):\n",
    "        filter_dt = defaultdict(list)\n",
    "        for i, geno_dt in self.reg_dt.items():\n",
    "            filter_key = self._region_filter_fun(geno_dt=geno_dt)\n",
    "            if filter_key is None:\n",
    "                continue\n",
    "            else:\n",
    "                filter_dt[filter_key].append(i)\n",
    "        self.filter_dt = filter_dt\n",
    "        return filter_dt\n",
    "\n",
    "    def _load_all_snpdata(self):\n",
    "        # load all regions\n",
    "        for i, geno_dt in self.reg_dt.items():\n",
    "            sda = geno_dt['srd'].read(dtype=self.dtype)\n",
    "            stansda = sda.train_standardizer(apply_in_place=True,\n",
    "                                             standardizer=geno_dt['stansda'])\n",
    "            geno_dt['sda'] = sda\n",
    "            geno_dt['stansda'] = stansda\n",
    "\n",
    "    #####################################\n",
    "    # General Linkage Retrieval Methods:\n",
    "\n",
    "    ###########################\n",
    "    ## Compute: ###############\n",
    "\n",
    "    # Local Linkage Stuff: ####\n",
    "    \n",
    "    def compute_linkage_sameregion(self, *, i):\n",
    "        return self.compute_linkage_shiftregion(i=i, shift=0)\n",
    "\n",
    "    def regions_compatible(self, *, i, j):\n",
    "        try:\n",
    "            if self.reg_dt[i]['chrom'] == self.reg_dt[j]['chrom']:\n",
    "                res = True\n",
    "            else:\n",
    "                res = False\n",
    "        except Exception as e:\n",
    "            if (not (i in self.reg_dt.keys())) or (not (j in self.reg_dt.keys())):\n",
    "                res = False\n",
    "            else:\n",
    "                raise e\n",
    "        return res\n",
    "\n",
    "    def compute_linkage_shiftregion(self, *, i, shift):\n",
    "        j = i + shift\n",
    "        if self.regions_compatible(i=i, j=j):\n",
    "            self_sda = self.get_sda(i=i)\n",
    "            dist_sda = self.get_sda(i=j)\n",
    "            n = len(self_sda.iid)\n",
    "            S_shift = self_sda.val.T.dot(dist_sda.val) / n\n",
    "            return S_shift\n",
    "        else:\n",
    "            self_sda = self.get_sda(i=i)\n",
    "            return np.zeros((self_sda.val.shape[1], 0))\n",
    "        \n",
    "    def compute_linkage_cmfromregion(self, *, i, cm):            \n",
    "        geno_dt = self.reg_dt[i]; lst = []\n",
    "        if cm < 0: # Doing left:\n",
    "            stop_j   = geno_dt['start_j']\n",
    "            cm_left  = geno_dt['sst_df'].loc[stop_j]['cm'] \n",
    "            slc_df = self.sst_df.loc[:stop_j-1]\n",
    "            slc_df = slc_df[slc_df.chrom==geno_dt['chrom']]\n",
    "            slc_df = slc_df[slc_df.cm > (cm_left + cm)]\n",
    "            start_i = slc_df['i'].min()\n",
    "            start_i = -7 if np.isnan(start_i) else start_i\n",
    "            for cur_i in range(start_i, i):\n",
    "                lst.append(self.compute_linkage_shiftregion(i=i, shift=cur_i-i))\n",
    "                if start_i == -7: break\n",
    "            L = np.concatenate(lst, axis=1)[:,-slc_df.shape[0]:] # concat & clip\n",
    "            if self.do_setzero:\n",
    "                cms_reg    = geno_dt['sst_df']['cm'].values\n",
    "                cms_distal = slc_df['cm'].values\n",
    "                cms_L      =  cms_distal[np.newaxis,:] - cms_reg[:,np.newaxis]\n",
    "                setzero_L  = cms_L < cm\n",
    "                L[setzero_L] = 0\n",
    "                assert L.shape == setzero_L.shape\n",
    "            return L\n",
    "        else:\n",
    "            start_j   = geno_dt['stop_j']\n",
    "            cm_right  = geno_dt['sst_df'].loc[start_j-1]['cm']\n",
    "            slc_df = self.sst_df.loc[start_j:]\n",
    "            slc_df = slc_df[slc_df.chrom==geno_dt['chrom']]\n",
    "            slc_df = slc_df[slc_df.cm < (cm_right + cm)]\n",
    "            stop_i = slc_df['i'].max()\n",
    "            stop_i = i+2 if np.isnan(stop_i) else stop_i + 1\n",
    "            for cur_i in range(i+1, stop_i):\n",
    "                lst.append(self.compute_linkage_shiftregion(i=i, shift=cur_i-i))\n",
    "            R = np.concatenate(lst, axis=1)[:,:slc_df.shape[0]] # concat & clip\n",
    "            if self.do_setzero:\n",
    "                cms_reg    = geno_dt['sst_df']['cm'].values\n",
    "                cms_distal = slc_df['cm'].values\n",
    "                cms_R     =  cms_distal[np.newaxis,:] - cms_reg[:,np.newaxis]\n",
    "                setzero_R = cms_R > cm\n",
    "                R[setzero_R] = 0\n",
    "                assert R.shape == setzero_R.shape\n",
    "            return R\n",
    "        \n",
    "    # Glocal Linkage Stuff: ####\n",
    "    \n",
    "    def compute_linkage_decompshiftregion(self, *, i, shift):\n",
    "        j = i + shift\n",
    "        if self.regions_compatible(i=i, j=j):\n",
    "            self_gda = self.get_gda(i=i)\n",
    "            dist_gda = self.get_gda(i=j)\n",
    "            # n = len(self_gda.iid)\n",
    "            # There is not division by n here, important difference!!\n",
    "            S_glocalshift = self_gda.val.T.dot(dist_gda.val)\n",
    "            return S_glocalshift\n",
    "        else:\n",
    "            self_gda = self.get_gda(i=i)\n",
    "            return np.zeros((self_gda.val.shape[1], 0))\n",
    "\n",
    "    def compute_linkage_globalregion(self, *, i):\n",
    "        gda = self.get_gda(i=i)\n",
    "        Z = gda.val\n",
    "        if gda.sqrtninvscaling:\n",
    "            n_q = Z.shape[0]\n",
    "            Z = Z / np.sqrt(n_q)\n",
    "        return Z\n",
    "    \n",
    "    # NON Linkage Stuff: #######\n",
    "    \n",
    "    def compute_sumstats_region(self, *, i):\n",
    "        geno_dt = self.reg_dt[i]\n",
    "        sda = self.get_sda(i=i)\n",
    "        X = sda.val\n",
    "        y = self.get_pda().val\n",
    "        n = len(y)\n",
    "        c_reg = X.T.dot(y) / n\n",
    "        return c_reg\n",
    "    \n",
    "    def compute_allelefreq_region(self, *, i):\n",
    "        # Speed might be improved by using dot prod here, instead of sums\n",
    "        # np.unique was way slower (5x)\n",
    "        geno_dt = self.reg_dt[i]\n",
    "        n, p_blk = sda.val.shape\n",
    "        sst_df = geno_dt['sst_df'].copy()\n",
    "        cnt0   = np.sum(sda.val==0, axis=0)\n",
    "        cnt1   = np.sum(sda.val==1, axis=0)\n",
    "        cnt2   = np.sum(sda.val==2, axis=0)\n",
    "        cntnan = np.sum(np.isnan(sda.val), axis=0)\n",
    "        assert np.allclose(cnt0 + cnt1 + cnt2 + cntnan, n)\n",
    "        sst_df['altcnt=0']   = cnt0\n",
    "        sst_df['altcnt=1']   = cnt1\n",
    "        sst_df['altcnt=2']   = cnt2\n",
    "        sst_df['altcnt=nan'] = cntnan\n",
    "        sst_df['altfreq']    = (cnt1 + cnt2)/(n - cntnan)\n",
    "        sst_df['missfreq']   = 1 - cntnan/n\n",
    "        return sst_df\n",
    "    \n",
    "    def compute_ldscores_region(self, *, i):\n",
    "        sst_df = self.reg_dt[i]['sst_df'].copy()\n",
    "        L = self.get_left_linkage_region(i=i)\n",
    "        D = self.get_auto_linkage_region(i=i)\n",
    "        R = self.get_right_linkage_region(i=i)\n",
    "        for k, j in enumerate(sst_df.index):\n",
    "            slds = np.sum(L[k]**2) + np.sum(D[k]**2) + np.sum(R[k]**2)\n",
    "            sst_df.loc[j, 'lds'] = np.sqrt(slds)\n",
    "        return sst_df\n",
    "        \n",
    "\n",
    "    ############################\n",
    "    ## Retrieve: ###############\n",
    "    \n",
    "    # Local Linkage: ############\n",
    "    \n",
    "    def retrieve_linkage_allregions_auto(self, compute_sumstats=False):\n",
    "        for i, geno_dt in self.reg_dt.items():\n",
    "            geno_dt['D'] = self.compute_linkage_sameregion(i=i)\n",
    "            if compute_sumstats:\n",
    "                geno_dt['beta_mrg'] = self.compute_sumstats_region(i=i)\n",
    "    \n",
    "    def retrieve_linkage_allregions_shiftwindow(self):\n",
    "        for i, geno_dt in self.reg_dt.items():\n",
    "            print(f'Processing region #{i} on chr{geno_dt[\"chrom\"]}', end='\\r') if self.verbose else None\n",
    "            self.retrieve_linkage_region_shiftwindow(i=i)\n",
    "        if self.clear_xda:\n",
    "            self.clear_all_xda()\n",
    "\n",
    "    def retrieve_linkage_selectedregions_shiftwindow(self, *, filter_i_lst):\n",
    "        filter_i_set = set(filter_i_lst)\n",
    "        for i in self.reg_dt.keys():\n",
    "            geno_dt = self.reg_dt[i]\n",
    "            if i in filter_i_set:\n",
    "                print(f'Processing region #{i} on chr{geno_dt[\"chrom\"]}', end='\\r') if self.verbose else None\n",
    "                self.retrieve_linkage_region_shiftwindow(i=i)\n",
    "            elif self.compute_sumstats:  # Padding of skipped regions\n",
    "                geno_dt['beta_mrg'] = np.zeros((geno_dt['n_snps_reg'], 1), dtype=self.dtype)\n",
    "        if self.clear_xda:\n",
    "            self.clear_all_xda()\n",
    "\n",
    "    def retrieve_linkage_region_shiftwindow(self, *, i):\n",
    "        shift = self.shift; cm = self.cm\n",
    "        compute_sumstats = self.compute_sumstats\n",
    "        geno_dt = self.reg_dt[i]\n",
    "        if 'L' in geno_dt.keys():\n",
    "            if 'D' in geno_dt.keys():\n",
    "                if 'R' in geno_dt.keys():\n",
    "                    return None  # everything is done now.\n",
    "                \n",
    "        if (shift > 0):\n",
    "            L_lst = []\n",
    "            R_lst = []\n",
    "            for cur_shift in range(1, shift + 1):\n",
    "                L_lst.append(self.compute_linkage_shiftregion(i=i, shift=-cur_shift))\n",
    "                R_lst.append(self.compute_linkage_shiftregion(i=i, shift=cur_shift))\n",
    "\n",
    "            # Store Linkage in geno_dt\n",
    "            geno_dt['L'] = np.concatenate(L_lst[::-1], axis=1)  # L stands for left\n",
    "            geno_dt['D'] = self.compute_linkage_sameregion(i=i)  # Linkage within region, D is convention from LDpred 1\n",
    "            geno_dt['R'] = np.concatenate(R_lst, axis=1)  # R stands for right\n",
    "\n",
    "            # Indices needed for slicing and dicing matched variables (e.g. beta weights):\n",
    "            geno_dt['start_j_L'] = geno_dt['start_j'] - geno_dt['L'].shape[1]\n",
    "            geno_dt['stop_j_L'] = geno_dt['start_j']\n",
    "            geno_dt['start_j_R'] = geno_dt['stop_j']\n",
    "            geno_dt['stop_j_R'] = geno_dt['stop_j'] + geno_dt['R'].shape[1]\n",
    "            \n",
    "        elif (shift==0) and (cm is None):  # Only same region has to be done.\n",
    "            geno_dt['D'] = self.compute_linkage_sameregion(i=i)\n",
    "        elif (shift==0) and cm > 0:\n",
    "            geno_dt['L'] = self.compute_linkage_cmfromregion(i=i, cm=-cm)\n",
    "            geno_dt['D'] = self.compute_linkage_sameregion(i=i)\n",
    "            geno_dt['R'] = self.compute_linkage_cmfromregion(i=i, cm=cm)\n",
    "            \n",
    "            # Indices needed for slicing and dicing matched variables (e.g. beta weights):\n",
    "            geno_dt['start_j_L'] = geno_dt['start_j'] - geno_dt['L'].shape[1]\n",
    "            geno_dt['stop_j_L'] = geno_dt['start_j']\n",
    "            geno_dt['start_j_R'] = geno_dt['stop_j']\n",
    "            geno_dt['stop_j_R'] = geno_dt['stop_j'] + geno_dt['R'].shape[1]\n",
    "            \n",
    "        if compute_sumstats:\n",
    "            self.retrieve_sumstats_region(i=i)\n",
    "\n",
    "    # Global Linkage: #######\n",
    "    \n",
    "    def retrieve_linkage_allregions_global(self):\n",
    "        for i, geno_dt in self.reg_dt.items():\n",
    "            print(f'Processing region #{i} on chr{geno_dt[\"chrom\"]}', end='\\r') if self.verbose else None\n",
    "            self.retrieve_linkage_region_global(i=i)\n",
    "\n",
    "    def retrieve_linkage_selectedregions_global(self, *, filter_i_lst):\n",
    "        raise NotImplementedError('Contact dev.')\n",
    "\n",
    "    def retrieve_linkage_region_global(self, *, i):\n",
    "        geno_dt = self.reg_dt[i]\n",
    "        if 'Z' in geno_dt.keys():\n",
    "            return None  # All is done already.\n",
    "        geno_dt['Z'] = self.compute_linkage_globalregion(i=i)\n",
    "        \n",
    "    def retrieve_linkage_allregions_all(self):\n",
    "        self.retrieve_linkage_allregions_shiftwindow()\n",
    "        if self.grd is not None:\n",
    "            self.retrieve_linkage_allregions_global()\n",
    "\n",
    "        \n",
    "    # SumStat: ##############\n",
    "\n",
    "    def retrieve_sumstats_allregions(self):\n",
    "        for i, geno_dt in self.reg_dt.items():\n",
    "            self.retrieve_sumstats_region(i=i)\n",
    "            \n",
    "    def retrieve_sumstats_region(self, *, i):\n",
    "        geno_dt = self.reg_dt[i] \n",
    "        sst_df  = geno_dt['sst_df']\n",
    "        if 'beta_mrg' in geno_dt.keys():\n",
    "            return None # Sumstat present to ne need to compute anything.\n",
    "        geno_dt['beta_mrg'] = self.compute_sumstats_region(i=i)\n",
    "        if not 'beta_mrg' in sst_df.columns:\n",
    "            geno_dt['sst_df']['beta_mrg'] = geno_dt['beta_mrg']\n",
    "            \n",
    "    def retrieve_betamrg_region(self, *, i):\n",
    "        geno_dt = self.reg_dt[i] \n",
    "        sst_df  = geno_dt['sst_df']\n",
    "        if 'beta_mrg' in geno_dt.keys():\n",
    "            return None # Sumstat present to ne need to compute anything.\n",
    "        geno_dt['beta_mrg'] = self.compute_sumstats_region(i=i)\n",
    "        if not 'beta_mrg' in sst_df.columns:\n",
    "            geno_dt['sst_df']['beta_mrg'] = geno_dt['beta_mrg']\n",
    "            \n",
    "    def retrieve_ldscores_allregions(self):\n",
    "        for i, geno_dt in self.reg_dt.items():\n",
    "            self.retrieve_ldscores_region(i=i)\n",
    "            \n",
    "    def retrieve_ldscores_region(self, *, i):\n",
    "        geno_dt = self.reg_dt[i]\n",
    "        sst_df = geno_dt['sst_df']\n",
    "        if not 'lds' in sst_df.columns:\n",
    "            newsst_df = self.compute_ldscores_region(i=i)\n",
    "            geno_dt['sst_df'] = newsst_df\n",
    "        if self.clear_linkage:\n",
    "            self.clear_linkage_region(i=i)\n",
    "            \n",
    "            \n",
    "    # Clearance Function: #####\n",
    "\n",
    "    def clear_all_xda(self):\n",
    "        while len(self.xda_q) != 0:\n",
    "            i_2_rm, key = self.xda_q.popleft()\n",
    "            if i_2_rm == -1:\n",
    "                continue  # Continue to next iter if encountering a padding -1\n",
    "            rmgeno_dt = self.reg_dt[i_2_rm]\n",
    "            self.cur_total_size_in_gb -= getsizeof(rmgeno_dt[key].val) / 1024 ** 3\n",
    "            rmgeno_dt.pop(key)\n",
    "        [self.xda_q.append((-1,'')) for _ in range(5)]  # put 5x -1 in queue\n",
    "        \n",
    "    def clear_linkage_region(self, *, i):\n",
    "        geno_dt = self.reg_dt[i]\n",
    "        filter_lst = ['L','D','R',\n",
    "                      'L_decomp','D_decomp','R_decomp',\n",
    "                      'L_glocal','D_glocal','R_glocal',\n",
    "                      'Z']\n",
    "        key_lst = list(geno_dt.keys())\n",
    "        for key in key_lst:\n",
    "            if key in filter_lst:\n",
    "                geno_dt.pop(key)\n",
    "        \n",
    "\n",
    "    ############################\n",
    "    ## Get: ####################\n",
    "\n",
    "    def get_auto_linkage_region(self, *, i):\n",
    "        return self.get_specificied_linkage_region(i=i, shiftletter='D')\n",
    "\n",
    "    def get_left_linkage_region(self, *, i):\n",
    "        return self.get_specificied_linkage_region(i=i, shiftletter='L')\n",
    "\n",
    "    def get_right_linkage_region(self, *, i):\n",
    "        return self.get_specificied_linkage_region(i=i, shiftletter='R')\n",
    "\n",
    "    def get_specificied_linkage_region(self, *, i, shiftletter):\n",
    "        try:\n",
    "            return self.reg_dt[i][shiftletter]\n",
    "        except KeyError as e:\n",
    "            if self.allow_onthefly_linkage_gen:\n",
    "                if '_glocal' in shiftletter:\n",
    "                    self.retrieve_linkage_region_glocalshiftwindow(i=i)\n",
    "                elif shiftletter in 'LDR':\n",
    "                    self.retrieve_linkage_region_shiftwindow(i=i)\n",
    "                elif shiftletter == 'Z':\n",
    "                    self.retrieve_linkage_region_global(i=i)\n",
    "                else:\n",
    "                    raise Exception(f'shiftletter={shiftletter}, on-the-fly loading not possible ')\n",
    "                try:\n",
    "                    return self.reg_dt[i][shiftletter]\n",
    "                except Exception as e:\n",
    "                    print('Fail eventough trying to do on the spot recovery.')\n",
    "                    raise e\n",
    "            else:\n",
    "                raise NotImplementedError('on-the-fly compute blocked, enable if desired')\n",
    "            ################################################################## NOOOW NEEDED FOR PPB, SWAPPING NEEDED\n",
    "            # Here respective retrieval should be activated\n",
    "            #return self.reg_dt[i][shiftletter]\n",
    "\n",
    "    def get_auto_range_region(self, *, i):\n",
    "        return self.reg_dt[i]['start_j'], self.reg_dt[i]['stop_j']\n",
    "\n",
    "    def get_left_range_region(self, *, i):\n",
    "        return self.reg_dt[i]['start_j_L'], self.reg_dt[i]['stop_j_L']\n",
    "\n",
    "    def get_right_range_region(self, *, i):\n",
    "        return self.reg_dt[i]['start_j_R'], self.reg_dt[i]['stop_j_R']\n",
    "\n",
    "    # Linkage Utility functions: sda, gda, gda\n",
    "    def get_sda(self, *, i):\n",
    "        geno_dt = self.reg_dt[i]\n",
    "        if 'sda' in geno_dt.keys():\n",
    "            return geno_dt['sda']\n",
    "        else:\n",
    "            if 'srd' in geno_dt.keys():\n",
    "                sda = geno_dt['srd'].read(dtype=self.dtype)\n",
    "                sda, stansda = sda.standardize(standardizer=geno_dt['stansda'], return_trained=True)\n",
    "                geno_dt['sda'] = sda\n",
    "                geno_dt['stansda'] = stansda\n",
    "                \n",
    "                if 'loaded_sda' in geno_dt.keys():\n",
    "                    self.reloaded_xda_cnt += 1\n",
    "                    if self.reloaded_xda_cnt in [5, 20, 100, 400]:\n",
    "                        warnings.warn(\n",
    "                            f'Reloaded sda for the {self.reloaded_xda_cnt}\\'th time. This causes memory swapping,'\n",
    "                            ' that might make the computation of linkage quite slow.'\n",
    "                            'Probably because memory limits and/or linkage size.')\n",
    "                # Size determination and accounting:\n",
    "                geno_dt['loaded_sda']=True\n",
    "                self.cur_total_size_in_gb += getsizeof(sda.val) / 1024 ** 3\n",
    "                self.xda_q.append((i,'sda'))  # put respective i in queue.\n",
    "                while self.cur_total_size_in_gb > self.gb_size_limit:  # Keep removing till size is ok\n",
    "                    i_2_rm, key = self.xda_q.popleft()\n",
    "                    if i_2_rm == -1:\n",
    "                        continue  # Continue to next iter if encountering a padding -1\n",
    "                    rmgeno_dt = self.reg_dt[i_2_rm]\n",
    "                    self.cur_total_size_in_gb -= getsizeof(rmgeno_dt[key].val) / 1024 ** 3\n",
    "                    rmgeno_dt.pop(key)\n",
    "                    if len(self.xda_q) <= 4:\n",
    "                        raise Exception('The memory footprint of current settings is too high, '\n",
    "                                        'reduce blocksize and/or correction windows or increase memory limits.')\n",
    "                return sda\n",
    "            else:\n",
    "                raise Exception(f'No srd or sda found in region i={i}, this is not supposed to happen.')\n",
    "\n",
    "    def get_pda(self):\n",
    "        if not hasattr(self, 'pda'):\n",
    "            pda = self.prd.read(dtype=self.dtype)\n",
    "            pda, self.stanpda = pda.standardize(return_trained=True,\n",
    "                            standardizer=self.pda_Standardizer())\n",
    "            self.pda = pda\n",
    "        return self.pda\n",
    "\n",
    "    def get_beta_marginal_full(self):\n",
    "        beta_mrg_lst = []\n",
    "        for i, geno_dt in self.reg_dt.items():\n",
    "            beta_mrg_lst.append(geno_dt['beta_mrg'])\n",
    "        beta_mrg_full = np.concatenate(beta_mrg_lst)\n",
    "        return beta_mrg_full\n",
    "\n",
    "    def get_beta_marginal_region(self, *, i):\n",
    "        return self.reg_dt[i]['beta_mrg']\n",
    "\n",
    "    def get_cur_sumstats_dataframe(self):\n",
    "        sst_df_lst = []\n",
    "        for i, geno_dt in self.reg_dt.items():\n",
    "            #if self.verbose: print(f'computing sumstats for region {i}', end='\\r')\n",
    "            #self.retrieve_sumstats_region(i=i)\n",
    "            sst_df = geno_dt['sst_df']\n",
    "            sst_df_lst.append(sst_df)\n",
    "        sst_df = pd.concat(sst_df_lst, axis=0)\n",
    "        return sst_df\n",
    "\n",
    "    # Standardisation Functionality:\n",
    "    def get_combined_unit_stansda(self):\n",
    "        if hasattr(self, 'stansda'):\n",
    "            if type(self.stansda) is UnitTrained:\n",
    "                return self.stansda\n",
    "\n",
    "        standardizer_list = []\n",
    "        for i, geno_dt in self.reg_dt.items():\n",
    "            if 'stansda' in geno_dt.keys():\n",
    "                if type(geno_dt['stansda']) is UnitTrained:\n",
    "                    standardizer_list.append(geno_dt['stansda'])\n",
    "\n",
    "        test = np.all([type(stan) is UnitTrained for stan in standardizer_list])\n",
    "        assert test\n",
    "        sid = np.concatenate([stan.sid for stan in standardizer_list])\n",
    "\n",
    "        test = np.unique(sid).shape[0] == sid.shape[0]\n",
    "        assert test\n",
    "\n",
    "        stats = np.concatenate([stan.stats for stan in standardizer_list], dtype=self.dtype)\n",
    "        combined_unit_standardizer = UnitTrained(sid, stats)\n",
    "        self.stansda = combined_unit_standardizer\n",
    "        return combined_unit_standardizer\n",
    "\n",
    "    def get_combined_stansda(self):\n",
    "        if self.always_unit:\n",
    "            return self.get_combined_unit_stansda()\n",
    "        else:\n",
    "            raise NotImplementedError('contact dev')\n",
    "\n",
    "\n",
    "class LinkageData(BaseLinkageData):\n",
    "    pass\n",
    "\n",
    "class DelayedLinkageData():\n",
    "    def __init__(self, *, generating_fun, args=None, kwargs=None, test=False):\n",
    "        self.generating_fun = generating_fun\n",
    "        self.args = args if args is not None else list()\n",
    "        self.kwargs = kwargs if kwargs is not None else dict()\n",
    "        assert test is False  # Later we can make a pre testing thing.\n",
    "        self.test = test\n",
    "\n",
    "    def init(self):\n",
    "        linkdata = self.generating_fun(*self.args, **self.kwargs)\n",
    "        assert type(linkdata) is LinkageData\n",
    "        return linkdata\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Utility fun:\n",
    "def nonenan_casting_and_copy_fun(dt):\n",
    "    dt = copy.deepcopy(dt)\n",
    "    new_dt = dict()\n",
    "    for key, item in dt.items():\n",
    "        if (item == 'none') or np.isnan(item):\n",
    "            item = None\n",
    "            \n",
    "        new_dt[key] = item\n",
    "    return new_dt\n",
    "\n",
    "def make_pgs_df(dn='../results/betas/'):\n",
    "\n",
    "    # Load all PGS's:\n",
    "    tic()\n",
    "    sbayesr_df = pd.read_csv(dn + 'final/sbayesr.csv').add_prefix('sbayesr_'); toc(1)\n",
    "    prscs_df  = pd.read_csv(dn + 'final/prscs.csv').add_prefix('prscs_'); toc(2)\n",
    "    ldpred2_df  = pd.read_csv(dn + 'ldpred2/ldpred2-selectpred.csv').add_prefix('ldpred2_'); toc(3)\n",
    "    lassosum_df = pd.read_csv(dn + 'lassosum/lassosum-pseudobetas.csv').add_prefix('lassosum_');\n",
    "    \n",
    "    # Combine into one big pgs dataframe:\n",
    "    pgs_df = pd.concat([sbayesr_df, prscs_df, ldpred2_df, lassosum_df], axis=1); toc(4)\n",
    "    \n",
    "    return pgs_df\n",
    "\n",
    "\n",
    "# Make Phenos:\n",
    "def mapperfun(arg):\n",
    "    if 'sim_' in arg:\n",
    "        exitstr = '.'+'.'.join((arg.split('.')[-2:]))\n",
    "        return 'SIM' + re.search(\"_i=\\\\s*(.*?)\\\\s*_.\", arg).group(1) + exitstr\n",
    "    else:\n",
    "        return arg\n",
    "mapper_dt = dict()\n",
    "def make_pheno_df(dn = '../lnk/data/ukbb/imp/pheno/', fold='test'):\n",
    "    \n",
    "    # Load Pheno data into dataframe:\n",
    "    ddf = dd.read_table(dn+f'*.{fold}.pheno', header=None, include_path_column=True)\n",
    "    df = ddf.compute()\n",
    "\n",
    "    # Process:\n",
    "    df['path'] = df['path'].str.split('/').str[-1].str.replace('.pheno','')\n",
    "    df = df.rename(columns={0:'fid', 1:'iid', 2:'pheno'})\n",
    "    df['iid'] = df['iid'].astype(str)\n",
    "    df = df.set_index(['fid','iid']); ori_index = df.index\n",
    "    df = pd.concat({key: sub_df['pheno'] for key, sub_df in df.groupby('path')}, axis=1, join='inner')\n",
    "    assert np.all(df.index == ori_index[:len(df.index)])\n",
    "    mapper_df = pd.DataFrame(df.columns.to_series().apply(mapperfun), index=df.columns, columns=['map'])\n",
    "    mapper_dt[fold] = mapper_df\n",
    "    df.columns = mapper_df['map']\n",
    "    pheno_df = df\n",
    "    \n",
    "    return pheno_df\n",
    "\n",
    "\n",
    "# The setup of xp:\n",
    "def experimental_setup(return_variable=None, trait='Asthma', adj_type='m16', geno_fn='UKBB_imp_HM3', fold='test',\n",
    "                       regdef_key='1blk_shift=0',shift=0, cm=None, random_state=42, gb_size_limit=10, dtype='float32'\n",
    "                       ):\n",
    "    \n",
    "    # Defining Paramters:\n",
    "    cfg = Struct()\n",
    "    cfg_dt = dict()\n",
    "    cfg_dt.update(dict(\n",
    "\n",
    "        test_fn    = f'pheno/{trait}.{adj_type}.{fold}.pheno',\n",
    "        geno_fn    = geno_fn,\n",
    "        base_fn    = '../lnk/data/ukbb/imp/',\n",
    "        regdef_dn  = '../lnk/data/regdef/',\n",
    "        n_pca        = None,\n",
    "        random_state = random_state,\n",
    "        dtype        = dtype\n",
    "        \n",
    "    ))\n",
    "    cfg.update(cfg_dt)\n",
    "    \n",
    "    # Computing full paths:\n",
    "    plink_fn      = cfg.base_fn + cfg.geno_fn\n",
    "    pheno_tst_fn  = cfg.base_fn + cfg.test_fn\n",
    "    regdef_fn     = cfg.regdef_dn + f'regions_{regdef_key}.regdef.tsv'\n",
    "\n",
    "    # Load SNPs filter list & region definition:\n",
    "    regdef_df = pd.read_csv(regdef_fn, delimiter='\\t') \n",
    "    \n",
    "    # Load Genotype data (mostly 'implicit')\n",
    "    bim_df, fam_df = load_bimfam(plink_fn, fil_arr=None)\n",
    "    df = pd.read_csv('../data/ukbb/mapper.csv') # a bunch of ugly stuff to get cM's in\n",
    "    assert np.all(df[['chromosome','marker.ID']].values == bim_df[['chrom','snp']].values)\n",
    "    mapper_df = df\n",
    "    bim_df['cm'] = mapper_df['cm']\n",
    "    tot_srd  = Bed(plink_fn, count_A1=True)\n",
    "    \n",
    "    # Load Pheno & SNP Reader (=implicit loading):\n",
    "    tst_prd  = Pheno(pheno_tst_fn) # Test\n",
    "    tst_srd, tst_prd = pstutil.intersect_apply([tot_srd, tst_prd])\n",
    "    \n",
    "    # Load test linkdata, perfectly prepped for PPBMeasureComputer\n",
    "    tst_linkdata = LinkageData(regdef_df=regdef_df, sst_df=bim_df.copy(), \n",
    "                               bim_df=bim_df, srd=tst_srd, prd=tst_prd, \n",
    "                               grd=None, gda_standardizer=None, gb_size_limit=gb_size_limit,\n",
    "                               compute_sumstats=True, verbose=True, shift=shift, cm=cm, \n",
    "                               allow_onthefly_linkage_gen=True, dtype=cfg.dtype);\n",
    "    \n",
    "    if return_variable is None: # Return everything\n",
    "        return locals()\n",
    "    else:\n",
    "        return locals()[return_variable]\n",
    "\n",
    "    \n",
    "nrows = None\n",
    "big_dt = dict()\n",
    "for fn in tqdm(glob.glob('../results/betas/*/*.csv')):\n",
    "    df = pd.read_csv(fn, nrows=nrows)\n",
    "#     df = pd.read_csv(fn)\n",
    "    for col in df.columns:\n",
    "        big_dt[col] = df[col].values.astype('float32')\n",
    "        \n",
    "\n",
    "df = pd.DataFrame(big_dt.keys())\n",
    "bnfo_df = df[0].str.split('_').apply(pd.Series)\n",
    "bnfo_df.columns = ['method','mtype','params', 'pheno']\n",
    "bnfo_df = bnfo_df[~bnfo_df.params.str.contains('sp=yes')]\n",
    "bnfo_df = bnfo_df.sort_values(['pheno','method','mtype'])\n",
    "bnfo_df.head(4)\n",
    "\n",
    "# Optional:\n",
    "with open('big_dt.pkl', 'wb') as f:\n",
    "    pickle.dump(big_dt, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('big_dt.pkl', 'rb') as handle:\n",
    "    b = pickle.load(handle)\n",
    "\n",
    "\n",
    "key_lst = bnfo_df.apply(lambda x: '_'.join(x), axis=1).to_list()\n",
    "prepgs_df=pd.DataFrame()\n",
    "for key in tqdm(key_lst):\n",
    "    prepgs_df[key] = big_dt[key]\n",
    "\n",
    "\n",
    "# ## Save Betas in Pst with wide matrix\n",
    "xp_dt = experimental_setup(geno_fn='UKBB_imp_HM3.valid', shift=1, fold='val', gb_size_limit=100.)\n",
    "modelstr_arr = prepgs_df.columns.values.astype(str)\n",
    "sid = xp_dt['tst_linkdata'].srd.sid\n",
    "pstda = PstData(row=modelstr_arr, col=sid, val=prepgs_df.values.T)\n",
    "PstHdf5.write('../results/betas/final/all-betas.pst.h5', pstda, col_major=True)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate PGS with PPB Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics Computers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from pysnptools.standardizer import UnitTrained\n",
    "\n",
    "locals_dt = dict()    \n",
    "\n",
    "\n",
    "class PrivacyPreservingMetricsComputer():\n",
    "    \n",
    "    def __init__(self, *, linkdata, brd, s, Bm, dtype='float32', cov_method='local', \n",
    "                 clear_linkage=True, verbose=True):\n",
    "        \n",
    "        self.linkdata   = linkdata\n",
    "        self.brd        = brd\n",
    "        assert (np.isnan(s).sum()+np.isinf(s).sum()) == 0\n",
    "        self.s          = s\n",
    "        self.Bm         = Bm \n",
    "        self.dtype      = dtype\n",
    "        self.cov_method = cov_method\n",
    "        self.clear_linkage = clear_linkage\n",
    "        self.verbose    = verbose\n",
    "        \n",
    "        self._do_global = False\n",
    "        self._do_local  = False\n",
    "        if cov_method == 'local': # Local Residualized Marginals\n",
    "            self._do_local  = True\n",
    "        elif cov_method == 'global': # Global Residualized Marginals\n",
    "            self._do_global = True\n",
    "        elif cov_method == 'glocal': # Global Local Residualized Marginals\n",
    "            self._do_global = True\n",
    "            self._do_local  = True\n",
    "        else:\n",
    "            raise Exception(f'Option not recognized: \\'{cov_method}\\' ')\n",
    "            \n",
    "    def evaluate(self, debug=False):\n",
    "        \n",
    "        # Load and init variables:\n",
    "        linkdata = self.linkdata\n",
    "        #linkdata = self.linkdata.init() # init, in case required.\n",
    "        brd = self.brd; s = self.s; Bm = self.Bm\n",
    "        bCb = 0.; BmBt = 0.\n",
    "        info_dt = dict()\n",
    "\n",
    "        # Cycle through the blocks:\n",
    "        for i, geno_dt in tqdm(linkdata.reg_dt.items()):\n",
    "            if self.verbose: print(f'PPB: Processing region {i}', end='\\r')\n",
    "\n",
    "            # Ready the LD:\n",
    "            L = linkdata.get_left_linkage_region(i=i)\n",
    "            D = linkdata.get_auto_linkage_region(i=i)\n",
    "            R = linkdata.get_right_linkage_region(i=i)\n",
    "            lr = linkdata.get_left_range_region(i=i)\n",
    "            ar = linkdata.get_auto_range_region(i=i)\n",
    "            rr = linkdata.get_right_range_region(i=i)\n",
    "\n",
    "            # Ready The Weights:\n",
    "            B_L = brd[:,lr[0]:lr[1]].read().val.astype(self.dtype).T\n",
    "            B_D = brd[:,ar[0]:ar[1]].read(dtype=self.dtype).val.T\n",
    "            B_R = brd[:,rr[0]:rr[1]].read(dtype=self.dtype).val.T\n",
    "            B_L = s[lr[0]:lr[1]]*B_L\n",
    "            B_D = s[ar[0]:ar[1]]*B_D\n",
    "            B_R = s[rr[0]:rr[1]]*B_R\n",
    "\n",
    "            # Do the computation:\n",
    "            CB = L.dot(B_L) + D.dot(B_D) + R.dot(B_R)\n",
    "            bCb += (B_D*CB).sum(axis=0)\n",
    "            BmBt += (B_D.T.dot(Bm.iloc[ar[0]:ar[1],:])).T\n",
    "            info_dt[i] = dict(shapeL=L.shape, shapeD=D.shape, shapeR=R.shape, \n",
    "                              lr=lr, ar=ar, rr=rr)\n",
    "\n",
    "            # Pruning to minimize memory overhead:\n",
    "            if (i > 0) and self.clear_linkage:\n",
    "                linkdata.clear_linkage_region(i=i-1)\n",
    "            if (i > 38) & debug:\n",
    "                break\n",
    "                return locals()\n",
    "\n",
    "        # Complete resutls:\n",
    "        linkdata.clear_all_xda()\n",
    "        cols = brd.row.astype(str).flatten()\n",
    "        bCb  = pd.DataFrame(bCb[np.newaxis,:], index=['bCb'], columns=cols)\n",
    "        BmBt = pd.DataFrame(BmBt, index=Bm.columns, columns=cols)\n",
    "        ppbr2_df = (BmBt**2)/bCb.loc['bCb']\n",
    "        res_dt = dict(ppbr2_df=ppbr2_df, bCb=bCb, BmBt=BmBt, info_dt=info_dt, s=s)\n",
    "\n",
    "        return res_dt\n",
    "    \n",
    "# locals_dt = dict()\n",
    "class MultiPGSModel():\n",
    "    \n",
    "    def __init__(self, *, brd, unscaled=True, verbose=False, dtype='float32', allow_nan=False):\n",
    "        self.brd   = brd\n",
    "        if hasattr(brd, 'val'):\n",
    "            assert np.sum(np.isnan(brd.val)) == 0 \n",
    "        self.unscaled = unscaled\n",
    "        self.verbose = verbose\n",
    "        self.dtype  = dtype\n",
    "        self.allow_nan = allow_nan\n",
    "        \n",
    "    def predict(self, *, srd, prd=None, n_inchunk=1000, stansda=None):\n",
    "            \n",
    "        # Load that PGS (& optionaly phenos)\n",
    "        brd = self.brd\n",
    "        Yhat = np.zeros((srd.shape[0], brd.shape[0]), dtype=self.dtype)        \n",
    "        assert np.all(brd.col.astype(str) == srd.sid)\n",
    "        if prd: \n",
    "            assert np.all(srd.iid == prd.iid)\n",
    "            pda   = prd.read(dtype=self.dtype).standardize()\n",
    "            Ytru  = pda.val\n",
    "            Bm    = np.zeros((srd.shape[1], Ytru.shape[1])) + np.nan\n",
    "        \n",
    "        # Loop through Genome:\n",
    "        stansda_lst = []; start=0\n",
    "        for start in tqdm(range(0, srd.shape[1], n_inchunk)):\n",
    "            stop = min(start+n_inchunk, srd.shape[1])\n",
    "            sda, stansda = srd[:,start:stop].read(dtype=self.dtype).standardize(return_trained=True)\n",
    "            X = sda.val\n",
    "            s = stansda.stats[:,1][:,np.newaxis]; s[np.isinf(s)] = 1\n",
    "            B = s*brd[:,start:stop].read(dtype=self.dtype).val.T\n",
    "            Yhat += X@B\n",
    "            if prd: Bm[start:stop] = X.T@Ytru\n",
    "            stansda_lst.append(stansda)\n",
    "            \n",
    "        if prd:    \n",
    "            Bm = Bm/Ytru.shape[0]\n",
    "            if not self.allow_nan: assert np.isnan(Bm).sum() == 0\n",
    "            if not self.allow_nan: assert np.isnan(Bm).sum() == 0\n",
    "            Bm = pd.DataFrame(Bm, index=srd.sid, columns=prd.col)\n",
    "            Ytru = pd.DataFrame(Ytru, # Make Ytru a proper dataframe\n",
    "                index=pd.MultiIndex.from_arrays(prd.iid.T, names=('fid','iid')),\n",
    "                columns=prd.col)\n",
    "        else:\n",
    "            Ytru=None; Bm=None\n",
    "            \n",
    "        # Combine Standardizers:\n",
    "        sid     = np.concatenate([stan.sid   for stan in stansda_lst])\n",
    "        assert  np.unique(sid).shape[0] == sid.shape[0]\n",
    "        stats   = np.concatenate([stan.stats for stan in stansda_lst])\n",
    "        stansda = UnitTrained(sid, stats)   \n",
    "        s = stansda.stats[:,1][:,np.newaxis]; s[np.isinf(s)] = 1\n",
    "        \n",
    "        # Create Yhat dataframe:\n",
    "        Yhat  = pd.DataFrame(\n",
    "            data    = Yhat, \n",
    "            index   = pd.MultiIndex.from_arrays(srd.iid.T, names=('fid','iid')),\n",
    "            columns = self.brd.row.astype(str)\n",
    "        ); assert Yhat.isna().sum().sum() == 0\n",
    "        \n",
    "        res_dt = dict(Yhat=Yhat, Bm=Bm, brd=brd, Ytru=Ytru, stansda=stansda, s=s)\n",
    "        \n",
    "        return locals()\n",
    "    \n",
    "    def run(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "brd = PstHdf5('../results/betas/final/all-betas.pst.h5')\n",
    "ind = ['auto' in str(elem) for elem in brd.row]\n",
    "bda = brd[ind,:].read()\n",
    "pgs_df = pd.DataFrame(bda.val.T, index=bda.col.astype(str), columns=bda.row.astype(str))\n",
    "pheno_df = make_pheno_df(fold='test')\n",
    "\n",
    "\n",
    "\n",
    "# [chaindt(minidt for minidt in dt.values()) for dt in params_lst]\n",
    "def nans2nones(in_df):\n",
    "    isna = in_df.isna().values\n",
    "    vals = in_df.values; vals[isna] = None\n",
    "    return pd.DataFrame(vals, index=in_df.index, columns=in_df.columns)\n",
    "\n",
    "\n",
    "ld_lst = [\n",
    "    dict(cm=0.01, shift=0), #0\n",
    "    dict(cm=2.0,  shift=0), #3\n",
    "    dict(cm=10.0, shift=0), #6\n",
    "]\n",
    "\n",
    "fold_lst = [\n",
    "    dict(fold='val', geno_fn='UKBB_imp_HM3.val'),\n",
    "    dict(fold='test', geno_fn='UKBB_imp_HM3')\n",
    "]\n",
    "\n",
    "dtype_lst = [\n",
    "    dict(dtype='float32'),\n",
    "    dict(dtype='float64')\n",
    "]\n",
    "\n",
    "dt = dict(\n",
    "    ld    = ld_lst,\n",
    "    fold  = fold_lst,\n",
    "    dtype = dtype_lst\n",
    ")\n",
    "\n",
    "params_lst = list(ParameterGrid(dt))\n",
    "df = pd.DataFrame(params_lst)\n",
    "param_df = pd.concat([df[cols].apply(lambda x: pd.Series(x, dtype='object')) for cols in df.columns], axis=1)\n",
    "param_df = nans2nones(param_df)\n",
    "# param_df.T.to_dict()\n",
    "param_df\n",
    "\n",
    "\n",
    "# # Define Internal Executor:\n",
    "debug = False; print('debug =', debug)\n",
    "folder = \"../lnk/menno/log_test/%j\"\n",
    "executor = submitit.AutoExecutor(folder=folder)\n",
    "executor.update_parameters(slurm_mem='119G', cpus_per_task=15, slurm_time='11:54:00',\n",
    "                           slurm_additional_parameters={'account': 'NCRR'})\n",
    "\n",
    "if debug:\n",
    "    make_brd_df = lambda : brd \n",
    "else:\n",
    "    def make_brd_df(fn='../results/betas/final/all-betas.pst.h5'):\n",
    "        brd = PstHdf5(fn)\n",
    "        return brd\n",
    "def debug_make_pheno_df(fold='test'):\n",
    "    return pheno_df\n",
    "wrapped_make_pheno_df = debug_make_pheno_df if debug else make_pheno_df\n",
    "\n",
    "job_dt = dict()\n",
    "for i, cfg_dt in param_df.T.to_dict().items(): \n",
    "\n",
    "    def ppb_fun():\n",
    "        xp_dt = experimental_setup(geno_fn=cfg_dt['geno_fn'], fold=cfg_dt['fold'], shift=cfg_dt['shift'], gb_size_limit=19., cm=cfg_dt['cm'], dtype=cfg_dt['dtype']) \n",
    "        pheno_df = wrapped_make_pheno_df(fold=cfg_dt['fold'])\n",
    "        brd = make_brd_df()\n",
    "\n",
    "        tst_prd = Pheno(dict(iid=pheno_df.index.to_frame()[['fid','iid']].values.astype(str),\n",
    "                   vals=pheno_df.values, header=list(pheno_df.columns)))\n",
    "        cur_srd, cur_prd = pstutil.intersect_apply([xp_dt['tst_srd'], tst_prd])\n",
    "\n",
    "        pgm = MultiPGSModel(brd=brd, verbose=True, dtype=cfg_dt['dtype'])\n",
    "        pred_dt = pgm.predict(srd=cur_srd, prd=cur_prd)\n",
    "\n",
    "        linkdata = xp_dt['tst_linkdata']\n",
    "        mc = PrivacyPreservingMetricsComputer(linkdata=linkdata, brd=brd, s=pred_dt['s'], Bm=pred_dt['Bm'], cov_method='local', dtype=cfg_dt['dtype'], clear_linkage=False)\n",
    "        mcres_dt = mc.evaluate(debug=debug)\n",
    "        \n",
    "        C = corr(pred_dt['Ytru'].astype(cfg_dt['dtype']), pred_dt['Yhat'].astype(cfg_dt['dtype']))\n",
    "        \n",
    "        res_dt = dict(brd=pred_dt['brd'], C=C,\n",
    "                      Bm=pred_dt['Bm'], stansda=pred_dt['stansda'], \n",
    "                      mcres_dt=mcres_dt, linkdata=linkdata)\n",
    "        \n",
    "        return res_dt\n",
    "                           \n",
    "    job_dt[i] = executor.submit(ppb_fun)\n",
    "    print('submitted: ', i, end='\\r')\n",
    "print('done')\n",
    "eval_job_dt = job_dt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tic(); iprev=-1\n",
    "dtype_Y = 'float64'\n",
    "res_dt = dict()\n",
    "for i, job in eval_job_dt.items():\n",
    "    try:\n",
    "        if job.done():\n",
    "            res_dt[i] = job.result()\n",
    "            print(f'Job {i} success.  --> ', end='')\n",
    "        else:\n",
    "            hkergkjhegr\n",
    "    except:\n",
    "        print(f'Job {i} failed.', end='')\n",
    "        continue\n",
    "        \n",
    "    if (iprev != -1):\n",
    "        for key in ['Yhat','Ytru', 'Bm']:\n",
    "            try:\n",
    "                isclose = np.allclose(res_dt[iprev][key].values, \n",
    "                                      res_dt[i][key].values)\n",
    "            except:\n",
    "                isclose = False\n",
    "            if isclose:\n",
    "                res_dt[i][key] = res_dt[iprev][key]\n",
    "                if key == 'Yhat':\n",
    "                    res_dt[i]['C'] = res_dt[iprev]['C']\n",
    "                print(key, end=', ')\n",
    "    \n",
    "    if not ('C' in res_dt[i].keys()):\n",
    "        print('computing C', end='')\n",
    "        res_dt[i]['C'] = corr(res_dt[i]['Ytru'].astype(dtype_Y), res_dt[i]['Yhat'].astype(dtype_Y))\n",
    "    iprev = i; print()\n",
    "toc()\n",
    "\n",
    "\n",
    "\n",
    "if not ('pheno_df_dt' in locals().keys()):\n",
    "    pheno_df_dt = dict()\n",
    "    pheno_df_dt['val'] = make_pheno_df(fold='val')\n",
    "    pheno_df_dt['test'] = make_pheno_df(fold='test')\n",
    "quickload_dt = dict(pheno_df_dt=pheno_df_dt, res_dt=res_dt)\n",
    "\n",
    "\n",
    "# Optional:\n",
    "if 'quickload_dt' in locals().keys():\n",
    "    with open('quickload_dt.pkl', 'wb') as f:\n",
    "        pickle.dump(quickload_dt, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open('quickload_dt.pkl', 'rb') as handle:\n",
    "        quickload_dt = pickle.load(handle)\n",
    "locals().update(quickload_dt)\n",
    "get_ipython().system('echo $TEMP')\n",
    "\n",
    "\n",
    "\n",
    "# Prevalence Stuff:\n",
    "def compute_corfac(K):\n",
    "    zscore = stats.norm.ppf(K)\n",
    "    z2 = stats.norm.pdf(zscore)**2\n",
    "    corfac = K*(1-K)/z2\n",
    "    return corfac\n",
    "\n",
    "def compute_nagelkerkefac(K):\n",
    "    zscore = stats.norm.ppf(K)\n",
    "    z2 = stats.norm.pdf(zscore)**2\n",
    "    corfac = (K**(2*K))*((1-K)**(2*(1-K)))\n",
    "    corfac = 1/(1-corfac)\n",
    "    return corfac\n",
    "\n",
    "def excl(arg,string, axis=1):\n",
    "    return arg.filter(regex=f'^((?!{string}).)*$', axis=axis)\n",
    "\n",
    "\n",
    "# Prevalence:\n",
    "pheno_df = pheno_df_dt['test']\n",
    "prev_df = pheno_df.filter(like='base', axis=1).mean(axis=0).to_frame(name='preval')\n",
    "prev_df['corfac'] = prev_df['preval'].apply(compute_corfac)\n",
    "prev_df['nagelkerke'] = prev_df['preval'].apply(compute_nagelkerkefac)\n",
    "prev_df['flip'] = 1.0/prev_df['preval']\n",
    "prev_df.index = prev_df.index.str.split('.').str[0]\n",
    "pheno_lst = list(prev_df.index.unique())\n",
    "# arr = pheno_df.columns.str.split('.', expand=True).to_frame().values\n",
    "\n",
    "bigres_dt = dict(); adjppbr2_dt={}; adjvanr2_dt={}; mega_dt = {}\n",
    "gb = sizegb(np.random.randn(1000,1000).astype('float32'))\n",
    "# for i_res, item in res_dt.items(): \n",
    "for i_res, job in job_dt.items(): \n",
    "    \n",
    "    try:\n",
    "        res = res_dt[i_res] if i_res in res_dt.keys() else job.result()\n",
    "        print('res', i_res, end=', ')\n",
    "    except:\n",
    "        print('continue, i_res: ', i_res, end=', ')\n",
    "        continue\n",
    "    \n",
    "    # Start stuffs:\n",
    "    fun = job_dt[i_res].submission().function\n",
    "    cfg_dt = fun.__globals__['cfg_dt']\n",
    "    locals().update(res)\n",
    "    info_df = pd.DataFrame(mcres_dt['info_dt']).T\n",
    "    gbs=(info_df.iloc[:,:3].applymap(lambda x: x[0]*x[1]).sum().sum()/1e6)*gb\n",
    "    cfg_dt.update(gbs=gbs)\n",
    "    ppbr2_df = mcres_dt['ppbr2_df']\n",
    "    adjppbr2_df = (ppbr2_df.T*prev_df['corfac'].loc[ppbr2_df.index.str.split('.').str[0]].values)\n",
    "    adjvanr2_df = ((C**2).T*prev_df['corfac'].loc[C.index.str.split('.').str[0]].values)\n",
    "    \n",
    "    # Store stuff:\n",
    "    keys = ['adjvanr2_df', 'adjppbr2_df', 'cfg_dt']\n",
    "    loc = locals()s\n",
    "    bigres_dt[i_res] = { key : loc[key] for key in keys}\n",
    "    cm = cfg_dt['cm']\n",
    "    fold = cfg_dt['fold']\n",
    "    if cm is None:\n",
    "        continue\n",
    "    adjppbr2_dt[(fold, f'cm={cm}')] = adjppbr2_df\n",
    "    adjvanr2_dt[(fold, f'cm={cm}')] = adjvanr2_df\n",
    "    if not (cm is None):\n",
    "        mega_dt[(fold, cm, 'van')] = adjvanr2_df\n",
    "        mega_dt[(fold, cm, 'ppb')] = adjppbr2_df\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tic()\n",
    "df = pd.concat(mega_dt, axis=1)\n",
    "df.columns.names = ['fold','cm','flav','pheno']\n",
    "df.index = df.index.str.split('_', expand=True)\n",
    "df.index.names = ['method','app','params','phenoprs']\n",
    "df = df.stack(['cm','flav'])\n",
    "df.columns = df.columns.get_level_values('pheno').str.split('.', expand=True)\n",
    "df.columns.names = ['pheno','adj','fold']\n",
    "df = df.stack('pheno')\n",
    "# idx_df = df.index.to_frame() # Other option\n",
    "df = df.stack().stack()\n",
    "df.name = 'adjr2'\n",
    "df = df.reset_index()\n",
    "df.pheno = df.pheno.str.replace('S.+\\D(?=[0-9]$)', 'SIM0', regex=True)\n",
    "df.phenoprs = df.phenoprs.str.replace('S.+\\D(?=[0-9]$)', 'SIM0', regex=True)\n",
    "display(df.head())\n",
    "mega_df = df\n",
    "toc()\n",
    "\n",
    "\n",
    "adj = 'm16'\n",
    "fullres_df = mega_df[mega_df.phenoprs == mega_df.pheno]\n",
    "fullres_df.loc[:,'cm'] = fullres_df.cm * 2.0 \n",
    "fullres_df = fullres_df.set_index(fullres_df.columns[:-1].to_list())\n",
    "fullres_df = fullres_df.droplevel('phenoprs')\n",
    "fullres_df = fullres_df.unstack(['adj','flav','fold'])['adjr2']\n",
    "fullres_df = fullres_df.sort_index(axis=1)\n",
    "fullres_df = fullres_df[adj]\n",
    "fullres_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fullres_df contains all the 'normal' and PPB R^2 results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "374.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
