{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinkageData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     46,
     88,
     113,
     168,
     171,
     186,
     189,
     190,
     202,
     214,
     257,
     259,
     268,
     287,
     303,
     309,
     359,
     364,
     375,
     426,
     450,
     484,
     493,
     498,
     513,
     516,
     519,
     522,
     523,
     543,
     546,
     549,
     553,
     568,
     607,
     617,
     625,
     626,
     651,
     654
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../pgsbenchmark/loaders.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../pgsbenchmark/loaders.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "LinkageData\n",
    "durr tst\n",
    "\"\"\"\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import linalg\n",
    "from sys import getsizeof\n",
    "\n",
    "import warnings, importlib, json, os, glob\n",
    "from collections import OrderedDict, deque, defaultdict\n",
    "from pysnptools.standardizer import Unit, UnitTrained\n",
    "import pysnptools as pst\n",
    "# import pysnptools.util as pstutil\n",
    "# from pysnptools.standardizer import UnitTrained\n",
    "# from dataclasses import dataclass\n",
    "\n",
    "class SqrtNinv(Unit):\n",
    "    def __init__(self):\n",
    "        super(SqrtNinv, self).__init__()\n",
    "\n",
    "# @dataclass\n",
    "class BaseLinkageData():\n",
    "\n",
    "    def __init__(self, *, sst_df=None, regdef_df=None, master_dt=None, #There should be sst_df or master_dt\n",
    "                 srd=None, sda_standardizer=Unit,\n",
    "                 prd=None, pda_standardizer=Unit,\n",
    "                 lrd=None, lda_standardizer=None,\n",
    "                 grd=None, gda_standardizer=False,\n",
    "                 \n",
    "                 shift=0, cm=None, _setzero=True,\n",
    "                 \n",
    "                 clear_xda=True, # Refactor with _clear?\n",
    "                 clear_linkage=False,\n",
    "                 compute_sumstats=False,\n",
    "                 calc_allelefreq=False,\n",
    "                 \n",
    "                 _onthefly_retrieval=True,\n",
    "                 _save_vars = ['L','D','R','sst_df'],\n",
    "                 _clear_vars = ['L','D','R'],\n",
    "                 gb_size_limit=10., dtype='float32', verbose=False):\n",
    "        \n",
    "        if True:\n",
    "            # bim and fam df have to be supplied because pysnptools halvely\n",
    "            # implemented these portions of the genetic data into their object\n",
    "            # meaning that srd cannot be relied uppon\n",
    "            excl_lst = ['self','kwg_dt','excl_lst']\n",
    "            kwg_dt = {key: item for key, item in locals().items() if not (key in excl_lst)}\n",
    "            for key, item in locals().items():\n",
    "                if not (key in excl_lst): \n",
    "                    self.__setattr__(key, item)\n",
    "            # New rule: blx have to be created from the inside\n",
    "            # Perhaps later it can be made into a special load instead of a compute\n",
    "\n",
    "            # first-checks & inits:\n",
    "            if cm is not None: assert cm > 0\n",
    "            if lrd is not None: raise NotImplementedError('lrd not possible atm.')\n",
    "            if grd is not None:\n",
    "                assert gda_standardizer or (gda_standardizer is None)\n",
    "            assert type(compute_sumstats) is bool\n",
    "            self.reg_dt = dict()\n",
    "            self.cur_total_size_in_gb = 0.0\n",
    "            self.xda_q = deque()\n",
    "            [self.xda_q.append((-1,'')) for _ in range(5)]  # put 5x -1 in queue\n",
    "            self.reloaded_xda_cnt = 0\n",
    "            self._fn_lst = []\n",
    "\n",
    "            # Checks            \n",
    "            if srd is not None:\n",
    "                assert type(sst_df) is pd.DataFrame\n",
    "                self._check_xrd()\n",
    "                assert isinstance(sst_df, pd.DataFrame)\n",
    "                assert isinstance(regdef_df, pd.DataFrame)\n",
    "                self.init_regions()\n",
    "            elif master_dt is not None:\n",
    "                # Fill attributes in case master_dt is present:\n",
    "                for key, item in master_dt.items():\n",
    "                    setattr(self, key, item)\n",
    "                reg_dt=dict()\n",
    "                for pre_i, geno_dt in self.reg_dt.items(): reg_dt[int(pre_i)] = geno_dt\n",
    "                self.reg_dt = reg_dt # An ugly type conversion hack cause json does not allow i to be integer, but forces it to be a string.\n",
    "            else:\n",
    "                raise Exception('Essentials not present')\n",
    "\n",
    "    def _check_xrd(self):\n",
    "\n",
    "        if self.srd is not None:\n",
    "            assert pst.snpreader.SnpReader in self.srd.__class__.__mro__\n",
    "\n",
    "        if self.prd is not None:\n",
    "            n_start = len(self.prd.iid)\n",
    "            self.srd, self.prd = pst.util.intersect_apply([self.srd, self.prd])\n",
    "            if len(self.prd.iid) != n_start:\n",
    "                warnings.warn('Number of samples do not match up after internal intersection, samples were lost:' \n",
    "                              f'{n_start - len(self.prd.iid)}, start = {n_start}, after_intersection = {len(self.prd.iid)}')\n",
    "\n",
    "        if self.grd is not None:\n",
    "            # Check alignment for now, auto alignment needs work cause iid stuffs:\n",
    "            if self.srd is not None:\n",
    "                if not np.all(self.grd.sid == self.srd.sid):\n",
    "                    raise Exception('snps of grd and srd not matching up, align first,'\n",
    "                                    ' auto align will be implemented later')\n",
    "            else:\n",
    "                raise NotImplementedError('Not sure what to do with grd if no srd is present. not implemented.')\n",
    "        \n",
    "    ###########################\n",
    "    # Regions Administration:\n",
    "    if True:\n",
    "\n",
    "        def init_regions(self):\n",
    "            do_beta_moving = ('beta_mrg' in self.sst_df.columns)\n",
    "            if not do_beta_moving:\n",
    "                warnings.warn('No \\'beta\\' column detected in sst_df! This means that no summary stats were detected.')\n",
    "            else:\n",
    "                assert 'n_eff' in self.sst_df.columns\n",
    "            cur_chrom = None\n",
    "            i = 0; n_snps_cumsum = 0\n",
    "            sst_df_lst = []\n",
    "            for reg_cnt, (_, row) in enumerate(self.regdef_df.iterrows()):\n",
    "                # Move region into specialized dictionary\n",
    "                regid = row['regid'];\n",
    "                chrom = row['chrom']\n",
    "                start = row['start'];\n",
    "                stop  = row['stop']\n",
    "\n",
    "                # Map Variants to region\n",
    "                ind = self.sst_df.chrom == chrom\n",
    "                ind = (self.sst_df['pos'] >= start) & ind\n",
    "                ind = (self.sst_df['pos'] < stop) & ind\n",
    "                sid = self.sst_df['snp'][ind].values\n",
    "                indices = self.srd.sid_to_index(sid)  # if sid not strickly present this will give an error!\n",
    "                n_snps_reg = len(indices)\n",
    "                if n_snps_reg == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    geno_dt = dict(regid=regid,\n",
    "                                   chrom=chrom,\n",
    "                                   start=start,\n",
    "                                   stop=stop,\n",
    "                                   start_j=n_snps_cumsum)\n",
    "                    n_snps_cumsum += n_snps_reg\n",
    "                    geno_dt['stop_j'] = n_snps_cumsum\n",
    "                    sst_df = self.sst_df[ind].copy(); sst_df['i'] = i\n",
    "                    geno_dt['sst_df'] = sst_df\n",
    "                    assert geno_dt['start_j'] == sst_df.index[0]; sst_df_lst.append(sst_df)\n",
    "                    assert geno_dt['stop_j']  == sst_df.index[-1] + 1\n",
    "                    if do_beta_moving:\n",
    "                        geno_dt['beta_mrg'] = geno_dt['sst_df']['beta_mrg'].copy().values[:, np.newaxis]\n",
    "                        assert len(geno_dt['beta_mrg'].shape) == 2\n",
    "                    if self.srd is not None:\n",
    "                        geno_dt['srd'] = self.srd[:, indices]\n",
    "                        geno_dt['stansda'] = self.sda_standardizer() if self.sda_standardizer is not None else None\n",
    "                    else:\n",
    "                        raise NotImplementedError()\n",
    "                    if self.grd is not None:\n",
    "                        geno_dt['grd'] = self.grd[:, indices]\n",
    "                        geno_dt['stangda'] = self.gda_standardizer() if self.gda_standardizer is not None else None\n",
    "                    # Count up if things are actually stored in reg_dt\n",
    "                    self.reg_dt[i] = geno_dt\n",
    "                    i += 1\n",
    "            self.n_snps_total = n_snps_cumsum\n",
    "            sst_df = pd.concat(sst_df_lst, axis=0)\n",
    "            self.sst_df = sst_df\n",
    "\n",
    "        def get_i_list(self):\n",
    "            return list(self.reg_dt.keys())\n",
    "\n",
    "        def _load_all_snpdata(self):\n",
    "            # load all regions\n",
    "            for i, geno_dt in self.reg_dt.items():\n",
    "                sda = geno_dt['srd'].read(dtype=self.dtype)\n",
    "                stansda = sda.train_standardizer(apply_in_place=True,\n",
    "                                                 standardizer=geno_dt['stansda'])\n",
    "                geno_dt['sda'] = sda\n",
    "                geno_dt['stansda'] = stansda\n",
    "\n",
    "    ###########################\n",
    "    ## Compute: ###############\n",
    "\n",
    "    # Local Linkage Stuff: ####\n",
    "    if True:\n",
    "    \n",
    "        def compute_linkage_sameregion(self, *, i):\n",
    "            return self.compute_linkage_shiftregion(i=i, shift=0)\n",
    "\n",
    "        def regions_compatible(self, *, i, j):\n",
    "            try:\n",
    "                if self.reg_dt[i]['chrom'] == self.reg_dt[j]['chrom']:\n",
    "                    res = True\n",
    "                else:\n",
    "                    res = False\n",
    "            except Exception as e:\n",
    "                if (not (i in self.reg_dt.keys())) or (not (j in self.reg_dt.keys())):\n",
    "                    res = False\n",
    "                else:\n",
    "                    raise e\n",
    "            return res\n",
    "\n",
    "        def compute_linkage_shiftregion(self, *, i, shift):\n",
    "            j = i + shift\n",
    "            if self.regions_compatible(i=i, j=j):\n",
    "                self_sda = self.get_sda(i=i)\n",
    "                dist_sda = self.get_sda(i=j)\n",
    "                n = len(self_sda.iid)\n",
    "                S_shift = self_sda.val.T.dot(dist_sda.val) / n\n",
    "                return S_shift\n",
    "            else:\n",
    "                self_sda = self.get_sda(i=i)\n",
    "                return np.zeros((self_sda.val.shape[1], 0))\n",
    "\n",
    "        def compute_linkage_cmfromregion(self, *, i, cm):            \n",
    "            geno_dt = self.reg_dt[i]; lst = []\n",
    "            if cm < 0: # Doing left:\n",
    "                stop_j   = geno_dt['start_j']\n",
    "                cm_left  = geno_dt['sst_df'].loc[stop_j]['cm'] \n",
    "                slc_df = self.sst_df.loc[:stop_j-1]\n",
    "                slc_df = slc_df[slc_df.chrom==geno_dt['chrom']]\n",
    "                slc_df = slc_df[slc_df.cm > (cm_left + cm)]\n",
    "                start_i = slc_df['i'].min()\n",
    "                start_i = -7 if np.isnan(start_i) else start_i\n",
    "                for cur_i in range(start_i, i):\n",
    "                    lst.append(self.compute_linkage_shiftregion(i=i, shift=cur_i-i))\n",
    "                    if start_i == -7: break\n",
    "                L = np.concatenate(lst, axis=1)[:,-slc_df.shape[0]:] # concat & clip\n",
    "                if self._setzero:\n",
    "                    cms_reg    = geno_dt['sst_df']['cm'].values\n",
    "                    cms_distal = slc_df['cm'].values\n",
    "                    cms_L      =  cms_distal[np.newaxis,:] - cms_reg[:,np.newaxis]\n",
    "                    setzero_L  = cms_L < cm\n",
    "                    L[setzero_L] = 0\n",
    "                    assert L.shape == setzero_L.shape\n",
    "                return L\n",
    "            else:\n",
    "                start_j   = geno_dt['stop_j']\n",
    "                cm_right  = geno_dt['sst_df'].loc[start_j-1]['cm']\n",
    "                slc_df = self.sst_df.loc[start_j:]\n",
    "                slc_df = slc_df[slc_df.chrom==geno_dt['chrom']]\n",
    "                slc_df = slc_df[slc_df.cm < (cm_right + cm)]\n",
    "                stop_i = slc_df['i'].max()\n",
    "                stop_i = i+2 if np.isnan(stop_i) else stop_i + 1\n",
    "                for cur_i in range(i+1, stop_i):\n",
    "                    lst.append(self.compute_linkage_shiftregion(i=i, shift=cur_i-i))\n",
    "                R = np.concatenate(lst, axis=1)[:,:slc_df.shape[0]] # concat & clip\n",
    "                if self._setzero:\n",
    "                    cms_reg    = geno_dt['sst_df']['cm'].values\n",
    "                    cms_distal = slc_df['cm'].values\n",
    "                    cms_R     =  cms_distal[np.newaxis,:] - cms_reg[:,np.newaxis]\n",
    "                    setzero_R = cms_R > cm\n",
    "                    R[setzero_R] = 0\n",
    "                    assert R.shape == setzero_R.shape\n",
    "                return R\n",
    "        \n",
    "    # Misc Stuff: #############\n",
    "    if True:\n",
    "    \n",
    "        def compute_sumstats_region(self, *, i):\n",
    "            geno_dt = self.reg_dt[i]\n",
    "            sda = self.get_sda(i=i)\n",
    "            X = sda.val\n",
    "            y = self.get_pda().val\n",
    "            n = len(y)\n",
    "            c_reg = X.T.dot(y) / n\n",
    "            return c_reg   \n",
    "\n",
    "        def compute_allelefreq_region(self, *, i):\n",
    "            # Speed might be improved by using dot prod here, instead of sums\n",
    "            # np.unique was way slower (5x)\n",
    "            geno_dt = self.reg_dt[i]\n",
    "            n, p_blk = sda.val.shape\n",
    "            sst_df = geno_dt['sst_df'].copy()\n",
    "            cnt0   = np.sum(sda.val==0, axis=0)\n",
    "            cnt1   = np.sum(sda.val==1, axis=0)\n",
    "            cnt2   = np.sum(sda.val==2, axis=0)\n",
    "            cntnan = np.sum(np.isnan(sda.val), axis=0)\n",
    "            assert np.allclose(cnt0 + cnt1 + cnt2 + cntnan, n)\n",
    "            sst_df['altcnt=0']   = cnt0\n",
    "            sst_df['altcnt=1']   = cnt1\n",
    "            sst_df['altcnt=2']   = cnt2\n",
    "            sst_df['altcnt=nan'] = cntnan\n",
    "            sst_df['altfreq']    = (cnt1 + cnt2)/(n - cntnan)\n",
    "            sst_df['missfreq']   = 1 - cntnan/n\n",
    "            return sst_df\n",
    "\n",
    "        def compute_ldscores_region(self, *, i):\n",
    "            sst_df = self.reg_dt[i]['sst_df'].copy()\n",
    "            L = self.get_left_linkage_region(i=i)\n",
    "            D = self.get_auto_linkage_region(i=i)\n",
    "            R = self.get_right_linkage_region(i=i)\n",
    "            for k, j in enumerate(sst_df.index):\n",
    "                slds = np.sum(L[k]**2) + np.sum(D[k]**2) + np.sum(R[k]**2)\n",
    "                sst_df.loc[j, 'lds'] = np.sqrt(slds)\n",
    "            return sst_df\n",
    "        \n",
    "    ############################\n",
    "    ## Retrieve: ###############\n",
    "    \n",
    "    # Local Linkage: ############\n",
    "    if True:\n",
    "    \n",
    "        def retrieve_linkage_allregions(self):\n",
    "            for i, geno_dt in self.reg_dt.items():\n",
    "                self.retrieve_linkage_region(i=i)\n",
    "            if self.verbose:   print('\\nDone')\n",
    "            if self.clear_xda: self.clear_all_xda()\n",
    "\n",
    "        def retrieve_linkage_region(self, *, i):\n",
    "\n",
    "            geno_dt = self.reg_dt[i]\n",
    "            if 'store_dt' in geno_dt.keys():\n",
    "                self.load_linkage_region(i=i)\n",
    "            shift = self.shift; cm = self.cm\n",
    "            compute_sumstats = self.compute_sumstats\n",
    "\n",
    "            if 'L' in geno_dt.keys():\n",
    "                if 'D' in geno_dt.keys():\n",
    "                    if 'R' in geno_dt.keys():\n",
    "                        return None  # everything is done now.\n",
    "\n",
    "            if self.verbose: print(f'Computing LD for region #{i} on chr{geno_dt[\"chrom\"]}', end='\\r')\n",
    "            # Refactor: if linkage is only in blocks this code will lead to recomputation...\n",
    "            if (shift > 0):\n",
    "                L_lst = []\n",
    "                R_lst = []\n",
    "                for cur_shift in range(1, shift + 1):\n",
    "                    L_lst.append(self.compute_linkage_shiftregion(i=i, shift=-cur_shift))\n",
    "                    R_lst.append(self.compute_linkage_shiftregion(i=i, shift=cur_shift))\n",
    "\n",
    "                # Store Linkage in geno_dt\n",
    "                geno_dt['L'] = np.concatenate(L_lst[::-1], axis=1)  # L stands for left\n",
    "                geno_dt['D'] = self.compute_linkage_sameregion(i=i)  # Linkage within region, D is convention from LDpred 1\n",
    "                geno_dt['R'] = np.concatenate(R_lst, axis=1)  # R stands for right\n",
    "\n",
    "                # Indices needed for slicing and dicing matched variables (e.g. beta weights):\n",
    "                geno_dt['start_j_L'] = geno_dt['start_j'] - geno_dt['L'].shape[1]\n",
    "                geno_dt['stop_j_L'] = geno_dt['start_j']\n",
    "                geno_dt['start_j_R'] = geno_dt['stop_j']\n",
    "                geno_dt['stop_j_R'] = geno_dt['stop_j'] + geno_dt['R'].shape[1]\n",
    "\n",
    "            elif (shift==0) and (cm is None):  # Only same region has to be done.\n",
    "                geno_dt['D'] = self.compute_linkage_sameregion(i=i)\n",
    "\n",
    "            elif (shift==0) and cm > 0:\n",
    "                geno_dt['L'] = self.compute_linkage_cmfromregion(i=i, cm=-cm)\n",
    "                geno_dt['D'] = self.compute_linkage_sameregion(i=i)\n",
    "                geno_dt['R'] = self.compute_linkage_cmfromregion(i=i, cm=cm)\n",
    "\n",
    "                # Indices needed for slicing and dicing matched variables (e.g. beta weights):\n",
    "                geno_dt['start_j_L'] = geno_dt['start_j'] - geno_dt['L'].shape[1]\n",
    "                geno_dt['stop_j_L'] = geno_dt['start_j']\n",
    "                geno_dt['start_j_R'] = geno_dt['stop_j']\n",
    "                geno_dt['stop_j_R'] = geno_dt['stop_j'] + geno_dt['R'].shape[1]\n",
    "\n",
    "            if compute_sumstats:\n",
    "                self.retrieve_sumstats_region(i=i)\n",
    "              \n",
    "        def load_linkage_allregions(self):\n",
    "            for i, geno_dt in self.reg_dt.items():\n",
    "                self.load_linkage_region(i=i)\n",
    "            if self.verbose: print('\\nDone')\n",
    "            \n",
    "        def load_linkage_region(self, *, i):\n",
    "            geno_dt = self.reg_dt[i]\n",
    "            store_dt = geno_dt['store_dt']\n",
    "            for varname, file_dt in store_dt.items():\n",
    "                module = importlib.import_module('.'.join(file_dt['typestr'].split('.')[:-1]))\n",
    "                cname  = file_dt['typestr'].split('.')[-1]\n",
    "                CurClass = getattr(module, cname) # Retrieves module.submodule.submodule.. etc\n",
    "                curfullfn = os.path.join(self.curdn, file_dt['fn'])\n",
    "                geno_dt[varname] = CurClass(pd.read_hdf(curfullfn, key=file_dt['key']))\n",
    "                if self.verbose: print(f'loading: fn={curfullfn} key={file_dt[\"key\"]}'+' '*50, end='\\r')\n",
    "                \n",
    "        def save(self, fn, keyfmt='ld/chrom{chrom}/i{i}/{varname}', fmt='hdf5', mkdir=False, dn=None):\n",
    "            self.curdn = os.path.dirname(fn) if (dn is None) else dn\n",
    "            fn = os.path.basename(fn) if (dn is None) else fn\n",
    "            if mkdir: os.makedirs(self.curdn, exist_ok=True)\n",
    "            if (fmt != 'hdf5'): raise Exception(f'Only hdf5 file format supported atm, not {fmt}') \n",
    "            for i, geno_dt in self.reg_dt.items():\n",
    "                self.save_linkage_region(i=i, fn=fn)\n",
    "                \n",
    "            # Saving of 'logistical' data for the object\n",
    "            master_lst = [ 'shift', 'cm', '_setzero',\n",
    "             'clear_xda', 'clear_linkage', 'compute_sumstats', 'calc_allelefreq', \n",
    "             '_onthefly_retrieval', '_save_vars', '_clear_vars', \n",
    "             'gb_size_limit', 'dtype', 'verbose', 'n_snps_total']\n",
    "            geno_lst = ['regid','chrom','start','stop','start_j','stop_j',\n",
    "                        'start_j_L', 'stop_j_L', 'start_j_R', 'stop_j_R','store_dt']\n",
    "                \n",
    "            def caster(arg, types):\n",
    "                if np.isscalar(arg):\n",
    "                    if isinstance(arg, np.integer): arg = int(arg)\n",
    "                if type(arg) is int: return int(arg)\n",
    "                assert type(arg) in types\n",
    "                return arg\n",
    "\n",
    "            master_dt = dict(); maxlen = 20\n",
    "            for key in master_lst:\n",
    "                var = getattr(self, key)\n",
    "                if type(var) is list:\n",
    "                    for item in var:\n",
    "                        assert type(item) in (bool, str, float, int)\n",
    "                        if type(item) is str: assert (len(item) < maxlen)\n",
    "                elif type(var) is str:\n",
    "                        assert len(var) < maxlen\n",
    "                master_dt[key] = caster(var, (list, bool, float, int, str))\n",
    "\n",
    "            reg_dt = dict()\n",
    "            for i, geno_dt in self.reg_dt.items():\n",
    "                newgeno_dt = dict()\n",
    "                for key in geno_lst:\n",
    "                    if not (key in geno_dt.keys()): continue\n",
    "                    newgeno_dt[key] = caster(geno_dt[key], (str, int, dict))\n",
    "                reg_dt[i] = newgeno_dt\n",
    "            master_dt['reg_dt'] = reg_dt     \n",
    "            self._fn_lst = list(np.unique(self._fn_lst))\n",
    "            for curfn in self._fn_lst:\n",
    "                pd.DataFrame([json.dumps(master_dt)]).to_hdf(os.path.join(self.curdn, curfn), key='master_dt')\n",
    "            \n",
    "            if self.verbose: print('\\nDone')\n",
    "\n",
    "        def save_linkage_region(self, *, i, fn, keyfmt='ld/chrom{chrom}/i{i}/{varname}'): \n",
    "            # using 'store' instead of 'save' to indicate a connected relationship with \n",
    "            # the files used for this storage.\n",
    "            geno_dt = self.reg_dt[i]\n",
    "            chrom = geno_dt['chrom']\n",
    "            curdn = self.curdn\n",
    "            store_dt = dict() #geno_dt['store_dt']\n",
    "            for varname, var in geno_dt.items():\n",
    "                if varname in self._save_vars:\n",
    "                    curfn  = fn.format(**locals())\n",
    "                    key    = keyfmt.format(**locals())\n",
    "                    var    = geno_dt[varname]\n",
    "                    vartype = type(var)\n",
    "                    if vartype is np.ndarray: vartype = var.dtype.type\n",
    "                    curfullfn = os.path.join(curdn,curfn)\n",
    "                    pd.DataFrame(var).to_hdf(curfullfn, key=key)\n",
    "                    file_dt = dict(fn=curfn, key=key, \n",
    "                                   typestr=vartype.__module__+'.'+vartype.__name__)\n",
    "                    store_dt[varname] = file_dt\n",
    "                    self._fn_lst.append(curfn)\n",
    "                    if self.verbose: print(f'saving: fn={curfullfn} key={key}'+' '*50,end='\\r')\n",
    "            geno_dt['store_dt'] = store_dt\n",
    "                   \n",
    "    # SumStat: ##############\n",
    "    if True:\n",
    "\n",
    "        def retrieve_sumstats_allregions(self):\n",
    "            for i, geno_dt in self.reg_dt.items():\n",
    "                self.retrieve_sumstats_region(i=i)\n",
    "\n",
    "        def retrieve_sumstats_region(self, *, i):\n",
    "            geno_dt = self.reg_dt[i] \n",
    "            sst_df  = geno_dt['sst_df']\n",
    "            if 'beta_mrg' in geno_dt.keys():\n",
    "                return None # Sumstat present so no need to compute anything.\n",
    "            geno_dt['beta_mrg'] = self.compute_sumstats_region(i=i)\n",
    "            if not 'beta_mrg' in sst_df.columns:\n",
    "                geno_dt['sst_df']['beta_mrg'] = geno_dt['beta_mrg']\n",
    "                \n",
    "        retrieve_betamrg_region = retrieve_sumstats_region\n",
    "\n",
    "        def retrieve_ldscores_allregions(self):\n",
    "            for i, geno_dt in self.reg_dt.items():\n",
    "                self.retrieve_ldscores_region(i=i)\n",
    "\n",
    "        def retrieve_ldscores_region(self, *, i):\n",
    "            geno_dt = self.reg_dt[i]\n",
    "            sst_df = geno_dt['sst_df']\n",
    "            if not 'lds' in sst_df.columns:\n",
    "                newsst_df = self.compute_ldscores_region(i=i)\n",
    "                geno_dt['sst_df'] = newsst_df\n",
    "            if self.clear_linkage:\n",
    "                self.clear_linkage_region(i=i)\n",
    "\n",
    "    # Clearing Functions: #####\n",
    "    if True:\n",
    "\n",
    "        def clear_all_xda(self):\n",
    "            while len(self.xda_q) != 0:\n",
    "                i_2_rm, key = self.xda_q.popleft()\n",
    "                if i_2_rm == -1:\n",
    "                    continue  # Continue to next iter if encountering a padding -1\n",
    "                rmgeno_dt = self.reg_dt[i_2_rm]\n",
    "                self.cur_total_size_in_gb -= getsizeof(rmgeno_dt[key].val) / 1024 ** 3\n",
    "                rmgeno_dt.pop(key)\n",
    "            [self.xda_q.append((-1,'')) for _ in range(5)]  # put 5x -1 in queue\n",
    "            \n",
    "        def clear_linkage_allregions(self):\n",
    "            for i, geno_dt in self.reg_dt.items():\n",
    "                self.clear_linkage_region(i=i)\n",
    "            if self.verbose: print('\\nDone')\n",
    "\n",
    "        def clear_linkage_region(self, *, i):\n",
    "            geno_dt = self.reg_dt[i]\n",
    "            key_lst = list(geno_dt.keys())\n",
    "            for key in key_lst:\n",
    "                if key in self._clear_vars:\n",
    "                    geno_dt.pop(key)\n",
    "            if self.verbose: print(f'Cleared linkage region #{i} on chr{geno_dt[\"chrom\"]}', end='\\r'); sys.stdout.flush()\n",
    "            \n",
    "\n",
    "    ############################\n",
    "    ## Get: ####################\n",
    "    \n",
    "    # Local Linkage: ###########\n",
    "    if True:\n",
    "\n",
    "        def get_auto_linkage_region(self, *, i):\n",
    "            return self.get_specificied_linkage_region(i=i, shiftletter='D')\n",
    "\n",
    "        def get_left_linkage_region(self, *, i):\n",
    "            return self.get_specificied_linkage_region(i=i, shiftletter='L')\n",
    "\n",
    "        def get_right_linkage_region(self, *, i):\n",
    "            return self.get_specificied_linkage_region(i=i, shiftletter='R')\n",
    "\n",
    "        def get_specificied_linkage_region(self, *, i, shiftletter):\n",
    "            try:\n",
    "                return self.reg_dt[i][shiftletter]\n",
    "            except KeyError as e:\n",
    "                if self._onthefly_retrieval:\n",
    "                    if '_glocal' in shiftletter:\n",
    "                        self.retrieve_linkage_region_glocalshiftwindow(i=i)\n",
    "                    elif shiftletter in 'LDR':\n",
    "                        self.retrieve_linkage_region(i=i)\n",
    "                    elif shiftletter == 'Z':\n",
    "                        self.retrieve_linkage_region_global(i=i)\n",
    "                    else:\n",
    "                        raise Exception(f'shiftletter={shiftletter}, on-the-fly retrieval not a valid option.')\n",
    "                    try:\n",
    "                        return self.reg_dt[i][shiftletter]\n",
    "                    except Exception as e:\n",
    "                        print('Failed, eventough on-the-fly retrieval was attempted')\n",
    "                        raise e\n",
    "                else:\n",
    "                    raise Exception('on-the-fly retrieval blocked, set _onthefly_retrieval=True if desired')\n",
    "\n",
    "        def get_auto_range_region(self, *, i):\n",
    "            return self.reg_dt[i]['start_j'], self.reg_dt[i]['stop_j']\n",
    "\n",
    "        def get_left_range_region(self, *, i):\n",
    "            return self.reg_dt[i]['start_j_L'], self.reg_dt[i]['stop_j_L']\n",
    "\n",
    "        def get_right_range_region(self, *, i):\n",
    "            return self.reg_dt[i]['start_j_R'], self.reg_dt[i]['stop_j_R']\n",
    "\n",
    "    # Sumstats: #################\n",
    "    if True:\n",
    "\n",
    "        def get_beta_marginal_full(self):\n",
    "            beta_mrg_lst = []\n",
    "            for i, geno_dt in self.reg_dt.items():\n",
    "                beta_mrg_lst.append(geno_dt['beta_mrg'])\n",
    "            beta_mrg_full = np.concatenate(beta_mrg_lst)\n",
    "            return beta_mrg_full\n",
    "\n",
    "        get_beta_marginal = get_beta_marginal_full\n",
    "        \n",
    "        def get_beta_marginal_region(self, *, i):\n",
    "            return self.reg_dt[i]['beta_mrg']\n",
    "\n",
    "    # Xda: ######################\n",
    "    if True:\n",
    "    \n",
    "        def get_sda(self, *, i):\n",
    "            geno_dt = self.reg_dt[i]\n",
    "            if 'sda' in geno_dt.keys():\n",
    "                return geno_dt['sda']\n",
    "            else:\n",
    "                if 'srd' in geno_dt.keys():\n",
    "                    sda = geno_dt['srd'].read(dtype=self.dtype)\n",
    "                    sda, stansda = sda.standardize(standardizer=geno_dt['stansda'], return_trained=True)\n",
    "                    geno_dt['sda'] = sda\n",
    "                    geno_dt['stansda'] = stansda\n",
    "\n",
    "                    if 'loaded_sda' in geno_dt.keys():\n",
    "                        self.reloaded_xda_cnt += 1\n",
    "                        if self.reloaded_xda_cnt in [5, 20, 100, 400]:\n",
    "                            warnings.warn(\n",
    "                                f'Reloaded sda for the {self.reloaded_xda_cnt}\\'th time. This causes memory swapping,'\n",
    "                                ' that might make the computation of linkage quite slow.'\n",
    "                                'Probably because memory limits and/or linkage size.')\n",
    "                    # Size determination and accounting:\n",
    "                    geno_dt['loaded_sda']=True\n",
    "                    self.cur_total_size_in_gb += getsizeof(sda.val) / 1024 ** 3\n",
    "                    self.xda_q.append((i,'sda'))  # put respective i in queue.\n",
    "                    while self.cur_total_size_in_gb > self.gb_size_limit:  # Keep removing till size is ok\n",
    "                        i_2_rm, key = self.xda_q.popleft()\n",
    "                        if i_2_rm == -1:\n",
    "                            continue  # Continue to next iter if encountering a padding -1\n",
    "                        rmgeno_dt = self.reg_dt[i_2_rm]\n",
    "                        self.cur_total_size_in_gb -= getsizeof(rmgeno_dt[key].val) / 1024 ** 3\n",
    "                        rmgeno_dt.pop(key)\n",
    "                        if len(self.xda_q) <= 4:\n",
    "                            raise Exception('The memory footprint of current settings is too high, '\n",
    "                                            'reduce blocksize and/or correction windows or increase memory limits.')\n",
    "                    return sda\n",
    "                else:\n",
    "                    raise Exception(f'No srd or sda found in region i={i}, this is not supposed to happen.')\n",
    "\n",
    "        def get_pda(self):\n",
    "            if not hasattr(self, 'pda'):\n",
    "                pda = self.prd.read(dtype=self.dtype)\n",
    "                pda, self.stanpda = pda.standardize(return_trained=True,\n",
    "                                standardizer=self.pda_standardizer())\n",
    "                self.pda = pda\n",
    "            return self.pda\n",
    "    \n",
    "    # Utils: ####################\n",
    "    if True:\n",
    "    \n",
    "        def get_sumstats_cur(self):\n",
    "            sst_df_lst = []\n",
    "            for i, geno_dt in self.reg_dt.items():\n",
    "                sst_df = geno_dt['sst_df']\n",
    "                sst_df_lst.append(sst_df)\n",
    "            sst_df = pd.concat(sst_df_lst, axis=0)\n",
    "            return sst_df\n",
    "\n",
    "        def get_stansda(self, standardizer='unit'):\n",
    "            if not standardizer=='unit':\n",
    "                raise NotImplementedError('contact dev')\n",
    "            \n",
    "            if hasattr(self, 'stansda'):\n",
    "                if type(self.stansda) is UnitTrained:\n",
    "                    return self.stansda\n",
    "                else:\n",
    "                    raise NotImplementedError('contact dev')\n",
    "                    \n",
    "            standardizer_list = []\n",
    "            for i, geno_dt in self.reg_dt.items():\n",
    "                if 'stansda' in geno_dt.keys():\n",
    "                    if type(geno_dt['stansda']) is UnitTrained:\n",
    "                        standardizer_list.append(geno_dt['stansda'])\n",
    "\n",
    "            assert np.all([type(stan) is UnitTrained for stan in standardizer_list])            \n",
    "            sid = np.concatenate([stan.sid for stan in standardizer_list])\n",
    "            assert np.unique(sid).shape[0] == sid.shape[0]\n",
    "\n",
    "            stats = np.concatenate([stan.stats for stan in standardizer_list], dtype=self.dtype)\n",
    "            combined_unit_standardizer = UnitTrained(sid, stats)\n",
    "            self.stansda = combined_unit_standardizer\n",
    "            return combined_unit_standardizer\n",
    "\n",
    "''\n",
    "class LinkageData(BaseLinkageData):\n",
    "    pass\n",
    "\n",
    "def load_linkagedata(fn):\n",
    "    curfn = glob.glob(fn.format_map(defaultdict(lambda:'*')))[-1]\n",
    "    master_dt = json.loads(pd.read_hdf(curfn, key='master_dt').loc[0,0])\n",
    "    master_dt['curdn'] = os.path.dirname(curfn)\n",
    "    return LinkageData(master_dt=master_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../pgsbenchmark/loaders.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regdef_df --> Checks that are needed: region ranges count upward, check if regid is present, else make it.\n",
    "# assert chrom contains no None's and nans\n",
    "# This should go different in the future:\n",
    "# https://github.com/scikit-learn/scikit-learn/blob/7e1e6d09bcc2eaeba98f7e737aac2ac782f0e5f1/sklearn/base.py#L31\n",
    "# clone objects instead of messing with classes.. skl -> clone(estimator)\n",
    "\n",
    "# A test of sst_df with all the other inputs should be done\n",
    "\n",
    "def _test_padding(self):\n",
    "    geno_dt = self.reg_dt[45]\n",
    "    implot(geno_dt['D'])\n",
    "type(linkdata)._test_padding = _test_padding\n",
    "linkdata._test_padding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://127.0.0.1:7090/notebooks/proj/ppb-paper/code/15__PPB-rerun---run%2Bsave-4-final-res.ipynb\n",
    "# this stuff is for creating pics of whole chromosomes:\n",
    "def fun(arg):\n",
    "    return asizeof(arg)/1024**3\n",
    "gb_dt = {}\n",
    "for i,  geno_dt in linkdata.reg_dt.items():\n",
    "    gb_dt[i] = {key: fun(geno_dt[key]) for key in 'LDR'}\n",
    "df = pd.DataFrame(gb_dt).T\n",
    "df.sum(),df.sum().sum()\n",
    "\n",
    "# Moar things:\n",
    "info_dt = mcres_dt['info_dt']\n",
    "df = pd.DataFrame(info_dt).T\n",
    "stop=df.iloc[:,-3:].applymap(lambda x: np.max(x)).max(axis=1)\n",
    "start=df.iloc[:,-3:].applymap(lambda x: np.min(x)).min(axis=1)\n",
    "ld_df = pd.DataFrame(dict(start=start,stop=stop))\n",
    "ld_df['ar'] = df['ar'].apply(np.array)\n",
    "\n",
    "fac = 10000\n",
    "p = 1117493\n",
    "M = np.zeros((fac,fac))\n",
    "dt = dict()\n",
    "for i, row in ld_df.iterrows():\n",
    "    chrom = linkdata.regdef_df.loc[i,'chrom']\n",
    "    ld_df.loc[i,'chrom'] = int(chrom)\n",
    "\n",
    "# rege  \n",
    "def fun(arg, ap):\n",
    "    res = pd.DataFrame(arg.to_list())\n",
    "    minv = np.min(res.values.flatten())\n",
    "    maxv = np.max(res.values.flatten())\n",
    "    return pd.DataFrame.from_dict(dict(minv=minv,maxv=maxv), orient='index').T\n",
    "for chrom, row in ld_df.groupby('chrom')['ar'].apply(fun,ap=np.min).droplevel(-1).iterrows():\n",
    "    a,b=int((row['minv']/p)*fac), int((row['maxv']/p)*fac)\n",
    "    M[a:b,a:b] = -1\n",
    "#     print(a,b)\n",
    "#     ewefwef\n",
    "#     if chrom > 1:\n",
    "#         rergerg\n",
    "# ld_df.groupby('chrom')['ar'].apply(fun,ap=np.max)\n",
    "    \n",
    "for i, row in ld_df.iterrows():\n",
    "    ystart, ystop = np.round(((row['ar']/p)*fac).astype(int))\n",
    "    xstart = np.round((row['start']/p)*fac).astype(int)\n",
    "    xstop = np.round((row['stop']/p)*fac).astype(int)\n",
    "    M[ystart:ystop,xstart:xstop] = 1\n",
    "implot(M)\n",
    "start, stop = 0,1000\n",
    "implot(M[start:stop,start:stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backlog\n",
    "\n",
    "- Create empty arrays for L&R if shift==0\n",
    "- implot:\n",
    "    - enhance implot to do many plots on same line\n",
    "    - make ldplot, with appropriate coloring\n",
    "- Add os.path.expanduser to pysnptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../pgsbenchmark/loaders.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../pgsbenchmark/loaders.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "LinkageData\n",
    "durr tst\n",
    "\"\"\"\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import linalg\n",
    "from sys import getsizeof\n",
    "\n",
    "import warnings, importlib, json, os, glob\n",
    "from collections import OrderedDict, deque, defaultdict\n",
    "from pysnptools.standardizer import Unit, UnitTrained\n",
    "import pysnptools as pst\n",
    "# import pysnptools.util as pstutil\n",
    "# from pysnptools.standardizer import UnitTrained\n",
    "# from dataclasses import dataclass\n",
    "\n",
    "class SqrtNinv(Unit):\n",
    "    def __init__(self):\n",
    "        super(SqrtNinv, self).__init__()\n",
    "\n",
    "# @dataclass\n",
    "class BaseLinkageData():\n",
    "\n",
    "    def __init__(self, *, sst_df=None, regdef_df=None, master_dt=None, #There should be sst_df or master_dt\n",
    "                 srd=None, sda_standardizer=Unit,\n",
    "                 prd=None, pda_standardizer=Unit,\n",
    "                 lrd=None, lda_standardizer=None,\n",
    "                 grd=None, gda_standardizer=False,\n",
    "                 \n",
    "                 shift=0, cm=None, _setzero=True,\n",
    "                 \n",
    "                 clear_xda=True, # Refactor with _clear?\n",
    "                 clear_linkage=False,\n",
    "                 compute_sumstats=False,\n",
    "                 calc_allelefreq=False,\n",
    "                 \n",
    "                 _onthefly_retrieval=True,\n",
    "                 _save_vars = ['L','D','R','sst_df'],\n",
    "                 _clear_vars = ['L','D','R'],\n",
    "                 gb_size_limit=10., dtype='float32', verbose=False):\n",
    "        \n",
    "        if True:\n",
    "            # bim and fam df have to be supplied because pysnptools halvely\n",
    "            # implemented these portions of the genetic data into their object\n",
    "            # meaning that srd cannot be relied uppon\n",
    "            excl_lst = ['self','kwg_dt','excl_lst']\n",
    "            kwg_dt = {key: item for key, item in locals().items() if not (key in excl_lst)}\n",
    "            for key, item in locals().items():\n",
    "                if not (key in excl_lst): \n",
    "                    self.__setattr__(key, item)\n",
    "            # New rule: blx have to be created from the inside\n",
    "            # Perhaps later it can be made into a special load instead of a compute\n",
    "\n",
    "            # first-checks & inits:\n",
    "            if cm is not None: assert cm > 0\n",
    "            if lrd is not None: raise NotImplementedError('lrd not possible atm.')\n",
    "            if grd is not None:\n",
    "                assert gda_standardizer or (gda_standardizer is None)\n",
    "            assert type(compute_sumstats) is bool\n",
    "            self.reg_dt = dict()\n",
    "            self.cur_total_size_in_gb = 0.0\n",
    "            self.xda_q = deque()\n",
    "            [self.xda_q.append((-1,'')) for _ in range(5)]  # put 5x -1 in queue\n",
    "            self.reloaded_xda_cnt = 0\n",
    "            self._fn_lst = []\n",
    "\n",
    "            # Checks            \n",
    "            if srd is not None:\n",
    "                assert type(sst_df) is pd.DataFrame\n",
    "                self._check_xrd()\n",
    "                assert isinstance(sst_df, pd.DataFrame)\n",
    "                assert isinstance(regdef_df, pd.DataFrame)\n",
    "                self.init_regions()\n",
    "            elif master_dt is not None:\n",
    "                # Fill attributes in case master_dt is present:\n",
    "                for key, item in master_dt.items():\n",
    "                    setattr(self, key, item)\n",
    "                reg_dt=dict()\n",
    "                for pre_i, geno_dt in self.reg_dt.items(): reg_dt[int(pre_i)] = geno_dt\n",
    "                self.reg_dt = reg_dt # An ugly type conversion hack cause json does not allow i to be integer, but forces it to be a string.\n",
    "            else:\n",
    "                raise Exception('Essentials not present')\n",
    "\n",
    "    def _check_xrd(self):\n",
    "\n",
    "        if self.srd is not None:\n",
    "            assert pst.snpreader.SnpReader in self.srd.__class__.__mro__\n",
    "\n",
    "        if self.prd is not None:\n",
    "            n_start = len(self.prd.iid)\n",
    "            self.srd, self.prd = pst.util.intersect_apply([self.srd, self.prd])\n",
    "            if len(self.prd.iid) != n_start:\n",
    "                warnings.warn('Number of samples do not match up after internal intersection, samples were lost:' \n",
    "                              f'{n_start - len(self.prd.iid)}, start = {n_start}, after_intersection = {len(self.prd.iid)}')\n",
    "\n",
    "        if self.grd is not None:\n",
    "            # Check alignment for now, auto alignment needs work cause iid stuffs:\n",
    "            if self.srd is not None:\n",
    "                if not np.all(self.grd.sid == self.srd.sid):\n",
    "                    raise Exception('snps of grd and srd not matching up, align first,'\n",
    "                                    ' auto align will be implemented later')\n",
    "            else:\n",
    "                raise NotImplementedError('Not sure what to do with grd if no srd is present. not implemented.')\n",
    "        \n",
    "    ###########################\n",
    "    # Regions Administration:\n",
    "    if True:\n",
    "\n",
    "        def init_regions(self):\n",
    "            do_beta_moving = ('beta_mrg' in self.sst_df.columns)\n",
    "            if not do_beta_moving:\n",
    "                warnings.warn('No \\'beta\\' column detected in sst_df! This means that no summary stats were detected.')\n",
    "            else:\n",
    "                assert 'n_eff' in self.sst_df.columns\n",
    "            cur_chrom = None\n",
    "            i = 0; n_snps_cumsum = 0\n",
    "            sst_df_lst = []\n",
    "            for reg_cnt, (_, row) in enumerate(self.regdef_df.iterrows()):\n",
    "                # Move region into specialized dictionary\n",
    "                regid = row['regid'];\n",
    "                chrom = row['chrom']\n",
    "                start = row['start'];\n",
    "                stop  = row['stop']\n",
    "\n",
    "                # Map Variants to region\n",
    "                ind = self.sst_df.chrom == chrom\n",
    "                ind = (self.sst_df['pos'] >= start) & ind\n",
    "                ind = (self.sst_df['pos'] < stop) & ind\n",
    "                sid = self.sst_df['snp'][ind].values\n",
    "                indices = self.srd.sid_to_index(sid)  # if sid not strickly present this will give an error!\n",
    "                n_snps_reg = len(indices)\n",
    "                if n_snps_reg == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    geno_dt = dict(regid=regid,\n",
    "                                   chrom=chrom,\n",
    "                                   start=start,\n",
    "                                   stop=stop,\n",
    "                                   start_j=n_snps_cumsum)\n",
    "                    n_snps_cumsum += n_snps_reg\n",
    "                    geno_dt['stop_j'] = n_snps_cumsum\n",
    "                    sst_df = self.sst_df[ind].copy(); sst_df['i'] = i\n",
    "                    geno_dt['sst_df'] = sst_df\n",
    "                    assert geno_dt['start_j'] == sst_df.index[0]; sst_df_lst.append(sst_df)\n",
    "                    assert geno_dt['stop_j']  == sst_df.index[-1] + 1\n",
    "                    if do_beta_moving:\n",
    "                        geno_dt['beta_mrg'] = geno_dt['sst_df']['beta_mrg'].copy().values[:, np.newaxis]\n",
    "                        assert len(geno_dt['beta_mrg'].shape) == 2\n",
    "                    if self.srd is not None:\n",
    "                        geno_dt['srd'] = self.srd[:, indices]\n",
    "                        geno_dt['stansda'] = self.sda_standardizer() if self.sda_standardizer is not None else None\n",
    "                    else:\n",
    "                        raise NotImplementedError()\n",
    "                    if self.grd is not None:\n",
    "                        geno_dt['grd'] = self.grd[:, indices]\n",
    "                        geno_dt['stangda'] = self.gda_standardizer() if self.gda_standardizer is not None else None\n",
    "                    # Count up if things are actually stored in reg_dt\n",
    "                    self.reg_dt[i] = geno_dt\n",
    "                    i += 1\n",
    "            self.n_snps_total = n_snps_cumsum\n",
    "            sst_df = pd.concat(sst_df_lst, axis=0)\n",
    "            self.sst_df = sst_df\n",
    "\n",
    "        def get_i_list(self):\n",
    "            return list(self.reg_dt.keys())\n",
    "\n",
    "#         def _load_all_snpdata(self):\n",
    "#             # load all regions\n",
    "#             for i, geno_dt in self.reg_dt.items():\n",
    "#                 sda = geno_dt['srd'].read(dtype=self.dtype)\n",
    "#                 stansda = sda.train_standardizer(apply_in_place=True,\n",
    "#                                                  standardizer=geno_dt['stansda'])\n",
    "#                 geno_dt['sda'] = sda\n",
    "#                 geno_dt['stansda'] = stansda\n",
    "\n",
    "    ###########################\n",
    "    ## Compute: ###############\n",
    "\n",
    "    # Local Linkage Stuff: ####\n",
    "    if True:\n",
    "    \n",
    "        def compute_linkage_sameregion(self, *, i):\n",
    "            return self.compute_linkage_shiftregion(i=i, shift=0)\n",
    "\n",
    "        def regions_compatible(self, *, i, j):\n",
    "            try:\n",
    "                if self.reg_dt[i]['chrom'] == self.reg_dt[j]['chrom']:\n",
    "                    res = True\n",
    "                else:\n",
    "                    res = False\n",
    "            except Exception as e:\n",
    "                if (not (i in self.reg_dt.keys())) or (not (j in self.reg_dt.keys())):\n",
    "                    res = False\n",
    "                else:\n",
    "                    raise e\n",
    "            return res\n",
    "\n",
    "        def compute_linkage_shiftregion(self, *, i, shift):\n",
    "            j = i + shift\n",
    "            if self.regions_compatible(i=i, j=j):\n",
    "                self_sda = self.get_sda(i=i)\n",
    "                dist_sda = self.get_sda(i=j)\n",
    "                n = len(self_sda.iid)\n",
    "                S_shift = self_sda.val.T.dot(dist_sda.val) / n\n",
    "                return S_shift\n",
    "            else:\n",
    "                self_sda = self.get_sda(i=i)\n",
    "                return np.zeros((self_sda.val.shape[1], 0))\n",
    "\n",
    "        def compute_linkage_cmfromregion(self, *, i, cm):            \n",
    "            geno_dt = self.reg_dt[i]; lst = []\n",
    "            if cm < 0: # Doing left:\n",
    "                stop_j   = geno_dt['start_j']\n",
    "                cm_left  = geno_dt['sst_df'].loc[stop_j]['cm'] \n",
    "                slc_df = self.sst_df.loc[:stop_j-1]\n",
    "                slc_df = slc_df[slc_df.chrom==geno_dt['chrom']]\n",
    "                slc_df = slc_df[slc_df.cm > (cm_left + cm)]\n",
    "                start_i = slc_df['i'].min()\n",
    "                start_i = -7 if np.isnan(start_i) else start_i\n",
    "                for cur_i in range(start_i, i):\n",
    "                    lst.append(self.compute_linkage_shiftregion(i=i, shift=cur_i-i))\n",
    "                    if start_i == -7: break\n",
    "                L = np.concatenate(lst, axis=1)[:,-slc_df.shape[0]:] # concat & clip\n",
    "                if self._setzero:\n",
    "                    cms_reg    = geno_dt['sst_df']['cm'].values\n",
    "                    cms_distal = slc_df['cm'].values\n",
    "                    cms_L      =  cms_distal[np.newaxis,:] - cms_reg[:,np.newaxis]\n",
    "                    setzero_L  = cms_L < cm\n",
    "                    L[setzero_L] = 0\n",
    "                    assert L.shape == setzero_L.shape\n",
    "                return L\n",
    "            else:\n",
    "                start_j   = geno_dt['stop_j']\n",
    "                cm_right  = geno_dt['sst_df'].loc[start_j-1]['cm']\n",
    "                slc_df = self.sst_df.loc[start_j:]\n",
    "                slc_df = slc_df[slc_df.chrom==geno_dt['chrom']]\n",
    "                slc_df = slc_df[slc_df.cm < (cm_right + cm)]\n",
    "                stop_i = slc_df['i'].max()\n",
    "                stop_i = i+2 if np.isnan(stop_i) else stop_i + 1\n",
    "                for cur_i in range(i+1, stop_i):\n",
    "                    lst.append(self.compute_linkage_shiftregion(i=i, shift=cur_i-i))\n",
    "                R = np.concatenate(lst, axis=1)[:,:slc_df.shape[0]] # concat & clip\n",
    "                if self._setzero:\n",
    "                    cms_reg    = geno_dt['sst_df']['cm'].values\n",
    "                    cms_distal = slc_df['cm'].values\n",
    "                    cms_R     =  cms_distal[np.newaxis,:] - cms_reg[:,np.newaxis]\n",
    "                    setzero_R = cms_R > cm\n",
    "                    R[setzero_R] = 0\n",
    "                    assert R.shape == setzero_R.shape\n",
    "                return R\n",
    "        \n",
    "    # Misc Stuff: #############\n",
    "    if True:\n",
    "    \n",
    "        def compute_sumstats_region(self, *, i):\n",
    "            geno_dt = self.reg_dt[i]\n",
    "            sda = self.get_sda(i=i)\n",
    "            X = sda.val\n",
    "            y = self.get_pda().val\n",
    "            n = len(y)\n",
    "            c_reg = X.T.dot(y) / n\n",
    "            return c_reg   \n",
    "\n",
    "        def compute_allelefreq_region(self, *, i):\n",
    "            # Speed might be improved by using dot prod here, instead of sums\n",
    "            # np.unique was way slower (5x)\n",
    "            geno_dt = self.reg_dt[i]\n",
    "            n, p_blk = sda.val.shape\n",
    "            sst_df = geno_dt['sst_df'].copy()\n",
    "            cnt0   = np.sum(sda.val==0, axis=0)\n",
    "            cnt1   = np.sum(sda.val==1, axis=0)\n",
    "            cnt2   = np.sum(sda.val==2, axis=0)\n",
    "            cntnan = np.sum(np.isnan(sda.val), axis=0)\n",
    "            assert np.allclose(cnt0 + cnt1 + cnt2 + cntnan, n)\n",
    "            sst_df['altcnt=0']   = cnt0\n",
    "            sst_df['altcnt=1']   = cnt1\n",
    "            sst_df['altcnt=2']   = cnt2\n",
    "            sst_df['altcnt=nan'] = cntnan\n",
    "            sst_df['altfreq']    = (cnt1 + cnt2)/(n - cntnan)\n",
    "            sst_df['missfreq']   = 1 - cntnan/n\n",
    "            return sst_df\n",
    "\n",
    "        def compute_ldscores_region(self, *, i):\n",
    "            sst_df = self.reg_dt[i]['sst_df'].copy()\n",
    "            L = self.get_left_linkage_region(i=i)\n",
    "            D = self.get_auto_linkage_region(i=i)\n",
    "            R = self.get_right_linkage_region(i=i)\n",
    "            for k, j in enumerate(sst_df.index):\n",
    "                slds = np.sum(L[k]**2) + np.sum(D[k]**2) + np.sum(R[k]**2)\n",
    "                sst_df.loc[j, 'lds'] = np.sqrt(slds)alle\n",
    "            return sst_df\n",
    "        \n",
    "    ############################\n",
    "    ## Retrieve: ###############\n",
    "    \n",
    "    # Local Linkage: ############\n",
    "    if True:\n",
    "    \n",
    "        def retrieve_linkage_allregions(self):\n",
    "            for i, geno_dt in self.reg_dt.items():\n",
    "                self.retrieve_linkage_region(i=i)\n",
    "            if self.verbose:   print('\\nDone')\n",
    "            if self.clear_xda: self.clear_all_xda()\n",
    "\n",
    "        def retrieve_linkage_region(self, *, i):\n",
    "\n",
    "            geno_dt = self.reg_dt[i]\n",
    "            if 'store_dt' in geno_dt.keys():\n",
    "                self.load_linkage_region(i=i)\n",
    "            shift = self.shift; cm = self.cm\n",
    "            compute_sumstats = self.compute_sumstats\n",
    "\n",
    "            if 'L' in geno_dt.keys():\n",
    "                if 'D' in geno_dt.keys():\n",
    "                    if 'R' in geno_dt.keys():\n",
    "                        return None  # everything is done now.\n",
    "\n",
    "            if self.verbose: print(f'Computing LD for region #{i} on chr{geno_dt[\"chrom\"]}', end='\\r')\n",
    "            # Refactor: if linkage is only in blocks this code will lead to recomputation...\n",
    "            if (shift > 0):\n",
    "                L_lst = []\n",
    "                R_lst = []\n",
    "                for cur_shift in range(1, shift + 1):\n",
    "                    L_lst.append(self.compute_linkage_shiftregion(i=i, shift=-cur_shift))\n",
    "                    R_lst.append(self.compute_linkage_shiftregion(i=i, shift=cur_shift))\n",
    "\n",
    "                # Store Linkage in geno_dt\n",
    "                geno_dt['L'] = np.concatenate(L_lst[::-1], axis=1)  # L stands for left\n",
    "                geno_dt['D'] = self.compute_linkage_sameregion(i=i)  # Linkage within region, D is convention from LDpred 1\n",
    "                geno_dt['R'] = np.concatenate(R_lst, axis=1)  # R stands for right\n",
    "\n",
    "                # Indices needed for slicing and dicing matched variables (e.g. beta weights):\n",
    "                geno_dt['start_j_L'] = geno_dt['start_j'] - geno_dt['L'].shape[1]\n",
    "                geno_dt['stop_j_L'] = geno_dt['start_j']\n",
    "                geno_dt['start_j_R'] = geno_dt['stop_j']\n",
    "                geno_dt['stop_j_R'] = geno_dt['stop_j'] + geno_dt['R'].shape[1]\n",
    "\n",
    "            elif (shift==0) and (cm is None):  # Only same region has to be done.\n",
    "                geno_dt['D'] = self.compute_linkage_sameregion(i=i)\n",
    "\n",
    "            elif (shift==0) and cm > 0:\n",
    "                geno_dt['L'] = self.compute_linkage_cmfromregion(i=i, cm=-cm)\n",
    "                geno_dt['D'] = self.compute_linkage_sameregion(i=i)\n",
    "                geno_dt['R'] = self.compute_linkage_cmfromregion(i=i, cm=cm)\n",
    "\n",
    "                # Indices needed for slicing and dicing matched variables (e.g. beta weights):\n",
    "                geno_dt['start_j_L'] = geno_dt['start_j'] - geno_dt['L'].shape[1]\n",
    "                geno_dt['stop_j_L'] = geno_dt['start_j']\n",
    "                geno_dt['start_j_R'] = geno_dt['stop_j']\n",
    "                geno_dt['stop_j_R'] = geno_dt['stop_j'] + geno_dt['R'].shape[1]\n",
    "\n",
    "            if compute_sumstats:\n",
    "                self.retrieve_sumstats_region(i=i)\n",
    "              \n",
    "        def load_linkage_allregions(self):\n",
    "            for i, geno_dt in self.reg_dt.items():\n",
    "                self.load_linkage_region(i=i)\n",
    "            if self.verbose: print('\\nDone')\n",
    "            \n",
    "        def load_linkage_region(self, *, i):\n",
    "            geno_dt = self.reg_dt[i]\n",
    "            store_dt = geno_dt['store_dt']\n",
    "            for varname, file_dt in store_dt.items():\n",
    "                module = importlib.import_module('.'.join(file_dt['typestr'].split('.')[:-1]))\n",
    "                cname  = file_dt['typestr'].split('.')[-1]\n",
    "                CurClass = getattr(module, cname) # Retrieves module.submodule.submodule.. etc\n",
    "                curfullfn = os.path.join(self.curdn, file_dt['fn'])\n",
    "                geno_dt[varname] = CurClass(pd.read_hdf(curfullfn, key=file_dt['key']))\n",
    "                if self.verbose: print(f'loading: fn={curfullfn} key={file_dt[\"key\"]}'+' '*50, end='\\r')\n",
    "                \n",
    "        def save(self, fn, keyfmt='ld/chrom{chrom}/i{i}/{varname}', fmt='hdf5', mkdir=False, dn=None):\n",
    "            self.curdn = os.path.dirname(fn) if (dn is None) else dn\n",
    "            fn = os.path.basename(fn) if (dn is None) else fn\n",
    "            if mkdir: os.makedirs(self.curdn, exist_ok=True)\n",
    "            if (fmt != 'hdf5'): raise Exception(f'Only hdf5 file format supported atm, not {fmt}') \n",
    "            for i, geno_dt in self.reg_dt.items():\n",
    "                self.save_linkage_region(i=i, fn=fn)\n",
    "                \n",
    "            # Saving of 'logistical' data for the object\n",
    "            master_lst = [ 'shift', 'cm', '_setzero',\n",
    "             'clear_xda', 'clear_linkage', 'compute_sumstats', 'calc_allelefreq', \n",
    "             '_onthefly_retrieval', '_save_vars', '_clear_vars', \n",
    "             'gb_size_limit', 'dtype', 'verbose', 'n_snps_total']\n",
    "            geno_lst = ['regid','chrom','start','stop','start_j','stop_j',\n",
    "                        'start_j_L', 'stop_j_L', 'start_j_R', 'stop_j_R','store_dt']\n",
    "                \n",
    "            def caster(arg, types):\n",
    "                if np.isscalar(arg):\n",
    "                    if isinstance(arg, np.integer): arg = int(arg)\n",
    "                if type(arg) is int: return int(arg)\n",
    "                assert type(arg) in types\n",
    "                return arg\n",
    "\n",
    "            master_dt = dict(); maxlen = 20\n",
    "            for key in master_lst:\n",
    "                var = getattr(self, key)\n",
    "                if type(var) is list:\n",
    "                    for item in var:\n",
    "                        assert type(item) in (bool, str, float, int)\n",
    "                        if type(item) is str: assert (len(item) < maxlen)\n",
    "                elif type(var) is str:\n",
    "                        assert len(var) < maxlen\n",
    "                master_dt[key] = caster(var, (list, bool, float, int, str))\n",
    "\n",
    "            reg_dt = dict()\n",
    "            for i, geno_dt in self.reg_dt.items():\n",
    "                newgeno_dt = dict()\n",
    "                for key in geno_lst:\n",
    "                    if not (key in geno_dt.keys()): continue\n",
    "                    newgeno_dt[key] = caster(geno_dt[key], (str, int, dict))\n",
    "                reg_dt[i] = newgeno_dt\n",
    "            master_dt['reg_dt'] = reg_dt     \n",
    "            self._fn_lst = list(np.unique(self._fn_lst))\n",
    "            for curfn in self._fn_lst:\n",
    "                pd.DataFrame([json.dumps(master_dt)]).to_hdf(os.path.join(self.curdn, curfn), key='master_dt')\n",
    "            \n",
    "            if self.verbose: print('\\nDone')\n",
    "                \n",
    "        def someextramethod(self):\n",
    "            return True\n",
    "\n",
    "        def save_linkage_region(self, *, i, fn, keyfmt='ld/chrom{chrom}/i{i}/{varname}'): \n",
    "            # using 'store' instead of 'save' to indicate a connected relationship with \n",
    "            # the files used for this storage.\n",
    "            geno_dt = self.reg_dt[i]\n",
    "            chrom = geno_dt['chrom']\n",
    "            curdn = self.curdn\n",
    "            store_dt = dict() #geno_dt['store_dt']\n",
    "            for varname, var in geno_dt.items():\n",
    "                if varname in self._save_vars:\n",
    "                    curfn  = fn.format(**locals())\n",
    "                    key    = keyfmt.format(**locals())\n",
    "                    var    = geno_dt[varname]\n",
    "                    vartype = type(var)\n",
    "                    if vartype is np.ndarray: vartype = var.dtype.type\n",
    "                    curfullfn = os.path.join(curdn,curfn)\n",
    "                    pd.DataFrame(var).to_hdf(curfullfn, key=key)\n",
    "                    file_dt = dict(fn=curfn, key=key, \n",
    "                                   typestr=vartype.__module__+'.'+vartype.__name__)\n",
    "                    store_dt[varname] = file_dt\n",
    "                    self._fn_lst.append(curfn)\n",
    "                    if self.verbose: print(f'saving: fn={curfullfn} key={key}'+' '*50,end='\\r')\n",
    "            geno_dt['store_dt'] = store_dt\n",
    "                   \n",
    "    # SumStat: ##############\n",
    "    if True:\n",
    "\n",
    "        def retrieve_sumstats_allregions(self):\n",
    "            for i, geno_dt in self.reg_dt.items():\n",
    "                self.retrieve_sumstats_region(i=i)\n",
    "\n",
    "        def retrieve_sumstats_region(self, *, i):\n",
    "            geno_dt = self.reg_dt[i] \n",
    "            sst_df  = geno_dt['sst_df']\n",
    "            if 'beta_mrg' in geno_dt.keys():\n",
    "                return None # Sumstat present so no need to compute anything.\n",
    "            geno_dt['beta_mrg'] = self.compute_sumstats_region(i=i)\n",
    "            if not 'beta_mrg' in sst_df.columns:\n",
    "                geno_dt['sst_df']['beta_mrg'] = geno_dt['beta_mrg']\n",
    "                \n",
    "        retrieve_betamrg_region = retrieve_sumstats_region\n",
    "\n",
    "        def retrieve_ldscores_allregions(self):\n",
    "            for i, geno_dt in self.reg_dt.items():\n",
    "                self.retrieve_ldscores_region(i=i)\n",
    "\n",
    "        def retrieve_ldscores_region(self, *, i):\n",
    "            geno_dt = self.reg_dt[i]\n",
    "            sst_df = geno_dt['sst_df']\n",
    "            if not 'lds' in sst_df.columns:\n",
    "                newsst_df = self.compute_ldscores_region(i=i)\n",
    "                geno_dt['sst_df'] = newsst_df\n",
    "            if self.clear_linkage:\n",
    "                self.clear_linkage_region(i=i)\n",
    "\n",
    "    # Clearing Functions: #####\n",
    "    if True:\n",
    "\n",
    "        def clear_all_xda(self):\n",
    "            while len(self.xda_q) != 0:\n",
    "                i_2_rm, key = self.xda_q.popleft()\n",
    "                if i_2_rm == -1:\n",
    "                    continue  # Continue to next iter if encountering a padding -1\n",
    "                rmgeno_dt = self.reg_dt[i_2_rm]\n",
    "                self.cur_total_size_in_gb -= getsizeof(rmgeno_dt[key].val) / 1024 ** 3\n",
    "                rmgeno_dt.pop(key)\n",
    "            [self.xda_q.append((-1,'')) for _ in range(5)]  # put 5x -1 in queue\n",
    "            \n",
    "        def clear_linkage_allregions(self):\n",
    "            for i, geno_dt in self.reg_dt.items():\n",
    "                self.clear_linkage_region(i=i)\n",
    "            if self.verbose: print('\\nDone')\n",
    "\n",
    "        def clear_linkage_region(self, *, i):\n",
    "            geno_dt = self.reg_dt[i]\n",
    "            key_lst = list(geno_dt.keys())\n",
    "            for key in key_lst:\n",
    "                if key in self._clear_vars:\n",
    "                    geno_dt.pop(key)\n",
    "            if self.verbose: print(f'Cleared linkage region #{i} on chr{geno_dt[\"chrom\"]}', end='\\r'); sys.stdout.flush()\n",
    "            \n",
    "\n",
    "    ############################\n",
    "    ## Get: ####################\n",
    "    \n",
    "    # Local Linkage: ###########\n",
    "    if True:\n",
    "\n",
    "        def get_auto_linkage_region(self, *, i):\n",
    "            return self.get_specificied_linkage_region(i=i, shiftletter='D')\n",
    "\n",
    "        def get_left_linkage_region(self, *, i):\n",
    "            return self.get_specificied_linkage_region(i=i, shiftletter='L')\n",
    "\n",
    "        def get_right_linkage_region(self, *, i):\n",
    "            return self.get_specificied_linkage_region(i=i, shiftletter='R')\n",
    "\n",
    "        def get_specificied_linkage_region(self, *, i, shiftletter):\n",
    "            try:\n",
    "                return self.reg_dt[i][shiftletter]\n",
    "            except KeyError as e:\n",
    "                if self._onthefly_retrieval:\n",
    "                    if '_glocal' in shiftletter:\n",
    "                        self.retrieve_linkage_region_glocalshiftwindow(i=i)\n",
    "                    elif shiftletter in 'LDR':\n",
    "                        self.retrieve_linkage_region(i=i)\n",
    "                    elif shiftletter == 'Z':\n",
    "                        self.retrieve_linkage_region_global(i=i)\n",
    "                    else:\n",
    "                        raise Exception(f'shiftletter={shiftletter}, on-the-fly retrieval not a valid option.')\n",
    "                    try:\n",
    "                        return self.reg_dt[i][shiftletter]\n",
    "                    except Exception as e:\n",
    "                        print('Failed, eventough on-the-fly retrieval was attempted')\n",
    "                        raise e\n",
    "                else:\n",
    "                    raise Exception('on-the-fly retrieval blocked, set _onthefly_retrieval=True if desired')\n",
    "\n",
    "        def get_auto_range_region(self, *, i):\n",
    "            return self.reg_dt[i]['start_j'], self.reg_dt[i]['stop_j']\n",
    "\n",
    "        def get_left_range_region(self, *, i):\n",
    "            return self.reg_dt[i]['start_j_L'], self.reg_dt[i]['stop_j_L']\n",
    "\n",
    "        def get_right_range_region(self, *, i):\n",
    "            return self.reg_dt[i]['start_j_R'], self.reg_dt[i]['stop_j_R']\n",
    "\n",
    "    # Sumstats: #################\n",
    "    if True:\n",
    "\n",
    "        def get_beta_marginal_full(self):\n",
    "            beta_mrg_lst = []\n",
    "            for i, geno_dt in self.reg_dt.items():\n",
    "                beta_mrg_lst.append(geno_dt['beta_mrg'])\n",
    "            beta_mrg_full = np.concatenate(beta_mrg_lst)\n",
    "            return beta_mrg_full\n",
    "\n",
    "        get_beta_marginal = get_beta_marginal_full\n",
    "        \n",
    "        def get_beta_marginal_region(self, *, i):\n",
    "            return self.reg_dt[i]['beta_mrg']\n",
    "\n",
    "    # Xda: ######################\n",
    "    if True:\n",
    "    \n",
    "        def get_sda(self, *, i):\n",
    "            geno_dt = self.reg_dt[i]\n",
    "            if 'sda' in geno_dt.keys():\n",
    "                return geno_dt['sda']\n",
    "            else:\n",
    "                if 'srd' in geno_dt.keys():\n",
    "                    sda = geno_dt['srd'].read(dtype=self.dtype)\n",
    "                    sda, stansda = sda.standardize(standardizer=geno_dt['stansda'], return_trained=True)\n",
    "                    geno_dt['sda'] = sda\n",
    "                    geno_dt['stansda'] = stansda\n",
    "\n",
    "                    if 'loaded_sda' in geno_dt.keys():\n",
    "                        self.reloaded_xda_cnt += 1\n",
    "                        if self.reloaded_xda_cnt in [5, 20, 100, 400]:\n",
    "                            warnings.warn(\n",
    "                                f'Reloaded sda for the {self.reloaded_xda_cnt}\\'th time. This causes memory swapping,'\n",
    "                                ' that might make the computation of linkage quite slow.'\n",
    "                                'Probably because memory limits and/or linkage size.')\n",
    "                    # Size determination and accounting:\n",
    "                    geno_dt['loaded_sda']=True\n",
    "                    self.cur_total_size_in_gb += getsizeof(sda.val) / 1024 ** 3\n",
    "                    self.xda_q.append((i,'sda'))  # put respective i in queue.\n",
    "                    while self.cur_total_size_in_gb > self.gb_size_limit:  # Keep removing till size is ok\n",
    "                        i_2_rm, key = self.xda_q.popleft()\n",
    "                        if i_2_rm == -1:\n",
    "                            continue  # Continue to next iter if encountering a padding -1\n",
    "                        rmgeno_dt = self.reg_dt[i_2_rm]\n",
    "                        self.cur_total_size_in_gb -= getsizeof(rmgeno_dt[key].val) / 1024 ** 3\n",
    "                        rmgeno_dt.pop(key)\n",
    "                        if len(self.xda_q) <= 4:\n",
    "                            raise Exception('The memory footprint of current settings is too high, '\n",
    "                                            'reduce blocksize and/or correction windows or increase memory limits.')\n",
    "                    return sda\n",
    "                else:\n",
    "                    raise Exception(f'No srd or sda found in region i={i}, this is not supposed to happen.')\n",
    "\n",
    "        def get_pda(self):\n",
    "            if not hasattr(self, 'pda'):\n",
    "                pda = self.prd.read(dtype=self.dtype)\n",
    "                pda, self.stanpda = pda.standardize(return_trained=True,\n",
    "                                standardizer=self.pda_standardizer())\n",
    "                self.pda = pda\n",
    "            return self.pda\n",
    "    \n",
    "    # Utils: ####################\n",
    "    if True:\n",
    "    \n",
    "        def get_sumstats_cur(self):\n",
    "            sst_df_lst = []\n",
    "            for i, geno_dt in self.reg_dt.items():\n",
    "                sst_df = geno_dt['sst_df']\n",
    "                sst_df_lst.append(sst_df)\n",
    "            sst_df = pd.concat(sst_df_lst, axis=0)\n",
    "            return sst_df\n",
    "\n",
    "        def get_stansda(self, standardizer='unit'):\n",
    "            if not standardizer=='unit':\n",
    "                raise NotImplementedError('contact dev')\n",
    "            \n",
    "            if hasattr(self, 'stansda'):\n",
    "                if type(self.stansda) is UnitTrained:\n",
    "                    return self.stansda\n",
    "                else:\n",
    "                    raise NotImplementedError('contact dev')\n",
    "                    \n",
    "            standardizer_list = []\n",
    "            for i, geno_dt in self.reg_dt.items():\n",
    "                if 'stansda' in geno_dt.keys():\n",
    "                    if type(geno_dt['stansda']) is UnitTrained:\n",
    "                        standardizer_list.append(geno_dt['stansda'])\n",
    "\n",
    "            assert np.all([type(stan) is UnitTrained for stan in standardizer_list])            \n",
    "            sid = np.concatenate([stan.sid for stan in standardizer_list])\n",
    "            assert np.unique(sid).shape[0] == sid.shape[0]\n",
    "\n",
    "            stats = np.concatenate([stan.stats for stan in standardizer_list], dtype=self.dtype)\n",
    "            combined_unit_standardizer = UnitTrained(sid, stats)\n",
    "            self.stansda = combined_unit_standardizer\n",
    "            return combined_unit_standardizer\n",
    "\n",
    "''\n",
    "class LinkageData(BaseLinkageData):\n",
    "    pass\n",
    "\n",
    "def load_linkagedata(fn):\n",
    "    curfn = glob.glob(fn.format_map(defaultdict(lambda:'*')))[-1]\n",
    "    master_dt = json.loads(pd.read_hdf(curfn, key='master_dt').loc[0,0])\n",
    "    master_dt['curdn'] = os.path.dirname(curfn)\n",
    "    return LinkageData(master_dt=master_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "282.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
