{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/mennowitteveen/pgsbenchmark/dev) \n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/mennowitteveen/pgsbenchmark/dev?labpath=nbs/loaders.ipynb)\n",
    "\n",
    "[![Google Colab](https://badgen.net/badge/Launch/on%20Google%20Colab/blue?icon=terminal)](https://colab.research.google.com/github/SMPyBandits/SMPyBandits/blob/master/notebooks/Do_we_even_need_UCB.ipynb)\n",
    "\n",
    "\n",
    "[![Google Colab](https://badgen.net/badge/Launch/on%20Google%20Colab/blue?icon=terminal)](https://colab.research.google.com/github/mennowitteveen/pgsbenchmark/blob/dev/nbs/loaders.ipynb)\n",
    "\n",
    "https://github.com/mennowitteveen/pgsbenchmark/blob/dev/nbs/loaders.ipynb\n",
    "\n",
    "https://github.com/binder-examples/jupyter-extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.core.magic import register_cell_magic\n",
    "\n",
    "# @register_cell_magic\n",
    "# def write_and_run(line, cell):\n",
    "#     argz = line.split()\n",
    "#     file = argz[-1]\n",
    "#     mode = 'w'\n",
    "#     if len(argz) == 2 and argz[0] == '-a':\n",
    "#         mode = 'a'\n",
    "#     with open(file, mode) as f:\n",
    "#         f.write(cell)\n",
    "#     get_ipython().run_cell(cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinkageData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     91,
     174,
     177,
     276,
     295,
     311,
     368,
     372,
     383,
     431,
     457,
     461,
     472,
     476,
     486,
     488,
     498,
     503,
     516,
     527,
     558,
     560,
     569,
     575,
     611,
     622,
     663,
     676
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../pgsbenchmark/loaders.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../pgsbenchmark/loaders.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "LinkageData\n",
    "durr tst\n",
    "\"\"\"\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import linalg\n",
    "from sys import getsizeof\n",
    "\n",
    "import warnings, importlib, json, os, glob\n",
    "from collections import OrderedDict, deque, defaultdict\n",
    "from pysnptools.standardizer import Unit, UnitTrained\n",
    "import pysnptools as pst\n",
    "# import pysnptools.util as pstutil\n",
    "# from pysnptools.standardizer import UnitTrained\n",
    "# from dataclasses import dataclass\n",
    "\n",
    "class SqrtNinv(Unit):\n",
    "    def __init__(self):\n",
    "        super(SqrtNinv, self).__init__()\n",
    "\n",
    "class BaseLinkageData():\n",
    "\n",
    "    def __init__(self, *, sst_df=None, regdef_df=None, master_dt=None, #There should be sst_df or master_dt\n",
    "                 srd=None, sda_standardizer=Unit,\n",
    "                 prd=None, pda_standardizer=Unit,\n",
    "                 lrd=None, lda_standardizer=None,\n",
    "                 grd=None, gda_standardizer=False,\n",
    "                 \n",
    "                 shift=0, cm=None, _setzero=True, ddof=0, #ddof should remain 0 for now\n",
    "                 \n",
    "                 clear_xda=True, # Refactor with _clear?\n",
    "                 clear_linkage=False,\n",
    "                 compute_sumstats=False,\n",
    "                 calc_allelefreq=False,\n",
    "                 intersect_apply=True,\n",
    "                 \n",
    "                 _onthefly_retrieval=True, # These underscore options are the advanced developer options\n",
    "                 _save_vars = ['L','D','R','sst_df'],\n",
    "                 _clear_vars = ['L','D','R'],\n",
    "                 _cross_chrom_ld = False,\n",
    "                 \n",
    "                 gb_size_limit=10., dtype='float32', verbose=False):\n",
    "        \n",
    "        if True:\n",
    "            # bim and fam df have to be supplied because pysnptools halvely\n",
    "            # implemented these portions of the genetic data into their object\n",
    "            # meaning that srd cannot be relied uppon\n",
    "            excl_lst = ['self','kwg_dt','excl_lst']\n",
    "            kwg_dt = {key: item for key, item in locals().items() if not (key in excl_lst)}\n",
    "            for key, item in locals().items():\n",
    "                if not (key in excl_lst): \n",
    "                    self.__setattr__(key, item)\n",
    "            # New rule: blx have to be created from the inside\n",
    "            # Perhaps later it can be made into a special load instead of a compute\n",
    "\n",
    "            # first-checks & inits:\n",
    "            if cm is not None: assert cm > 0\n",
    "            if lrd is not None: raise NotImplementedError('lrd not possible atm.')\n",
    "            if grd is not None:\n",
    "                assert gda_standardizer or (gda_standardizer is None)\n",
    "            assert type(compute_sumstats) is bool\n",
    "            if ddof != 0: raise NotImplementedError('delta deg. of freedom have to 0 for this version')\n",
    "            self.reg_dt = dict()\n",
    "            self.cur_total_size_in_gb = 0.0\n",
    "            self.xda_q = deque()\n",
    "            [self.xda_q.append((-1,'')) for _ in range(5)]  # put 5x -1 in queue\n",
    "            self.reloaded_xda_cnt = 0\n",
    "            self._fn_lst = []\n",
    "\n",
    "            # Checks            \n",
    "            if srd is not None:\n",
    "                assert type(sst_df) is pd.DataFrame\n",
    "                self._check_xrd()\n",
    "                assert isinstance(sst_df, pd.DataFrame)\n",
    "                assert isinstance(regdef_df, pd.DataFrame)\n",
    "                self.init_regions()\n",
    "            elif master_dt is not None:\n",
    "                # Fill attributes in case master_dt is present:\n",
    "                for key, item in master_dt.items():\n",
    "                    setattr(self, key, item)\n",
    "                reg_dt=dict()\n",
    "                for pre_i, geno_dt in self.reg_dt.items(): reg_dt[int(pre_i)] = geno_dt\n",
    "                self.reg_dt = reg_dt # An ugly type conversion hack cause json does not allow i to be integer, but forces it to be a string.\n",
    "            else:\n",
    "                raise Exception('Essentials not present')\n",
    "\n",
    "    def _check_xrd(self):\n",
    "\n",
    "        if self.srd is not None:\n",
    "            assert pst.snpreader.SnpReader in self.srd.__class__.__mro__\n",
    "\n",
    "        if self.prd is not None:\n",
    "            n_start = len(self.prd.iid)\n",
    "            n_pheno = self.prd.shape[1]\n",
    "            if n_pheno > 1: raise NotImplementedError(f'only one pheno in prd allowed {n_pheno} detected ({prd.col}), for now. remove other phenos')\n",
    "            self.srd, self.prd = pst.util.intersect_apply([self.srd, self.prd])\n",
    "            if len(self.prd.iid) != n_start:\n",
    "                warnings.warn('Number of samples do not match up after internal intersection, samples were lost:' \n",
    "                              f'{n_start - len(self.prd.iid)}, start = {n_start}, after_intersection = {len(self.prd.iid)}')\n",
    "                if not self.intersect_apply: raise Exception('Intersection was required, but may not performed. Hence raising this error.')\n",
    "\n",
    "        if self.grd is not None:\n",
    "            # Check alignment for now, auto alignment needs work cause iid stuffs:\n",
    "            if self.srd is not None:\n",
    "                if not np.all(self.grd.sid == self.srd.sid):\n",
    "                    raise Exception('snps of grd and srd not matching up, align first,'\n",
    "                                    ' auto align will be implemented later')\n",
    "            else:\n",
    "                raise NotImplementedError('Not sure what to do with grd if no srd is present. not implemented.')\n",
    "        \n",
    "    ###########################\n",
    "    # Regions Administration:\n",
    "    if True:\n",
    "\n",
    "        def init_regions(self):\n",
    "            do_beta_moving = ('beta_mrg' in self.sst_df.columns)\n",
    "            if not do_beta_moving:\n",
    "                warnings.warn('No \\'beta_mrg\\' column detected in sst_df! This means that no summary stats were detected.')\n",
    "            else:\n",
    "                assert 'n_eff' in self.sst_df.columns\n",
    "            cur_chrom = None\n",
    "            i = 0; n_snps_cumsum = 0\n",
    "            sst_df_lst = []\n",
    "            for reg_cnt, (_, row) in enumerate(self.regdef_df.iterrows()):\n",
    "                # Move region into specialized dictionary\n",
    "                regid = row['regid'];\n",
    "                chrom = row['chrom']\n",
    "                start = row['start'];\n",
    "                stop  = row['stop']\n",
    "\n",
    "                # Map Variants to region\n",
    "                ind = self.sst_df.chrom == chrom\n",
    "                ind = (self.sst_df['pos'] >= start) & ind\n",
    "                ind = (self.sst_df['pos'] < stop) & ind\n",
    "                sid = self.sst_df['snp'][ind].values\n",
    "                indices = self.srd.sid_to_index(sid)  # if sid not strickly present this will give an error!\n",
    "                n_snps_reg = len(indices)\n",
    "                if n_snps_reg == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    geno_dt = dict(regid=regid,\n",
    "                                   chrom=chrom,\n",
    "                                   start=start,\n",
    "                                   stop=stop,\n",
    "                                   start_j=n_snps_cumsum)\n",
    "                    n_snps_cumsum += n_snps_reg\n",
    "                    geno_dt['stop_j'] = n_snps_cumsum\n",
    "                    sst_df = self.sst_df[ind].copy(); sst_df['i'] = i\n",
    "                    geno_dt['sst_df'] = sst_df\n",
    "                    assert geno_dt['start_j'] == sst_df.index[0]; sst_df_lst.append(sst_df)\n",
    "                    assert geno_dt['stop_j']  == sst_df.index[-1] + 1\n",
    "                    if do_beta_moving:\n",
    "                        geno_dt['beta_mrg'] = geno_dt['sst_df']['beta_mrg'].copy().values[:, np.newaxis]\n",
    "                        assert len(geno_dt['beta_mrg'].shape) == 2\n",
    "                    if self.srd is not None:\n",
    "                        geno_dt['srd'] = self.srd[:, indices]\n",
    "                        geno_dt['stansda'] = self.sda_standardizer() if self.sda_standardizer is not None else None\n",
    "                    else:\n",
    "                        raise NotImplementedError()\n",
    "                    if self.grd is not None:\n",
    "                        geno_dt['grd'] = self.grd[:, indices]\n",
    "                        geno_dt['stangda'] = self.gda_standardizer() if self.gda_standardizer is not None else None\n",
    "                    # Count up if things are actually stored in reg_dt\n",
    "                    self.reg_dt[i] = geno_dt\n",
    "                    i += 1\n",
    "            self.n_snps_total = n_snps_cumsum\n",
    "            sst_df = pd.concat(sst_df_lst, axis=0)\n",
    "            self.sst_df = sst_df\n",
    "\n",
    "        def get_i_list(self):\n",
    "            return list(self.reg_dt.keys())\n",
    "\n",
    "        def _load_all_snpdata(self):\n",
    "            # load all regions\n",
    "            for i, geno_dt in self.reg_dt.items():\n",
    "                sda = geno_dt['srd'].read(dtype=self.dtype)\n",
    "                stansda = sda.train_standardizer(apply_in_place=True,\n",
    "                                                 standardizer=geno_dt['stansda'])\n",
    "                geno_dt['sda'] = sda\n",
    "                geno_dt['stansda'] = stansda\n",
    "\n",
    "    ###########################\n",
    "    ## Compute: ###############\n",
    "\n",
    "    # Local Linkage Stuff: ####\n",
    "    if True:\n",
    "    \n",
    "        def compute_linkage_sameregion(self, *, i):\n",
    "            return self.compute_linkage_shiftregion(i=i, shift=0)\n",
    "\n",
    "        def regions_compatible(self, *, i, j):\n",
    "            try:\n",
    "                if self.reg_dt[i]['chrom'] == self.reg_dt[j]['chrom']:\n",
    "                    res = True\n",
    "                elif self._cross_chrom_ld:\n",
    "                    res = True\n",
    "                else:\n",
    "                    res = False\n",
    "            except Exception as e:\n",
    "                if (not (i in self.reg_dt.keys())) or (not (j in self.reg_dt.keys())):\n",
    "                    res = False\n",
    "                else:\n",
    "                    raise e\n",
    "            return res\n",
    "\n",
    "        def compute_linkage_shiftregion(self, *, i, shift):\n",
    "            j = i + shift\n",
    "            if self.regions_compatible(i=i, j=j):\n",
    "                self_sda = self.get_sda(i=i)\n",
    "                dist_sda = self.get_sda(i=j)\n",
    "                n = len(self_sda.iid)\n",
    "                S_shift = self_sda.val.T.dot(dist_sda.val) / (n - self.ddof)\n",
    "                return S_shift\n",
    "            else:\n",
    "                self_sda = self.get_sda(i=i)\n",
    "                return np.zeros((self_sda.val.shape[1], 0))\n",
    "\n",
    "        def compute_linkage_cmfromregion(self, *, i, cm):\n",
    "            geno_dt = self.reg_dt[i]; lst = []\n",
    "            if cm < 0: # Doing left:\n",
    "                stop_j   = geno_dt['start_j']\n",
    "                cm_left  = geno_dt['sst_df'].loc[stop_j]['cm'] \n",
    "                slc_df = self.sst_df.loc[:stop_j-1]\n",
    "                slc_df = slc_df[slc_df.chrom==geno_dt['chrom']]\n",
    "                slc_df = slc_df[slc_df.cm > (cm_left + cm)]\n",
    "                start_i = slc_df['i'].min()\n",
    "                start_i = -7 if np.isnan(start_i) else start_i\n",
    "                for cur_i in range(start_i, i):\n",
    "                    lst.append(self.compute_linkage_shiftregion(i=i, shift=cur_i-i))\n",
    "                    if start_i == -7: break\n",
    "                L = np.concatenate(lst, axis=1)[:,-slc_df.shape[0]:] # concat & clip\n",
    "                if self._setzero:\n",
    "                    cms_reg    = geno_dt['sst_df']['cm'].values\n",
    "                    cms_distal = slc_df['cm'].values\n",
    "                    cms_L      =  cms_distal[np.newaxis,:] - cms_reg[:,np.newaxis]\n",
    "                    setzero_L  = cms_L < cm\n",
    "                    L[setzero_L] = 0\n",
    "                    assert L.shape == setzero_L.shape\n",
    "                return L\n",
    "            else:\n",
    "                start_j   = geno_dt['stop_j']\n",
    "                cm_right  = geno_dt['sst_df'].loc[start_j-1]['cm']\n",
    "                slc_df = self.sst_df.loc[start_j:]\n",
    "                slc_df = slc_df[slc_df.chrom==geno_dt['chrom']]\n",
    "                slc_df = slc_df[slc_df.cm < (cm_right + cm)]\n",
    "                stop_i = slc_df['i'].max()\n",
    "                stop_i = i+2 if np.isnan(stop_i) else stop_i + 1\n",
    "                for cur_i in range(i+1, stop_i):\n",
    "                    lst.append(self.compute_linkage_shiftregion(i=i, shift=cur_i-i))\n",
    "                R = np.concatenate(lst, axis=1)[:,:slc_df.shape[0]] # concat & clip\n",
    "                if self._setzero:\n",
    "                    cms_reg    = geno_dt['sst_df']['cm'].values\n",
    "                    cms_distal = slc_df['cm'].values\n",
    "                    cms_R     =  cms_distal[np.newaxis,:] - cms_reg[:,np.newaxis]\n",
    "                    setzero_R = cms_R > cm\n",
    "                    R[setzero_R] = 0\n",
    "                    assert R.shape == setzero_R.shape\n",
    "                return R\n",
    "        \n",
    "    # Misc Stuff: #############\n",
    "    if True:\n",
    "    \n",
    "        def compute_sumstats_region(self, *, i):\n",
    "            geno_dt = self.reg_dt[i]\n",
    "            sda = self.get_sda(i=i)\n",
    "            X = sda.val\n",
    "            y = self.get_pda().val\n",
    "            n = len(y)\n",
    "            c_reg = X.T.dot(y) / (n - self.ddof)\n",
    "            return c_reg   \n",
    "\n",
    "        def compute_allelefreq_region(self, *, i):\n",
    "            # Speed might be improved by using dot prod here, instead of sums\n",
    "            # np.unique was way slower (5x)\n",
    "            geno_dt = self.reg_dt[i]\n",
    "            n, p_blk = sda.val.shape\n",
    "            sst_df = geno_dt['sst_df'].copy()\n",
    "            cnt0   = np.sum(sda.val==0, axis=0)\n",
    "            cnt1   = np.sum(sda.val==1, axis=0)\n",
    "            cnt2   = np.sum(sda.val==2, axis=0)\n",
    "            cntnan = np.sum(np.isnan(sda.val), axis=0)\n",
    "            assert np.allclose(cnt0 + cnt1 + cnt2 + cntnan, n)\n",
    "            sst_df['altcnt=0']   = cnt0\n",
    "            sst_df['altcnt=1']   = cnt1\n",
    "            sst_df['altcnt=2']   = cnt2\n",
    "            sst_df['altcnt=nan'] = cntnan\n",
    "            sst_df['altfreq']    = (cnt1 + cnt2)/(n - cntnan)\n",
    "            sst_df['missfreq']   = 1 - cntnan/n\n",
    "            return sst_df\n",
    "\n",
    "        def compute_ldscores_region(self, *, i):\n",
    "            sst_df = self.reg_dt[i]['sst_df'].copy()\n",
    "            L = self.get_left_linkage_region(i=i)\n",
    "            D = self.get_auto_linkage_region(i=i)\n",
    "            R = self.get_right_linkage_region(i=i)\n",
    "            for k, j in enumerate(sst_df.index):\n",
    "                slds = np.sum(L[k]**2) + np.sum(D[k]**2) + np.sum(R[k]**2)\n",
    "                sst_df.loc[j, 'lds'] = np.sqrt(slds)\n",
    "            return sst_df\n",
    "        \n",
    "    ############################\n",
    "    ## Retrieve: ###############\n",
    "    \n",
    "    # Local Linkage: ############\n",
    "    if True:\n",
    "    \n",
    "        def retrieve_linkage_allregions(self):\n",
    "            for i, geno_dt in self.reg_dt.items():\n",
    "                self.retrieve_linkage_region(i=i)\n",
    "            if self.verbose:   print('\\nDone')\n",
    "            if self.clear_xda: self.clear_all_xda()\n",
    "\n",
    "        def retrieve_linkage_region(self, *, i):\n",
    "\n",
    "            geno_dt = self.reg_dt[i]\n",
    "            if 'store_dt' in geno_dt.keys():\n",
    "                self.load_linkage_region(i=i)\n",
    "            shift = self.shift; cm = self.cm\n",
    "            compute_sumstats = self.compute_sumstats\n",
    "\n",
    "            if 'L' in geno_dt.keys():\n",
    "                if 'D' in geno_dt.keys():\n",
    "                    if 'R' in geno_dt.keys():\n",
    "                        return None  # everything is done now.\n",
    "\n",
    "            if self.verbose: print(f'Computing LD for region #{i} on chr{geno_dt[\"chrom\"]}', end='\\r')\n",
    "            # Refactor: if linkage is only in blocks this code will lead to recomputation...\n",
    "            if (shift > 0):\n",
    "                L_lst = []\n",
    "                R_lst = []\n",
    "                for cur_shift in range(1, shift + 1):\n",
    "                    L_lst.append(self.compute_linkage_shiftregion(i=i, shift=-cur_shift))\n",
    "                    R_lst.append(self.compute_linkage_shiftregion(i=i, shift=cur_shift))\n",
    "\n",
    "                # Store Linkage in geno_dt\n",
    "                geno_dt['L'] = np.concatenate(L_lst[::-1], axis=1)  # L stands for left\n",
    "                geno_dt['D'] = self.compute_linkage_sameregion(i=i)  # Linkage within region, D is convention from LDpred 1\n",
    "                geno_dt['R'] = np.concatenate(R_lst, axis=1)  # R stands for right\n",
    "\n",
    "                # Indices needed for slicing and dicing matched variables (e.g. beta weights):\n",
    "                geno_dt['start_j_L'] = geno_dt['start_j'] - geno_dt['L'].shape[1]\n",
    "                geno_dt['stop_j_L'] = geno_dt['start_j']\n",
    "                geno_dt['start_j_R'] = geno_dt['stop_j']\n",
    "                geno_dt['stop_j_R'] = geno_dt['stop_j'] + geno_dt['R'].shape[1]\n",
    "\n",
    "            elif (shift==0) and (cm is None):  # Only same region has to be done.\n",
    "                geno_dt['D'] = self.compute_linkage_sameregion(i=i)\n",
    "\n",
    "            elif (shift==0) and cm > 0:\n",
    "                geno_dt['L'] = self.compute_linkage_cmfromregion(i=i, cm=-cm)\n",
    "                geno_dt['D'] = self.compute_linkage_sameregion(i=i)\n",
    "                geno_dt['R'] = self.compute_linkage_cmfromregion(i=i, cm=cm)\n",
    "\n",
    "                # Indices needed for slicing and dicing matched variables (e.g. beta weights):\n",
    "                geno_dt['start_j_L'] = geno_dt['start_j'] - geno_dt['L'].shape[1]\n",
    "                geno_dt['stop_j_L'] = geno_dt['start_j']\n",
    "                geno_dt['start_j_R'] = geno_dt['stop_j']\n",
    "                geno_dt['stop_j_R'] = geno_dt['stop_j'] + geno_dt['R'].shape[1]\n",
    "\n",
    "            if compute_sumstats:\n",
    "                self.retrieve_sumstats_region(i=i)\n",
    "              \n",
    "        def load_linkage_allregions(self):\n",
    "            for i, geno_dt in self.reg_dt.items():\n",
    "                self.load_linkage_region(i=i)\n",
    "            if self.verbose: print('\\nDone')\n",
    "            \n",
    "        def load_linkage_region(self, *, i):\n",
    "            geno_dt = self.reg_dt[i]\n",
    "            store_dt = geno_dt['store_dt']\n",
    "            for varname, file_dt in store_dt.items():\n",
    "                module = importlib.import_module('.'.join(file_dt['typestr'].split('.')[:-1]))\n",
    "                cname  = file_dt['typestr'].split('.')[-1]\n",
    "                CurClass = getattr(module, cname) # Retrieves module.submodule.submodule.. etc\n",
    "                curfullfn = os.path.join(self.curdn, file_dt['fn'])\n",
    "                geno_dt[varname] = CurClass(pd.read_hdf(curfullfn, key=file_dt['key']))\n",
    "                if self.verbose: print(f'loading: fn={curfullfn} key={file_dt[\"key\"]}'+' '*50, end='\\r')\n",
    "                \n",
    "        def save(self, fn, keyfmt='ld/chrom{chrom}/i{i}/{varname}', fmt='hdf5', mkdir=False, dn=None):\n",
    "            self.curdn = os.path.dirname(fn) if (dn is None) else dn\n",
    "            fn = os.path.basename(fn) if (dn is None) else fn\n",
    "            if mkdir: os.makedirs(self.curdn, exist_ok=True)\n",
    "            if (fmt != 'hdf5'): raise Exception(f'Only hdf5 file format supported atm, not {fmt}') \n",
    "            for i, geno_dt in self.reg_dt.items():\n",
    "                self.save_linkage_region(i=i, fn=fn)\n",
    "                \n",
    "            # Saving of 'logistical' data for the object\n",
    "            master_lst = [ 'shift', 'cm', '_setzero',\n",
    "             'clear_xda', 'clear_linkage', 'compute_sumstats', 'calc_allelefreq', \n",
    "             '_onthefly_retrieval', '_save_vars', '_clear_vars', \n",
    "             'gb_size_limit', 'dtype', 'verbose', 'n_snps_total']\n",
    "            geno_lst = ['regid','chrom','start','stop','start_j','stop_j',\n",
    "                        'start_j_L', 'stop_j_L', 'start_j_R', 'stop_j_R','store_dt']\n",
    "                \n",
    "            def caster(arg, types):\n",
    "                if np.isscalar(arg):\n",
    "                    if isinstance(arg, np.integer): arg = int(arg)\n",
    "                if type(arg) is int: return int(arg)\n",
    "                assert type(arg) in types\n",
    "                return arg\n",
    "\n",
    "            master_dt = dict(); maxlen = 20\n",
    "            for key in master_lst:\n",
    "                var = getattr(self, key)\n",
    "                if type(var) is list:\n",
    "                    for item in var:\n",
    "                        assert type(item) in (bool, str, float, int)\n",
    "                        if type(item) is str: assert (len(item) < maxlen)\n",
    "                elif type(var) is str:\n",
    "                        assert len(var) < maxlen\n",
    "                master_dt[key] = caster(var, (list, bool, float, int, str))\n",
    "\n",
    "            reg_dt = dict()\n",
    "            for i, geno_dt in self.reg_dt.items():\n",
    "                newgeno_dt = dict()\n",
    "                for key in geno_lst:\n",
    "                    if not (key in geno_dt.keys()): continue\n",
    "                    newgeno_dt[key] = caster(geno_dt[key], (str, int, dict))\n",
    "                reg_dt[i] = newgeno_dt\n",
    "            master_dt['reg_dt'] = reg_dt     \n",
    "            self._fn_lst = list(np.unique(self._fn_lst))\n",
    "            for curfn in self._fn_lst:\n",
    "                pd.DataFrame([json.dumps(master_dt)]).to_hdf(os.path.join(self.curdn, curfn), key='master_dt')\n",
    "            \n",
    "            if self.verbose: print('\\nDone')\n",
    "\n",
    "        def save_linkage_region(self, *, i, fn, keyfmt='ld/chrom{chrom}/i{i}/{varname}'): \n",
    "            # using 'store' instead of 'save' to indicate a connected relationship with \n",
    "            # the files used for this storage.\n",
    "            geno_dt = self.reg_dt[i]\n",
    "            chrom = geno_dt['chrom']\n",
    "            curdn = self.curdn\n",
    "            store_dt = dict() #geno_dt['store_dt']\n",
    "            for varname, var in geno_dt.items():\n",
    "                if varname in self._save_vars:\n",
    "                    curfn  = fn.format(**locals())\n",
    "                    key    = keyfmt.format(**locals())\n",
    "                    var    = geno_dt[varname]\n",
    "                    vartype = type(var)\n",
    "                    if vartype is np.ndarray: vartype = var.dtype.type\n",
    "                    curfullfn = os.path.join(curdn,curfn)\n",
    "                    pd.DataFrame(var).to_hdf(curfullfn, key=key)\n",
    "                    file_dt = dict(fn=curfn, key=key, \n",
    "                                   typestr=vartype.__module__+'.'+vartype.__name__)\n",
    "                    store_dt[varname] = file_dt\n",
    "                    self._fn_lst.append(curfn)\n",
    "                    if self.verbose: print(f'saving: fn={curfullfn} key={key}'+' '*50,end='\\r')\n",
    "            geno_dt['store_dt'] = store_dt\n",
    "                   \n",
    "    # SumStat: ##############\n",
    "    if True:\n",
    "\n",
    "        def retrieve_sumstats_allregions(self):\n",
    "            for i, geno_dt in self.reg_dt.items():\n",
    "                self.retrieve_sumstats_region(i=i)\n",
    "\n",
    "        def retrieve_sumstats_region(self, *, i):\n",
    "            geno_dt = self.reg_dt[i] \n",
    "            sst_df  = geno_dt['sst_df']\n",
    "            if 'beta_mrg' in geno_dt.keys():\n",
    "                return None # Sumstat present so no need to compute anything.\n",
    "            geno_dt['beta_mrg'] = self.compute_sumstats_region(i=i)\n",
    "            if not 'beta_mrg' in sst_df.columns:\n",
    "                geno_dt['sst_df']['beta_mrg'] = geno_dt['beta_mrg']\n",
    "                \n",
    "        retrieve_betamrg_region = retrieve_sumstats_region\n",
    "\n",
    "        def retrieve_ldscores_allregions(self):\n",
    "            for i, geno_dt in self.reg_dt.items():\n",
    "                self.retrieve_ldscores_region(i=i)\n",
    "\n",
    "        def retrieve_ldscores_region(self, *, i):\n",
    "            geno_dt = self.reg_dt[i]\n",
    "            sst_df = geno_dt['sst_df']\n",
    "            if not 'lds' in sst_df.columns:\n",
    "                newsst_df = self.compute_ldscores_region(i=i)\n",
    "                geno_dt['sst_df'] = newsst_df\n",
    "            if self.clear_linkage:\n",
    "                self.clear_linkage_region(i=i)\n",
    "\n",
    "    # Clearing Functions: #####\n",
    "    if True:\n",
    "\n",
    "        def clear_all_xda(self):\n",
    "            while len(self.xda_q) != 0:\n",
    "                i_2_rm, key = self.xda_q.popleft()\n",
    "                if i_2_rm == -1:\n",
    "                    continue  # Continue to next iter if encountering a padding -1\n",
    "                rmgeno_dt = self.reg_dt[i_2_rm]\n",
    "                self.cur_total_size_in_gb -= getsizeof(rmgeno_dt[key].val) / 1024 ** 3\n",
    "                rmgeno_dt.pop(key)\n",
    "            [self.xda_q.append((-1,'')) for _ in range(5)]  # put 5x -1 in queue\n",
    "            \n",
    "        def clear_linkage_allregions(self):\n",
    "            for i, geno_dt in self.reg_dt.items():\n",
    "                self.clear_linkage_region(i=i)\n",
    "            if self.verbose: print('\\nDone')\n",
    "\n",
    "        def clear_linkage_region(self, *, i):\n",
    "            geno_dt = self.reg_dt[i]\n",
    "            key_lst = list(geno_dt.keys())\n",
    "            for key in key_lst:\n",
    "                if key in self._clear_vars:\n",
    "                    geno_dt.pop(key)\n",
    "            if self.verbose: print(f'Cleared linkage region #{i} on chr{geno_dt[\"chrom\"]}', end='\\r'); sys.stdout.flush()\n",
    "            \n",
    "\n",
    "    ############################\n",
    "    ## Get: ####################\n",
    "    \n",
    "    # Local Linkage: ###########\n",
    "    if True:\n",
    "\n",
    "        def get_auto_linkage_region(self, *, i):\n",
    "            return self.get_specificied_linkage_region(i=i, shiftletter='D')\n",
    "\n",
    "        def get_left_linkage_region(self, *, i):\n",
    "            return self.get_specificied_linkage_region(i=i, shiftletter='L')\n",
    "\n",
    "        def get_right_linkage_region(self, *, i):\n",
    "            return self.get_specificied_linkage_region(i=i, shiftletter='R')\n",
    "\n",
    "        def get_specificied_linkage_region(self, *, i, shiftletter):\n",
    "            try:\n",
    "                return self.reg_dt[i][shiftletter]\n",
    "            except KeyError as e:\n",
    "                if self._onthefly_retrieval:\n",
    "                    if '_glocal' in shiftletter:\n",
    "                        self.retrieve_linkage_region_glocalshiftwindow(i=i)\n",
    "                    elif shiftletter in 'LDR':\n",
    "                        self.retrieve_linkage_region(i=i)\n",
    "                    elif shiftletter == 'Z':\n",
    "                        self.retrieve_linkage_region_global(i=i)\n",
    "                    else:\n",
    "                        raise Exception(f'shiftletter={shiftletter}, on-the-fly retrieval not a valid option.')\n",
    "                    try:\n",
    "                        return self.reg_dt[i][shiftletter]\n",
    "                    except Exception as e:\n",
    "                        print('Failed, eventough on-the-fly retrieval was attempted')\n",
    "                        raise e\n",
    "                else:\n",
    "                    raise Exception('on-the-fly retrieval blocked, set _onthefly_retrieval=True if desired')\n",
    "\n",
    "        def get_auto_range_region(self, *, i):\n",
    "            return self.reg_dt[i]['start_j'], self.reg_dt[i]['stop_j']\n",
    "\n",
    "        def get_left_range_region(self, *, i):\n",
    "            return self.reg_dt[i]['start_j_L'], self.reg_dt[i]['stop_j_L']\n",
    "\n",
    "        def get_right_range_region(self, *, i):\n",
    "            return self.reg_dt[i]['start_j_R'], self.reg_dt[i]['stop_j_R']\n",
    "\n",
    "    # Sumstats: #################\n",
    "    if True:\n",
    "\n",
    "        def get_beta_marginal_full(self):\n",
    "            beta_mrg_lst = []\n",
    "            for i, geno_dt in self.reg_dt.items():\n",
    "                beta_mrg_lst.append(geno_dt['beta_mrg'])\n",
    "            beta_mrg_full = np.concatenate(beta_mrg_lst)\n",
    "            return beta_mrg_full\n",
    "\n",
    "        get_beta_marginal = get_beta_marginal_full\n",
    "        \n",
    "        def get_beta_marginal_region(self, *, i):\n",
    "            return self.reg_dt[i]['beta_mrg']\n",
    "\n",
    "    # Xda: ######################\n",
    "    if True:\n",
    "    \n",
    "        def get_sda(self, *, i):\n",
    "            geno_dt = self.reg_dt[i]\n",
    "            if 'sda' in geno_dt.keys():\n",
    "                return geno_dt['sda']\n",
    "            else:\n",
    "                if 'srd' in geno_dt.keys():\n",
    "                    sda = geno_dt['srd'].read(dtype=self.dtype)\n",
    "                    sda, stansda = sda.standardize(standardizer=geno_dt['stansda'], return_trained=True)\n",
    "                    geno_dt['sda'] = sda\n",
    "                    geno_dt['stansda'] = stansda\n",
    "\n",
    "                    if 'loaded_sda' in geno_dt.keys():\n",
    "                        self.reloaded_xda_cnt += 1\n",
    "                        if self.reloaded_xda_cnt in [5, 20, 100, 400]:\n",
    "                            warnings.warn(\n",
    "                                f'Reloaded sda for the {self.reloaded_xda_cnt}\\'th time. This causes memory swapping,'\n",
    "                                ' that might make the computation of linkage quite slow.'\n",
    "                                'Probably because memory limits and/or linkage size.')\n",
    "                    # Size determination and accounting:\n",
    "                    geno_dt['loaded_sda']=True\n",
    "                    self.cur_total_size_in_gb += getsizeof(sda.val) / 1024 ** 3\n",
    "                    self.xda_q.append((i,'sda'))  # put respective i in queue.\n",
    "                    while self.cur_total_size_in_gb > self.gb_size_limit:  # Keep removing till size is ok\n",
    "                        i_2_rm, key = self.xda_q.popleft()\n",
    "                        if i_2_rm == -1:\n",
    "                            continue  # Continue to next iter if encountering a padding -1\n",
    "                        rmgeno_dt = self.reg_dt[i_2_rm]\n",
    "                        self.cur_total_size_in_gb -= getsizeof(rmgeno_dt[key].val) / 1024 ** 3\n",
    "                        rmgeno_dt.pop(key)\n",
    "                        if len(self.xda_q) <= 4:\n",
    "                            raise Exception('The memory footprint of current settings is too high, '\n",
    "                                            'reduce blocksize and/or correction windows or increase memory limits.')\n",
    "                    return sda\n",
    "                else:\n",
    "                    raise Exception(f'No srd or sda found in region i={i}, this is not supposed to happen.')\n",
    "\n",
    "        def get_pda(self):\n",
    "            if not hasattr(self, 'pda'):\n",
    "                pda = self.prd.read(dtype=self.dtype)\n",
    "                pda, self.stanpda = pda.standardize(return_trained=True,\n",
    "                                standardizer=self.pda_standardizer())\n",
    "                self.pda = pda\n",
    "            return self.pda\n",
    "    \n",
    "    # Utils: ####################\n",
    "    if True:\n",
    "    \n",
    "        def get_sumstats_cur(self):\n",
    "            sst_df_lst = []\n",
    "            for i, geno_dt in self.reg_dt.items():\n",
    "                sst_df = geno_dt['sst_df']\n",
    "                sst_df_lst.append(sst_df)\n",
    "            sst_df = pd.concat(sst_df_lst, axis=0)\n",
    "            return sst_df\n",
    "\n",
    "        def get_stansda(self, standardizer='unit'):\n",
    "            if not standardizer=='unit':\n",
    "                raise NotImplementedError('contact dev')\n",
    "            \n",
    "            if hasattr(self, 'stansda'):\n",
    "                if type(self.stansda) is UnitTrained:\n",
    "                    return self.stansda\n",
    "                else:\n",
    "                    raise NotImplementedError('contact dev')\n",
    "                    \n",
    "            standardizer_list = []\n",
    "            for i, geno_dt in self.reg_dt.items():\n",
    "                #(not 'stansda' in geno_dt.keys())\n",
    "                if (not type(geno_dt['stansda']) is UnitTrained) & self._onthefly_retrieval:\n",
    "                    self.retrieve_linkage_region(i=i)\n",
    "                if type(geno_dt['stansda']) is UnitTrained:\n",
    "                    standardizer_list.append(geno_dt['stansda'])\n",
    "                else:\n",
    "                    raise Exception('No standardizer detected. Compute this first. Contact dev if issue persists.')\n",
    "\n",
    "            assert np.all([type(stan) is UnitTrained for stan in standardizer_list])            \n",
    "            sid = np.concatenate([stan.sid for stan in standardizer_list])\n",
    "            assert np.unique(sid).shape[0] == sid.shape[0]\n",
    "\n",
    "            stats = np.concatenate([stan.stats for stan in standardizer_list])\n",
    "            combined_unit_standardizer = UnitTrained(sid, stats)\n",
    "            self.stansda = combined_unit_standardizer\n",
    "            return combined_unit_standardizer\n",
    "\n",
    "''\n",
    "class LinkageData(BaseLinkageData):\n",
    "    pass\n",
    "\n",
    "def load_bimfam(base_fn, strip=True, bim=True, fam=True, fil_arr=None):\n",
    "    if strip and (base_fn.split('.')[-1] in ('bim','fam','bed')): base_fn = '.'.join(base_fn.split('.')[:-1])\n",
    "    bim_df = pd.read_csv(base_fn + '.bim', delim_whitespace=True, header=None, \n",
    "                         names=['chrom', 'snp', 'cm', 'pos', 'A1', 'A2']) if bim else None\n",
    "    fam_df = pd.read_csv(base_fn + '.fam', delim_whitespace=True, header=None, \n",
    "                         names=['fid', 'iid', 'father', 'mother', 'gender', 'trait']) if fam else None\n",
    "    \n",
    "    if fil_arr is not None:\n",
    "        bim_df = bim_df[bim_df.snp.isin(fil_arr)]\n",
    "        bim_df = bim_df.reset_index(drop=True)\n",
    "        \n",
    "    return bim_df, fam_df\n",
    "\n",
    "def load_linkagedata(fn):\n",
    "    curfn = glob.glob(fn.format_map(defaultdict(lambda:'*')))[-1]\n",
    "    master_dt = json.loads(pd.read_hdf(curfn, key='master_dt').loc[0,0])\n",
    "    master_dt['curdn'] = os.path.dirname(curfn)\n",
    "    return LinkageData(master_dt=master_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../pgsbenchmark/loaders.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/proj/repos/pgsbenchmark/nbs\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rm -r ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '../pgsbenchmark/data': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!cp ../../../mjwt/mjwt/tools.py ../pgsbenchmark/\n",
    "!rm -r ../pgsbenchmark/data\n",
    "!mkdir ../pgsbenchmark/data\n",
    "!cp -r ../../../data/mini ../pgsbenchmark/data/\n",
    "!cp -r ../../../data/defs ../pgsbenchmark/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp -r ../../../data/mini ../pgsbenchmark/data/\n",
    "!cp -r ../../../data/defs ../pgsbenchmark/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ../../../mjwt/mjwt/tools.py ../pgsbenchmark/\n",
    "!mkdir ../pgsbenchmark/data\n",
    "!cp -r ../../../data/mini ../pgsbenchmark/data/\n",
    "!cp -r ../../../data/defs ../pgsbenchmark/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regdef_df --> Checks that are needed: region ranges count upward, check if regid is present, else make it.\n",
    "# assert chrom contains no None's and nans\n",
    "# This should go different in the future:\n",
    "# https://github.com/scikit-learn/scikit-learn/blob/7e1e6d09bcc2eaeba98f7e737aac2ac782f0e5f1/sklearn/base.py#L31\n",
    "# clone objects instead of messing with classes.. skl -> clone(estimator)\n",
    "\n",
    "# A test of sst_df with all the other inputs should be done\n",
    "\n",
    "def _test_padding(self):\n",
    "    geno_dt = self.reg_dt[45]\n",
    "    implot(geno_dt['D'])\n",
    "type(linkdata)._test_padding = _test_padding\n",
    "linkdata._test_padding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://127.0.0.1:7090/notebooks/proj/ppb-paper/code/15__PPB-rerun---run%2Bsave-4-final-res.ipynb\n",
    "# this stuff is for creating pics of whole chromosomes:\n",
    "def fun(arg):\n",
    "    return asizeof(arg)/1024**3\n",
    "gb_dt = {}\n",
    "for i,  geno_dt in linkdata.reg_dt.items():\n",
    "    gb_dt[i] = {key: fun(geno_dt[key]) for key in 'LDR'}\n",
    "df = pd.DataFrame(gb_dt).T\n",
    "df.sum(),df.sum().sum()\n",
    "\n",
    "# Moar things:\n",
    "info_dt = mcres_dt['info_dt']\n",
    "df = pd.DataFrame(info_dt).T\n",
    "stop=df.iloc[:,-3:].applymap(lambda x: np.max(x)).max(axis=1)\n",
    "start=df.iloc[:,-3:].applymap(lambda x: np.min(x)).min(axis=1)\n",
    "ld_df = pd.DataFrame(dict(start=start,stop=stop))\n",
    "ld_df['ar'] = df['ar'].apply(np.array)\n",
    "\n",
    "fac = 10000\n",
    "p = 1117493\n",
    "M = np.zeros((fac,fac))\n",
    "dt = dict()\n",
    "for i, row in ld_df.iterrows():\n",
    "    chrom = linkdata.regdef_df.loc[i,'chrom']\n",
    "    ld_df.loc[i,'chrom'] = int(chrom)\n",
    "\n",
    "# rege  \n",
    "def fun(arg, ap):\n",
    "    res = pd.DataFrame(arg.to_list())\n",
    "    minv = np.min(res.values.flatten())\n",
    "    maxv = np.max(res.values.flatten())\n",
    "    return pd.DataFrame.from_dict(dict(minv=minv,maxv=maxv), orient='index').T\n",
    "for chrom, row in ld_df.groupby('chrom')['ar'].apply(fun,ap=np.min).droplevel(-1).iterrows():\n",
    "    a,b=int((row['minv']/p)*fac), int((row['maxv']/p)*fac)\n",
    "    M[a:b,a:b] = -1\n",
    "#     print(a,b)\n",
    "#     ewefwef\n",
    "#     if chrom > 1:\n",
    "#         rergerg\n",
    "# ld_df.groupby('chrom')['ar'].apply(fun,ap=np.max)\n",
    "    \n",
    "for i, row in ld_df.iterrows():\n",
    "    ystart, ystop = np.round(((row['ar']/p)*fac).astype(int))\n",
    "    xstart = np.round((row['start']/p)*fac).astype(int)\n",
    "    xstop = np.round((row['stop']/p)*fac).astype(int)\n",
    "    M[ystart:ystop,xstart:xstop] = 1\n",
    "implot(M)\n",
    "start, stop = 0,1000\n",
    "implot(M[start:stop,start:stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10350"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5*6*345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mjwt.tools import jobinfo, corr, implot, sizegb, psrc, beep, Timer, Struct as mStruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.56404383  0.37390618 -0.69397673  0.96153954 -0.75066267]\n",
      "[ 1.02947067 -0.11771206  0.3349158   0.33427038 -0.06006182]\n",
      "[ 3.30922771e-01  3.58586299e-01  1.15133979e-03 -1.64115643e+00\n",
      " -9.03058168e-02]\n",
      "[-1.20886132 -0.22098197 -1.16759257  0.14830859  0.47689666]\n",
      "[ 0.4166947  -0.80918126  0.5614036  -0.43858988 -0.65692014]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for i in range(5):\n",
    "    print(np.random.randn(5))\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !git pull\n",
    "# !git add loaders.ipynb\n",
    "# !git status\n",
    "# !git commit -m \"@dataclass removed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backlog\n",
    "\n",
    "**Loaders:**\n",
    "- Create empty arrays for L&R if shift==0\n",
    "- implot:\n",
    "    - enhance implot to do many plots on same line\n",
    "    - make ldplot, with appropriate coloring\n",
    "- Add os.path.expanduser to pysnptools\n",
    "- binder links\n",
    "- https://stackoverflow.com/questions/33358611/ipython-notebook-writefile-and-execute-cell-at-the-same-time\n",
    "\n",
    "**scores:**\n",
    "- on the fly generation of standardisation (s) for PPMC\n",
    "\n",
    "\n",
    "**misc:**\n",
    "- Make a run-code-cell if browser-tab or focus changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't evaluate or find in history: jupyter_notebook_cell_number\n"
     ]
    }
   ],
   "source": [
    "%recall jupyter_notebook_cell_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this\n",
      "cell: \n",
      "kjejkrgkjerg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%testfun this\n",
    "\n",
    "kjejkrgkjerg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotld(linkdata=linkdata, ppbres_dt=ppbres_dt)\n",
    "# def plotld(*,linkdata, ppbres_dt):\n",
    "#     #http://127.0.0.1:7090/notebooks/proj/ppb-paper/code/15__PPB-rerun---run%2Bsave-4-final-res.ipynb\n",
    "#     # this stuff is for creating pics of whole chromosomes:\n",
    "#     def fun(arg):\n",
    "#         #asizeof(arg)/1024**3 \"pympyler\" oid works better.\n",
    "#         return sizegb(arg)\n",
    "#     gb_dt = {}\n",
    "#     for i,  geno_dt in linkdata.reg_dt.items():\n",
    "#         gb_dt[i] = {key: fun(geno_dt[key]) for key in 'LDR'}\n",
    "#     df = pd.DataFrame(gb_dt).T\n",
    "#     df.sum(),df.sum().sum()\n",
    "\n",
    "#     # Moar things:\n",
    "#     info_dt = ppbres_dt['info_dt']\n",
    "#     df = pd.DataFrame(info_dt).T\n",
    "#     stop=df.iloc[:,-3:].applymap(lambda x: np.max(x)).max(axis=1)\n",
    "#     start=df.iloc[:,-3:].applymap(lambda x: np.min(x)).min(axis=1)\n",
    "#     ld_df = pd.DataFrame(dict(start=start,stop=stop))\n",
    "#     ld_df['ar'] = df['ar'].apply(np.array)\n",
    "\n",
    "\n",
    "#     p = linkdata.n_snps_total\n",
    "#     fac = p #4000\n",
    "#     M = np.zeros((fac,fac))\n",
    "#     dt = dict()\n",
    "#     for i, row in ld_df.iterrows():\n",
    "#         chrom = linkdata.regdef_df.loc[i,'chrom']\n",
    "#         ld_df.loc[i,'chrom'] = int(chrom)\n",
    "\n",
    "#     # rege  \n",
    "#     def fun(arg, ap):\n",
    "#         res = pd.DataFrame(arg.to_list())\n",
    "#         minv = np.min(res.values.flatten())\n",
    "#         maxv = np.max(res.values.flatten())\n",
    "#         return pd.DataFrame.from_dict(dict(minv=minv,maxv=maxv), orient='index').T\n",
    "#     for chrom, row in ld_df.groupby('chrom')['ar'].apply(fun,ap=np.min).droplevel(-1).iterrows():\n",
    "#         a,b=int((row['minv']/p)*fac), int((row['maxv']/p)*fac)\n",
    "#         M[a:b,a:b] = -1\n",
    "#     #     print(a,b)\n",
    "#     #     ewefwef\n",
    "#     #     if chrom > 1:\n",
    "#     #         rergerg\n",
    "#     # ld_df.groupby('chrom')['ar'].apply(fun,ap=np.max)\n",
    "\n",
    "#     for i, row in ld_df.iterrows():\n",
    "#         ystart, ystop = np.round(((row['ar']/p)*fac).astype(int))\n",
    "#         xstart = np.round((row['start']/p)*fac).astype(int)\n",
    "#         xstop = np.round((row['stop']/p)*fac).astype(int)\n",
    "#         M[ystart:ystop,xstart:xstop] = 1\n",
    "\n",
    "#     implot(M)\n",
    "# # start, stop = 0,1000\n",
    "# # implot(M[start:stop,start:stop])\n",
    "\n",
    "\n",
    "# display(metrix['ppbr2_df'])\n",
    "# display(metrix['bCb'])\n",
    "# display((metrix['BmBt'].values**2)/metrix['bCb'])\n",
    "\n",
    "\n",
    "# X = srd[:,:200].read().standardize().val\n",
    "# df = pd.DataFrame(X)\n",
    "# D = np.cov(X.T)\n",
    "# implot(D)\n",
    "\n",
    "# n_eff = X.shape[0]\n",
    "# np.allclose(D,X.T@X/n_eff)\n",
    "\n",
    "# n_eff = X.shape[0]-1\n",
    "# np.allclose(D,X.T@X/n_eff)\n",
    "\n",
    "\n",
    "# ppbres_dt = ppmc.evaluate()\n",
    "# ppbr2_df  = ppbres_dt['ppbr2_df']\n",
    "# lst = ppbr2_df.columns.to_list() # This code here is needed because ppbr2_df contains R^2 of all PGS vs all traits (from GWAS sumstats)\n",
    "# idx = list(np.array([lst,lst]).T)\n",
    "# ppbr2_df = ppbr2_df.stack().loc[idx].to_frame().T.droplevel(0,axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# # Init LinkageData:\n",
    "# linkdata = LinkageData(sst_df=bim_df, srd=srd, prd=prd[:,1], regdef_df=regdef_df, shift=10, compute_sumstats=True, dtype='float64', \n",
    "#                        _cross_chrom_ld=False) # Mind! _cross_chrom_ld=True lets this object calculate \"LD\" between chromosomes. \n",
    "# # This allows the full D matrix to be computed (D=X'*X/N, see manuscript for more info)\n",
    "# linkdata.retrieve_linkage_allregions()\n",
    "\n",
    "# s = linkdata.get_stansda().stats[:,[1]]\n",
    "# snpids     = linkdata.get_sumstats_cur()['snp'].values.astype(str)\n",
    "# pgsname    = effects_df.columns\n",
    "# pgsweights = effects_df.values.T\n",
    "# # brd        = pst.pstreader.PstData(col=pgsname, row=snpids, val=pgsweights)\n",
    "# brd        = pst.pstreader.PstData(row=pgsname, col=snpids, val=pgsweights)\n",
    "# sst_df = linkdata.get_sumstats_cur()\n",
    "# Bm = sst_df[['beta_mrg']]\n",
    "\n",
    "# # Do the Privacy Preserving Benchmark Approach:\n",
    "# ppmc     = PrivacyPreservingMetricsComputer(linkdata=linkdata, brd=brd, s=s, Bm=Bm, clear_linkage=False, dtype='float64')\n",
    "# metrix   = ppmc.evaluate()\n",
    "# ppbr2_df = metrix['ppbr2_df']\n",
    "\n",
    "# # Do the normal approach for performance estimation:\n",
    "# mpc = MultiPGSComputer(verbose=False, dtype='float64', brd=brd)\n",
    "# res = mpc.predict(srd=srd)\n",
    "# vanillar2_df = pheno_df.corrwith(Yhat).to_frame().T**2\n",
    "\n",
    "# # Display:\n",
    "# display(vanillar2_df, ppbr2_df)\n",
    "# # assert np.allclose(vanillar2_df, ppbr2_df)\n",
    "# # print('The results (R^2) for the normal individual-level and privacy-preserving approach match up perfectly.')\n",
    "\n",
    "# # Download Publically available Human Genetic Data if not present in data folder.\n",
    "# # WARNING!!!! : After all the simulations the .data folder will have 25GB of data in it.\n",
    "# if not os.path.exists('./data/human/g1000_eur.zip'): # 500MB\n",
    "#     !wget -P ./data/human http://ctg.cncr.nl/software/MAGMA/ref_data/g1000_eur.zip\n",
    "#     !unzip ./data/human/g1000_eur.zip -d ./data/human/\n",
    "\n",
    "# # Do some unixy pre-things:\n",
    "# !mkdir ../data\n",
    "# !ln -s ~/proj/data/g1000/ ../data/g1000\n",
    "# !ln -s ~/proj/data/defs/ ../data/defs\n",
    "# !ls -lah ../data/\n",
    "\n",
    "# ## Load Genomics Data:\n",
    "\n",
    "# # Defining Paramters:\n",
    "# cfg_dt = dict()\n",
    "# cfg_dt.update(dict(\n",
    "    \n",
    "#     base_fn  = '../data/g1000/',\n",
    "#     rsid_fn  = '../data/defs/hm3.bim',\n",
    "#     geno_fn  = 'g1000_eur',\n",
    "#     hdf5_fn  = 'sim_mvn_chr22_100K.h5',\n",
    "#     chroms   = [22],\n",
    "#     dtype    = 'float32',\n",
    "#     n_sim_tot = 100000,\n",
    "#     n_trn     = 84000,\n",
    "#     n_val     = 8000,\n",
    "#     n_tst     = 8000,\n",
    "#     random_state = 42\n",
    "    \n",
    "# )) \n",
    "# locals().update(cfg_dt)\n",
    "\n",
    "# # Load bim, fam & rsids:\n",
    "# bed_fn = base_fn + f'{geno_fn}.bed'\n",
    "# # bim_df, fam_df = load_bimfam(base_fn + geno_fn) # A bit slow\n",
    "# # rsid_df, _ = load_bimfam(rsid_fn, fam=False) # A bit slow\n",
    "\n",
    "# # Load Pheno & Bed Reader (=implicit loading):\n",
    "# # tot_srd  = Bed(bed_fn, count_A1=True)\n",
    "\n",
    "\n",
    "# def experimental_setup(return_variable=None, trait='Asthma', adj_type='m16', geno_fn='UKBB_imp_HM3', fold='test',\n",
    "#                        regdef_key='1blk_shift=0',shift=0, cm=None, random_state=42, gb_size_limit=10, dtype='float32',\n",
    "#                        _save_vars = ['L','D','R', 'sst_df'],\n",
    "#                        verbose=True):\n",
    "    \n",
    "#     # Defining Paramters:\n",
    "#     cfg = mStruct()\n",
    "#     cfg_dt = dict()\n",
    "#     cfg_dt.update(dict(\n",
    "\n",
    "#         test_fn    = f'pheno/{trait}.{adj_type}.{fold}.pheno',\n",
    "#         sstrds_fn  = f'../data/sumstats/{trait}.rds',\n",
    "#         geno_fn    = geno_fn,\n",
    "#         base_fn    = '../lnk/data/ukbb/imp/',\n",
    "#         regdef_dn  = '../lnk/data/regdef/',\n",
    "#         n_pca        = None,\n",
    "#         random_state = random_state,\n",
    "#         dtype        = dtype\n",
    "        \n",
    "#     ))\n",
    "#     cfg.update(cfg_dt)\n",
    "    \n",
    "#     # Computing full paths:\n",
    "#     plink_fn      = cfg.base_fn + cfg.geno_fn\n",
    "#     pheno_tst_fn  = cfg.base_fn + cfg.test_fn\n",
    "#     regdef_fn     = cfg.regdef_dn + f'regions_{regdef_key}.regdef.tsv'\n",
    "\n",
    "#     # Load SNPs filter list & region definition:\n",
    "#     regdef_df = pd.read_csv(regdef_fn, delimiter='\\t') \n",
    "    \n",
    "#     # Load Genotype data (mostly 'implicit')\n",
    "#     bim_df, fam_df = load_bimfam(plink_fn, fil_arr=None)\n",
    "#     df = pd.read_csv('../data/ukbb/mapper.csv') # a bunch of ugly stuff to get cM's in\n",
    "#     assert np.all(df[['chromosome','marker.ID']].values == bim_df[['chrom','snp']].values)\n",
    "#     mapper_df = df\n",
    "#     bim_df['cm'] = mapper_df['cm']\n",
    "#     tot_srd  = Bed(plink_fn, count_A1=True)\n",
    "#     # Load Pheno & SNP Reader (=implicit loading):\n",
    "#     tst_prd  = Pheno(pheno_tst_fn) # Test\n",
    "# #     tst_srd, tst_prd = pstutil.intersect_apply([tot_srd, tst_prd])\n",
    "    \n",
    "#     # Load GWAS summary stats & create beta_mrg:\n",
    "#     rss_df = pyreadr.read_r(cfg.sstrds_fn)[None]; d=rss_df\n",
    "#     sst_df = bim_df.loc[rss_df['_NUM_ID_'].values-1].copy().reset_index(drop=True)\n",
    "#     colp_lst = [['pos','pos'],['chrom','chr'],['A1','a0'],['A2','a1']]\n",
    "#     assert np.all([np.all(sst_df[colp[0]].values == rss_df[colp[1]].values) for colp in colp_lst])\n",
    "#     sst_df['beta_mrg'] = d.beta / np.sqrt(d.n_eff*d.beta_se**2 + d.beta**2)\n",
    "#     sst_df['n_eff'] = rss_df.n_eff\n",
    "    \n",
    "#     # Load test linkdata, perfectly prepped for PPBMeasureComputer\n",
    "#     linkdata = LinkageData(sst_df=sst_df, regdef_df=regdef_df, srd=tot_srd, prd=None, \n",
    "#                            gb_size_limit=gb_size_limit, shift=shift, cm=cm, \n",
    "#                            dtype=cfg.dtype, _save_vars=_save_vars, verbose=verbose);\n",
    "    \n",
    "#     if return_variable is None: # Return everything\n",
    "#         return locals()\n",
    "#     else:\n",
    "#         return locals()[return_variable]\n",
    "\n",
    "    \n",
    "# # Dev stuff:\n",
    "# xp = experimental_setup(geno_fn='UKBB_imp_HM3.val', fold='val', cm=2.)# _save_vars = ['L','D','R'])#,'sst_df'])\n",
    "# locals().update(xp)\n",
    "# display(rss_df.head())\n",
    "# display(sst_df.head())\n",
    "# self = linkdata\n",
    "# linkdata._clear_vars = ['L', 'D', 'R', 'sst_df']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%javascript\n",
    "# var output_area = this;\n",
    "# // find my cell element\n",
    "# var cell_element = output_area.element.parents('.cell');\n",
    "# // which cell is it?\n",
    "# var cell_idx = Jupyter.notebook.get_cell_elements().index(cell_element);\n",
    "# // get the cell object\n",
    "# var cell = Jupyter.notebook.get_cell(cell_idx-1)\n",
    "# var code = cell.get_text();\n",
    "# console.log(cell)\n",
    "# console.log(5)\n",
    "# console.log(code)\n",
    "# IPython.notebook.kernel.execute(`myvar = '${encodeURI(code)}'`);\n",
    "# // IPython.notebook.kernel.execute(`myvar = '${code}'`);\n",
    "\n",
    "# from IPython.display import Javascript\n",
    "# import urllib.parse\n",
    "\n",
    "# Javascript(\"const code = Jupyter.notebook.get_cell(0).get_text(); IPython.notebook.kernel.execute(`myvar = '${encode(code)}'`);\")\n",
    "# urllib.parse.unquote('cell ergergerg')\n",
    "# urllib.parse.unquote(myvar)\n",
    "\n",
    "\n",
    "# %%javascript\n",
    "# // querySelectorAll returns a NodeList which lacks most Array functions\n",
    "# // so destructuring into array\n",
    "# const outputs=[...document.querySelectorAll(\".cell\")].map(\n",
    "#         cell=> {\n",
    "#             const RawCellN=cell.querySelector(\".input_prompt\").innerText;\n",
    "#             // \\xa0 is star. current: skip.\n",
    "#             if (RawCellN.match(/\\[(\\d+)\\]/) === null) return null; \n",
    "#             const cellN =  parseInt(RawCellN.match(/\\[(\\d+)\\]/)[1]);\n",
    "#             const outputs= [...cell.querySelectorAll(\".output_subarea\")].map(\n",
    "#                 subarea => subarea.innerText.trim());\n",
    "#             return [cellN, outputs.filter(out => out.length !== 0)];\n",
    "#         }\n",
    "#     ).filter(value => value !== null); // star cell was skipped.\n",
    "\n",
    "# // pass on the data to python\n",
    "# IPython.notebook.kernel.execute(\"outputs=dict(\"+JSON.stringify(outputs)+\")\")\n",
    "\n",
    "\n",
    "# from IPython.display import Javascript\n",
    "# from urllib.parse import unquote\n",
    "# from json import loads as jsonloads\n",
    "\n",
    "# def markread(cellnumber,notebookname=None,callbackvar=None):\n",
    "#     try:\n",
    "#         if type(cellnumber) is int:# maybe check if (varname in globals()):\n",
    "#             if callbackvar is not None and type(callbackvar) is str:\n",
    "#                 return Javascript(\"const mdtjs = Jupyter.notebook.get_cells().filter(c=>c.cell_type==\\\"markdown\\\")[\"+str(cellnumber)+\"].get_text(); IPython.notebook.kernel.execute(\\\"mdtp = unquote('\\\"+encodeURI(mdtjs)+\\\"');mdtp=mdtp[mdtp.find('\\\\\\\\n',mdtp.find('```'))+1:min(mdtp.rfind('\\\\\\\\n'),mdtp.rfind('```'))].strip();\"+callbackvar+\"=mdtp;del mdtp\\\");\")\n",
    "#             if notebookname is not None and type (notebookname) is str:\n",
    "#                 if not notebookname.endswith('.ipynb'):\n",
    "#                     notebookname += '.ipynb'\n",
    "#                 with open(notebookname) as f:\n",
    "#                     j = jsonloads(f.read())\n",
    "#                 mdts = [''.join(c['source'][1:]).strip().strip('`').strip() for c in j['cells'] if c['cell_type']=='markdown']\n",
    "#                 return mdts[cellnumber]\n",
    "#     except:\n",
    "#         return None\n",
    "#     return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "282.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
